= LLM Configuration
:type: lesson
:order: 5
:optional: true
:branch: new-course

You can modify which large language model (LLM) you use to suit your own needs.

You may want to:

- Use a different LLM provider for cost or feature reasons.
- Adjust model parameters to control the output.
- Run a local LLM for data privacy or latency reasons.
- Modify the prompt to cater to your specific domain.

== LLM Provider

The LLM you use is configurable in the `SimpleKGPipeline` via the `llm` parameter.

The `neo4j_graphrag` package includes link:https://neo4j.com/docs/neo4j-graphrag-python/current/api.html#openaillm[adapters for several popular LLM providers^], including OpenAI, Azure OpenAI, and Ollama.

For example, you could run the OpenAI open source model link:https://huggingface.co/openai/gpt-oss-20b[`openai/gpt-oss-20b`^] locally using a application such as link:https://lmstudio.ai/[LM Studio^]:

[source,python]
----
llm = OpenAILLM(
    model_name="openai/gpt-oss-20b",
    model_params={
       "temperature": 0
    },
    base_url = "http://localhost:1234/v1"
)
----

[TIP]
.Model Parameters
Through `model_params` you can change how by model responds by adjusting model parameters such as `temperature` and `response_format`.
The parameters available will depend on the specific LLM you are using.

You can also create your own LLM adapter by inheriting from the link:https://neo4j.com/docs/neo4j-graphrag-python/current/api.html#llminterface[LLMIntercace^] class.

== Prompt Customization

The prompt used for entity extraction and other tasks can also be customized by modifying the `prompt_template` parameter of the `SimpleKGPipeline`.

You can provide an entirely new prompt, but it is often easier to add to the existing.

If you wanted to restrict entity extraction to a specific domain, such as technology companies, you could modify the prompt as follows:

[source,python]
.Custom prompt
----
include::{repository-raw}/{branch}/genai-graphrag-python/examples/entity_extraction_prompt.py[tag=import_prompt]

include::{repository-raw}/{branch}/genai-graphrag-python/examples/entity_extraction_prompt.py[tag=prompt]
----

The `domain_instructions` are added to the start of the default entity extraction prompt to guide the LLM to only extract relevant entities.

The custom prompt can then be used in the `SimpleKGPipeline` by setting the `prompt_template` parameter:

[source,python]
----
include::{repository-raw}/{branch}/genai-graphrag-python/examples/entity_extraction_prompt.py[tag=kg_builder]
----

[%collapsible]
.Reveal the complete code
====
This example code shows how to create and use a custom prompt in a `SimpleKGPipeline`:
[source, python]
----
include::{repository-raw}/{branch}/genai-graphrag-python/examples/entity_extraction_prompt.py[tag=**]
----
====

When you're ready you can continue.

read::Continue[]

[.summary]
== Lesson Summary

In this lesson, you learned about the options for configuring the LLM used in the knowledge graph pipeline, including selecting different LLM providers and customizing prompts.

In the next lesson, you will use what you have learned to create your own knowledge graph from your documents.