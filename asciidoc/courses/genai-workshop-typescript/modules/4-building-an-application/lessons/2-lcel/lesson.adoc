= LCEL
:type: challenge
:order: 2
:optional: true

To pass this lesson, you will need to use LangChain Expression Language (LCEL) to create a `RunnableSequence` that:

1. Replaces the input values in a Prompt Template
2. Sends the formatted prompt to an LLM
3. Parses the response as a string

Update the `call()` function in the `src/modules/agent/index.ts` to integrate the chain into the Chatbot application.


[%collapsible]
.Solution
====

[source,typescript]
----
import { ChatPromptTemplate, SystemMessagePromptTemplate, HumanMessagePromptTemplate } from "@langchain/core/prompts";
import { ChatOpenAI } from "@langchain/openai";
import { StringOutputParser } from "@langchain/core/output_parsers";
import { RunnableSequence } from "@langchain/core/runnables";

export async function call(
  message: string,
  sessionId: string
): Promise<string> {
  // 1. create a prompt template
  const prompt = ChatPromptTemplate.fromMessages([
    SystemMessagePromptTemplate.fromTemplate(
      `You are a helpful assistant helping users with queries
      about the CityJS Athens conference.
      Answer the user's question to the best of your ability.
      If you do not know the answer, just say you don't know.
      `
    ),
    HumanMessagePromptTemplate.fromTemplate(`Question: {message}`),
  ]);

  // 2. choose an LLM
  const llm = new ChatOpenAI({
    openAIApiKey: process.env.OPENAI_API_KEY,
    temperature: 0.1,
  });

  // 3. parse the response
  const parser = new StringOutputParser();

  // 4. runnable sequence (LCEL)
  const chain = RunnableSequence.from<RunInput, string>([
    prompt,
    llm,
    new StringOutputParser(),
  ]);

  // 5. invoke the chain
  const output = await chain.invoke(
    { message },
  );

  return output;
}
----
====

You can run the application using:

[source,sh]
----
npm run dev
----

read::It works![]

[.summary]
== Summary

Good job, you're ready for the next challenge.

