Stuff I removed from the lesson. need to find a home for it.


// You cant define the model and temperature and still call llm directly

You can use the LLM instantation to define the `module` to use, and the `temperature`.

When selecting a model, it is worth considering the quality of the output and the cost per token.

As we discussed in the first module, all prompts are accompanied by a `temperature`.
The temperature is a value between `0.0` and `1.0` which affects the randomness, or creativeness of the response.

The following code block creates an interface to communicate with OpenAIs GPT-4 model with a temperature of `0.0`.
In general, this should produce a good response, as grounded in fact as possible.


[source,python]
llm = OpenAI(
    openai_api_key="...",
    model="gpt-4",
    temperature=0.0
)

// It seems too early to include this


[TIP]
.LCEL
====
LLMs implement the link:https://python.langchain.com/docs/expression_language/interface[Runnable interface^], the basic building block of the _LangChain Expression Language (LCEL)_. This means they support `invoke`, `stream` and `batch` for synchronous calls and `ainvoke`, `astream` and `abatch` for asynchronous calls.

You can link:https://python.langchain.com/docs/expression_language/interface[learn more in the Interface documentation^].
====


