= Chat Models

In the previous lesson, you learned how to perform basic communication with an LLM using one of the many LLM providers supported by Langchain.

* Simple communication only
* Provide text input and get a response



== Chat Models vs Language Models

* Chat models differ, they are designed to provide a conversational interface
* Think of the difference as the difference between the underlying GPT-4 model and ChatGPT, which is an interface on top
* Chat APIs support different types of messages - system, ai, human

* System Messages are designed to instruct the LLM on how to act on human messages
* Human messages are messages sent from the user, or prompts formatted with user input
* Responses from the AI are called AI Responses

* Chat models can be used as language models, language models cannot be used as chat models? **Verify**

[source,python]
----
from langchain.chat_models.openai import ChatOpenAI

chat_llm = ChatOpenAI()
----


The chat model expects a list of messages, which can be a mix of System, Human and AI.

* System messages are designed to instruct the LLM on how to respond to the conversation.
* They can be created using a `SystemMessagePromptTemplate` or by instantiating the `langchain.schema.SystemMessage`.

[source,python]
----
from langchain.prompts import SystemMessagePromptTemplate

instructions = SystemMessagePromptTemplate.from_template("""
You are a {location} surfer dude, having a conversation about the surf conditions on the beach.
Respond using surfer slang.
""")
----

Human messages can be created in the same way.

[source,python]
----
question  = HumanMessage(content="What is the weather like?")
----


These objects can be passed to the `ChatLLM` model as a list.

[source,python]
----
chat_llm([
    instructions.format(location="California"),
    question
] )
----

The response comes back in the form of an `AIMessage`.

    AIMessage(content="Dude, the weather is totally gnarly! It's sunny with a few scattered clouds, and the breeze is just perfect for some sick waves. The stoke level is high, my friend!", additional_kwargs={}, example=False)


== Wrappinng in a Chain

* A `QAChain` is a type of chain that brings together multiple components to answer questions.
* The main components of a QA chain are:
** Question Parser: Analyzes the input question to extract key information.
** Retriever: Retrieves relevant documents or passages from a corpus based on the question. This is often done using semantic search and vector embeddings.
** Reader: Reads and comprehends the retrieved documents/passages to identify the answer. A machine reading model like a Transformer is commonly used.
** Response Generator: Formulates a natural language response based on the answer extracted by the reader.


The benefits of using this chain are:

* **Modularity**: LangChain provides many modules that can be used to build language model applications. These modules can be used as stand-alones in simple applications and they can be combined for more complex use cases.

* **Customizability**: Most LangChain applications allow you to configure the LLM and/or the prompt used, so knowing how to take advantage of this will be a big enabler.

* **Ease** of Use: The components are designed to be easy to use, regardless of whether you are using the rest of the LangChain framework or not.

* **Standard** Interface: LangChain provides a standard interface for chains, enabling developers to create sequences of calls that go beyond a single LLM call.


=== Adding Memory

[source,python]
----
from langchain.chains.conversation.memory import ConversationBufferWindowMemory


memory = ConversationBufferWindowMemory(
    memory_key='chat_history',
    k=5,
    return_messages=True
)
----

* k = keep five messages



== Summary

* Chat models are designed for conversations

* Chains are used to piece different elements together into one call

* Chains support memory, system commands
*



* How QA Chains work
* Creating a custom prompt
* Manual RAG example
