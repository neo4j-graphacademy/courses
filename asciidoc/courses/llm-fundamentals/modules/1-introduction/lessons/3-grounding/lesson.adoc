= Grounding LLMs
:order: 3

// TODO: Do we include in-context learning, fine-tuning or prompt emgineering?

In the previous lesson, you learned about the potential methods that you could use to avoid Hallucination.
The last point mentioned was **Grounding**.

For developers and data scientists, this method offers the highest impact for the lowest effort.

In the previous lesson, we mentioned an example chatbot for a news agency.
Let's drill down into that example.

== Data Cut-Off Dates

As you can imagine, training a Large Language Model has a high computational cost.
According to Wikipedia, OpenAI's GPT-3 model was link:https://en.wikipedia.org/wiki/GPT-3[trained on 175 billion parameters^], and the resulting trained model takes 800GB to store.

This would be expensive and time-consuming to constantly train on real data.
A model the size of GPT-3 would typically take weeks to months to train on supercomputing clusters with multiple GPUs or TPUs.

For a news agency providing real-time updates on fast-moving breaking news would be out of the question.

In this case, it makes sense to train a model on static data and supplement the pre-existing knowledge base with real-time data retrieved from up-to-date sources.

This is where **Retrieval Augmented Generation**, or **RAG** comes in.


== Retrieval Augmented Generation

Retrieval Augmented Generation (RAG) is a hybrid approach that combines the strengths of large-scale language models with external retrieval or search mechanisms, enabling the model to dynamically pull relevant information from vast datasets during the generation process, thereby enhancing its ability to provide detailed and contextually accurate responses.

If you take the RAG acronym in reverse, it describes a technique in which an LLM **generates** a piece of content, which is **augmented** by data **retrieved** from another source.
In other words, content from additional data sources can be used to improve the content generated by an LLM.

=== Benefits of RAG

The main benefit of RAG is its **enhanced accuracy**.
By dynamically pulling information from external, domain-specific sources, a response grounded by RAG can provide more detailed and contextually accurate responses than a standalone LLM.

RAG also provides the benefit of increased transparency, as the sources of the information can be stored and examined.


[WARNING]
.Bad data in, bad data out
====
When prompting the LLM to respond based on the context provided, the answer will always be as good as the provided context.

If your prompt suggests pineapple as a pizza topping, don't be surprised if it suggests that you order a Hawaiian Pizza.
====


=== Example

For our news agency chatbot, real-time headlines or news articles could be pulled from a database, or even read from the website in real-time, and appended to the prompt.

If the news articles are stored in a knowledge graph along with relationships to the entities involved, the metadata surrounding the article can also be passed to the LLM to further improve the response.


== Implementing RAG with Neo4j

// TODO: rewrite
To implement RAG with Neo4j, you have two options.

For unstructured, text content, you can use the **Vector index** to store vector representations of the text and use the embedding of a user's question to discover the most relevant chunks of text.


// TODO: too soon?
For structured content in the graph, we can instruct the LLM to use out data model to discover the correct data.

In the next module, you will see these two options in action.


== Check Your Understanding

include::questions/1-rag-operation.adoc[leveloffset=+1]
include::questions/2-benefits.adoc[leveloffset=+1]


[.summary]
== Lesson Summary

In this lesson, you learned about the importance of grounding to enhance LLM accuracy, the computational challenges of constantly training large models, and how Retrieval Augmented Generation (RAG) combines LLMs with external data for improved and transparent responses.

// TODO:
In the next module, you will learn how to do something...


// TODO: References
