= Grounding LLMs
:order: 3
:type: lesson

// TODO: Do we include in-context learning, fine-tuning or prompt emgineering?

In the previous lesson, you learned about the potential methods that you could use to avoid Hallucination, including _Grounding_.

Grounding is the process of providing context to an LLM to improve the accuracy of its responses. For developers and data scientists, grounding usually offers the highest impact for the lowest effort.

You reflected on an example chatbot for a news agency that pulled real-time headlines or articles from a news API.

In this lesson, you will explore the news agency use case in more detail.

== Data Cut-Off Dates

Training a Large Language Model has a high computational cost. According to Wikipedia, OpenAI's GPT-3 model was link:https://en.wikipedia.org/wiki/GPT-3[trained on 175 billion parameters^], and the resulting trained model takes 800GB to store.

Retraining this model on new data would be expensive and time-consuming. A model the size of GPT-3 would also typically take weeks to months to train on supercomputing clusters with many GPUs or TPUs.

Providing real-time updates on fast-moving breaking news would be out of the question.

A new agency could train a model on static data and supplement the pre-existing knowledge base with real-time data retrieved from up-to-date sources.

**Retrieval Augmented Generation**, or **RAG** is a solution for this problem.

== Retrieval Augmented Generation

Retrieval Augmented Generation (RAG) is a hybrid approach that combines the strengths of large-scale language models with external retrieval or search mechanisms, enabling relevant information from vast datasets to be dynamically fed into the model during the generation process, thereby enhancing its ability to provide detailed and contextually accurate responses.

In summary, by adding content from additional data sources, you can improve the responses generated by an LLM.

=== Benefits of RAG

The main benefit of RAG is its **enhanced accuracy**. By dynamically pulling information from external, domain-specific sources, a response grounded by RAG can provide more detailed and contextually accurate answers than a standalone LLM.

RAG provides additional benefits of: 

* Increased transparency, as the sources of the information can be stored and examined.
* Security, as the data sources can be secured and access controlled.
* Accuracy and timeliness, as the data sources can be updated in real-time.
* Access to private or proprietary data

[WARNING]
.Bad data in, bad data out
====
When prompting the LLM to respond based on the context provided, the answer will always be as good as the provided context.

If your prompt suggests pineapple as a pizza topping, don't be surprised if it suggests that you order a Hawaiian Pizza.
====


RAG could support the new agency chatbot by:

. Accessing real-time new feeds
. Pulling recent headlines or news articles from a database, 
. Giving this additional context to the LLM

New articles stored in a knowledge graph would be ideal for this use case. A knowledge graph could pass the LLM detail about the relationship between the entities involved and the article's metadata.

For example, when asking about the results of a recent election, the knowledge could provide additional context about the candidates, news stories relating to them, or interesting articles from the same author.

During this course, you will explore methods for implementing RAG with Neo4j, including:

* Semantic Search
* Zero-shot and few-shot learning and prompts
* Text embedding and vector indexes with unstructured data
* Cypher generation to gather structured data from a knowledge graph


== Check Your Understanding

include::questions/1-benefits.adoc[leveloffset=+1]


[.summary]
== Lesson Summary

In this lesson, you learned about the importance of grounding to enhance LLM accuracy, the computational challenges of constantly training large models, and how Retrieval Augmented Generation (RAG) combines LLMs with external data for improved responses with increased transparency.

In the next module, you will learn how to implement semantic search and use vector indexes in Neo4j.

