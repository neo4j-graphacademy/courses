= Avoiding Hallucination
:order: 2

As you learned in the previous lesson, LLMs can "make things up".

LLMs are designed to generate human-like text based on the patterns they've identified in vast amounts of data. Their primary objective is to provide coherent, contextually relevant, and informative responses.

However, due to their reliance on patterns and the sheer volume of training information, LLMs sometimes **hallucinate** or produce outputs that manifest as generating untrue facts, asserting details with unwarranted confidence, or crafting plausible yet nonsensical explanations.

These manifestations arise from a mix of _overfitting_, biases in the training data, and the model's attempt to generalize from vast amounts of information.

== Common Hallucination Problems

Let's take a closer look at some reasons why this may occur.

=== Temperature

LLMs have a _temperature_, corresponding to the amount of randomness the underlying model should use when generating the text.

The higher the temperature value, the more random the generated result will become, and the more likely the response will contain false statements.

A higher temperature may be appropriate when configuring an LLM to respond with more diverse and creative outputs, but it comes at the expense of consistency and precision.

For example, a higher temperature may be suitable for constructing a work of fiction or a novel joke.

On the other hand, a lower temperature, even `0`, is required when a response grounded in facts is essential.

[TIP]
.Consider the correct temperature
====
In June 2023, link:https://www.reuters.com/legal/new-york-lawyers-sanctioned-using-fake-chatgpt-cases-legal-brief-2023-06-22/[A US judged sanctioned two US lawyers for submitting an LLM-generated legal brief^] that contained six fictitious case citations.
====

A quick fix _may_ be to reduce the temperature.  But more likely, the LLM is hallucinating because it hasn't got the information required.

=== Missing Information

The training process for LLMs is intricate and time-intensive, often requiring vast datasets compiled over extended periods. As such, these models might lack the most recent information or might miss out on specific niche topics not well-represented in their training data.

For instance, if an LLM's last update was in September 2022, it would be unaware of world events or advancements in various fields that occurred post that date, leading to potential gaps in its knowledge or responses that seem out of touch with current realities.

If the user asks a question on information that is hard to find, or outside of the public domain, it will be virtually impossible for an LLM to respond accurately.

Luckily, this is where factual information from data sources such as knowledge graphs can help.

=== Model Training and Complexity

Large Language Models (LLMs) are often considered "black boxes" due to the difficulty of deciphering their decision-making processes.

The complexity of these models, combined with potential training on erroneous or misleading data, means that their outputs can sometimes be unpredictable or inaccurate.

For example, when asked about a controversial historical event an LLM might produce a biased or incorrect answer. 

Furthermore, it would be near impossible to trace back how the model arrived at that conclusion. The LLM would also be unable to provide the sources for its output or explain its reasoning.

// How?
== Improving LLM Accuracy

The following methods can be employed to help guide LLMs to produce more consistent and accurate results.

=== Prompt Engineering

Prompt engineering is an iterative process of crafting specific and deliberate instructions that guide the LLM toward the desired response.

By refining how you pose instructions, developers can achieve better results from existing models without retraining.

For example, if you require a blog post summary, rather than asking _"What is this blog post about?"_, a more appropriate response would be _"Provide a concise, three-sentence summary and three tags for this blog post."_

You could also include _"Return the response as JSON"_ and provide an example output to make it easier to parse in the programming language of your choice.

Providing additional instructions and context in the question is also known as **Zero-shot learning**.

[TIP]
.Be Positive
====
When writing a prompt, aim to provide positive instructions.
For the same reason a child may ignore you when telling them not to draw on the walls, the model will act based on what you don't want; rather than what you need.
====
// TODO: better example

=== In-Context Learning

In-context learning provides the model with examples to inform its responses, helping it comprehend the task better.

The model can deliver more accurate answers by presenting relevant examples, especially for niche or specialized tasks.

For example, if the LLM starts talking about Tim Cook and iPhones every time you mention apples, you may want to add to the prompt that _apple_ refers to the fruit.

Providing relevant examples for specific tasks is a form of **Few-shot learning**.


=== Fine-Tuning

Fine-tuning involves additional language model training on a smaller, task-specific dataset after its primary training phase. This approach allows developers to specialize the model for specific domains or tasks, enhancing its accuracy and relevance. 

Fine-tuning an existing model on the nuances of your programming language would enhance its capability to write performant code.


You could also prompt the LLM to generate responses in a distinctive style or as if written by a particular type of person. For example, "You are an 18th-century poet" or "You are an expert programmer in Python".

This method is the most complicated, involving technical knowledge and domain expertise.

A more straightforward approach would be to _ground_ the model by providing information with the prompt.

=== Grounding

Grounding allows a language model to reference external, up-to-date sources or databases to enrich the responses.
By integrating real-time data or APIs, developers ensure the model remains current and provides factual information beyond its last training cut-off.

For instance, if building a chatbot for a news agency, instead of solely relying on the model's last training data, grounding could allow the model to pull real-time headlines or articles from a news API. When a user asks, "What's the latest news on the Olympics?", the chatbot, through grounding, can provide a current headline or summary from the most recent articles, ensuring the response is timely and accurate.

== LLMs and Knowledge Graphs

In the coming lessons, you will explore these topics in detail and discover how LLMs can use Knowledge Graphs to improve their accuracy and relevance.

== Check Your Understanding

include::questions/1-temperature.adoc[leveloffset=+1]
include::questions/2-external-data.adoc[leveloffset=+1]


[.summary]
== Lesson Summary

In this lesson, you explored the intricacies of Large Language Models (LLMs), understanding their tendencies to hallucinate and the various strategies to improve their accuracy, such as temperature settings, prompt engineering, in-context learning, fine-tuning, and grounding with external data sources like APIs.

In the next lesson, you will learn about the techniques for _grounding_ an LLM.

// TODO: References
