= Avoiding Hallucination
:order: 2

// Why?

As you learned in the previous lesson, LLMs tend to make things up.
LLMs are designed to generate human-like text based on the patterns they've identified in vast amounts of data.
Their primary objective is to provide coherent, contextually relevant, and informative responses.

However, due to their reliance on patterns and the sheer volume of information they have been trained on, LLMs sometimes **hallucinate** or produce outputs that manifest as generating untrue facts, asserting details with unwarranted confidence, or crafting plausible yet nonsensical explanations.

These manifestations arise from a mix of overfitting, biases in the training data, and the model's attempt to generalize from the vast information it has been trained on.



// What?
== Common Hallucination Problems

Let's take a closer look at some reasons why this may occur.

=== Temperature

LLMs are configured with a _temperature_, that corresponds to the amount of randomness that the underlying model should use when generating the text.
The higher the temperature value, the more random the generated result will become, and the more likely the response will contain unfactual statements.

A higher temperature may be appropriate when configuring an LLM to respond with more diverse and creative outputs, but it comes at the expense of consistency and precision.

For example, a higher temperature may be suitable when constructing a work of fiction or a novel joke.
On the other hand, a lower temperature, even `0`, is required when a response grounded in facts is important.

[TIP]
.Consider the correct temperature
====
In June 2023, link:https://www.reuters.com/legal/new-york-lawyers-sanctioned-using-fake-chatgpt-cases-legal-brief-2023-06-22/[two US lawyers were sanctioned for submitting an LLM-generated legal brief] that contained six fictitious case citations.
====

A quick fix _may_ be to reduce the temperature.  But more likely, the LLM is hallucinating because it hasn't got the information required.


=== Model Training and Complexity

Large Language Models (LLMs) are often considered "black boxes" due to the difficulty deciphering their decision-making processes.
The complexity of these models combined with potential training on erroneous or misleading data means that their outputs can sometimes be unpredictable or inaccurate.

For example, if asked about a controversial historical event, an LLM might produce an answer that's biased or misrepresentative, largely due to the data it was trained on.

Furthermore, it would be near impossible to trace back how the model arrived at that conclusion transparently.
The LLM would not be able to provide the sources for its output.



=== Missing Information

The training process for LLMs is intricate and time-intensive, often requiring vast datasets compiled over extended periods. As such, these models might lack the most recent information or might miss out on specific niche topics not well-represented in their training data.

For instance, if an LLM's last update was in September 2022, it would be unaware of world events or advancements in various fields that occurred post that date, leading to potential gaps in its knowledge or responses that seem out of touch with current realities.

If the user asks a question on information that is hard to find, or outside of the public domain, it will be virtually impossible for an LLM to respond accurately.

Luckily, this is where factual information from data sources such as knowledge graphs can help.


// How?
== Improving LLM Accuracy

The following methods can be employed to help guide LLMs to produce more consistent and accurate results.


=== Prompt Engineering

Prompt engineering is an iterative process of crafting specific and deliberate instructions that guide the LLM toward the desired response.
By refining the way instructions are posed, developers can achieve better results from existing models without retraining.

For example, if you require a summary of a blog post, rather than asking _"What is this book about?"_, a more appropriate response would be _"Provide a concise, three-sentence summary and three tags for this blog post."_

You could also include _"Return the response as JSON"_ and provide an example output to make it easier to parse in the programming language of your choice.

This is also known as **Zero-shot learning**.


[TIP]
.Be Positive
====
When writing a prompt, aim to provide positive instructions.
For the same reason a toddler will ignore you when telling them not to draw on the walls, the model will act based on what you don't want, rather than what you need.
====
// TODO: better example



=== In-Context Learning

In-context learning provides the model with a set of examples to inform its responses, helping it comprehend the task better.
The model can deliver more accurate answers by presenting relevant examples, especially for niche or specialized tasks.

For example, if the LLM starts talking about Tim Cook and iPhones every time you mention apples, you may want to add to the prompt that _apple_ refers to the fruit.


=== Fine-Tuning

Fine-tuning involves additional language model training on a smaller, task-specific dataset after its primary training phase. This approach allows developers to specialize the model for specific domains or tasks, enhancing its accuracy and relevance.

For instance, fine-tuning an existing model on the fine nuances of your programming language would enhance its capability to write performant code.
This method is by far the most complicated, involving both technical knowledge and domain expertise.

A simpler approach would be to _ground_ the model by providing information with the prompt.


=== Grounding

Grounding allows a language model to reference external, up-to-date sources or databases to enrich the responses.
By integrating real-time data or APIs, developers ensure the model remains current and provides factual information beyond its last training cut-off.

For instance, if building a chatbot for a news agency, instead of solely relying on the model's last training data, grounding could allow the model to pull real-time headlines or articles from a news API. When a user asks, "What's the latest news on the Olympics?", the chatbot, through grounding, can provide a current headline or summary from the most recent articles, ensuring the response is timely and accurate.



== LLMs and Knowledge Graphs

In the coming lessons, you will discover how Knowledge Graphs can be used alongside LLMs, both directly and indirectly, as we explore these topics in more detail.


== Check Your Understanding

include::questions/1-temperature.adoc[leveloffset=+1]
include::questions/2-external-data.adoc[leveloffset=+1]


[.summary]
== Lesson Summary

In this lesson, you explored the intricacies of Large Language Models (LLMs), understanding their tendencies to hallucinate and the various strategies to improve their accuracy, such as temperature settings, prompt engineering, in-context learning, fine-tuning, and grounding with external data sources like APIs.

// TODO:
In the next lesson, you will learn how to do something...

// TODO: References
