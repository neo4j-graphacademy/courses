= Introduction to Neo4j & GenAI
:order: 1

In this lesson, we will provide an introduction to Neo4j and Generative AI, and explore the synergy between these two powerful technologies.

Neo4j is a graph database management system that allows us to store, manage, and query highly connected data. Unlike traditional relational databases, which use tables and rows, Neo4j uses a graph-based model with nodes and relationships. This makes it particularly well-suited for representing and querying complex, interconnected data.

Now, let's move on to Generative AI. Generative AI refers to a class of algorithms and models that can generate new content, such as images, text, or even music, based on patterns and examples from existing data. These models learn the underlying structure and distribution of the data and can then generate new samples that resemble the original data.

Generative AI has found applications in a wide range of fields, including image synthesis, text generation, chatbots, and recommendation systems. By leveraging the power of machine learning and deep learning techniques, generative AI models can generate highly realistic and creative outputs.

So, what is the synergy between Neo4j and Generative AI? Neo4j provides a powerful platform for managing and querying complex, interconnected data, while Generative AI enables us to generate new content based on patterns and examples from that data.

By combining the strengths of Neo4j and Generative AI, we can create intelligent systems that can not only store and query data but also generate new insights and recommendations based on that data. For example, we can build a movie recommendation system that uses a Neo4j database to store information about movies, users, and their interactions, and then use Generative AI techniques to generate personalized movie recommendations for users.

Throughout this course, we will explore how to leverage the capabilities of Neo4j and Generative AI to build such intelligent systems. We will learn how to model and store data in Neo4j, how to query and analyze the data using the Cypher query language, and how to integrate Generative AI techniques to generate new content and insights.

By the end of this course, you will have the knowledge and skills to create your own intelligent systems using Neo4j and Generative AI. You will be able to build recommendation systems, chatbots, and other applications that can provide personalized and creative outputs based on complex, interconnected data.

So, let's dive into the world of Neo4j and Generative AI and unlock the potential of these technologies in creating intelligent systems.


// // === Temperature

// // The temperature variable passed to an LLM controls the randomness of the prediction.
// // The higher the temperature, the wilder the suggesti2on may become.


// // === Training Cut-off Date

// // LLMs are computationally expensive to train

// // GPT 3.5 was trained up until 28 August 2023.



// // * for over a decade neo4j has been helping the world make sense of data
// // * today neo4j the leader in graph database and analytics technology is unlocking new possibilities in generative AI
// // * together, neo4j and large language models allow you to use a public pre-trained llm with your own data subject to your privacy controls
// // * reduce hallucination through a powerful combination of deterministic facts and probabilistic conclusions and * enhance explainability and transparency through explicit knowledge representation for getting transparent reasoning and explainable AI
// // * neo4j's scalable and flexible technology seamlessly integrates with generative AI Frameworks like Lang chain vertex AI open Ai and Beyond
// // *  democratizing access to the world's information while setting a new standard for AI accuracy transparency and explainability
// // * unlock new possibilities in generative AI today at


// Chances are, you have ended up enrolling to this course because you have heard some hype around Neo4j and LLMs and you are wondering what the fuss is all about.
// You may also be here

// In this lesson, we will cover the basics of LLMs and how you can reduce their tendency to hallucinate, or make things up, their output by passing data from Neo4j.


// == Large Language Models

// Large Language Models, commonly abbreviated as LLMs, are advanced machine learning models designed to understand, interpret, and generate human-like text based on the input they receive. LLMs are trained on vast datasets, allowing them to have a broad understanding of various subjects, languages, and contexts.

// They can perform a range of tasks, from simple text completion and translation to more complex tasks like content creation, code generation, and question-answering.

// Despite their power, they do have at least one drawback.  They tend to _make things up_.

// == The Hallucination Problem

// At their core, LLMs are trained to predict the next word(s) in a sequence.
// They convert words into tokens, text into complex vector embeddings, lists of numbers that can be compared and contrasted.
// In that sense, the model is unaware of what any one word in a sequence means compared to another.
// It is only aware of the similarity.

// When presented with a question on a subject that wasn't included in the dataset, this prediction model can end up on a tangent and produce inaccurate statements.
// This is known as hallucination.

// This hallucination can occur for several reasons.


// * **Bad Data** - The training data itself may be false, or the LLM may not have the ability to verify the source of the data.
// * **Cut-off Date** - LLMs are computationally expensive to train, and this training takes a long time.  Furthermore, the model itself will not have access to the latest data.
// * **Lack of explainability** - it is next to impossible to work out which of the trillions of parameters led the LLM to produce the result that it did
// * **Prompt Sensitivity** - an LLM may respond differently to different, semantically identical, prompts
// * **Lack of data/enterprise domain knowledge** - When it comes to private data, the LLM will not be aware


// These problems can be solved by:

// * **Prompt Engineering** - Modifying the prompt to answer
// * **In-context learning** - Provide completed examples to the AI as contexts
// * **Fine-tuning** - providing additional training data to better tune the LLM
// * **Grounding** - Provide AI with information to use when generating the response

// Grounding is where a database can help.


== Check Your Understanding

include::questions/1-hallucination.adoc[]
include::questions/2-grounding.adoc[]


[.summary]
== {1:Lesson Summary}

In this lesson, you learned about LLMs, their potential drawbacks and how they can be avoided.

In the next lesson, you will learn how a knowledge graph can be used to help ground the LLM and avoid hallucinations.
