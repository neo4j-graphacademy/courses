= Introduction to Neo4j & GenAI
:order: 1

In this course, we will cover everything you need to know to use Neo4j effectively with Generative AI.
You will learn to all about Large Language Models (LLMs), Knowledge Graphs and how they can be used together effectively.

So we start on the same page, let's first cover the basics.

== What is Neo4j?

Neo4j is a graph database and analytics system that allows us to store, manage, and query highly connected data.
Unlike traditional relational databases, which use tables and rows, Neo4j uses a graph-based model with nodes and relationships.
This makes it particularly well-suited for representing and querying complex, interconnected data.

[TIP]
.New to Neo4j?
To understand more about graph databases in general, link:/courses/neo4j-fundamentals/[check out the Neo4j Fundamentals course^].


== What are Knowledge Graphs?

Knowledge graphs are a specific implementation of a Graph Database, where iinformation is captured and integrated from many different sources, representing the inherent knowledge of a specific domain.
They provide a structured way to represent entities, their attributes, and the relationships between them, allowing for a comprehensive and interconnected understanding of the information within that domain.
This integration from diverse sources allows knowledge graphs to offer a more holistic view and facilitate complex queries, analytics, and insights.

Knowledge graphs are tailored for semantic search, data retrieval, reasoning, and data amalgamation, frequently powering search engines, AI, and research.

Knowledge graphs typically lean on ontologies, offering structured concepts within a domain and their interrelations. Conversely, while Neo4j can engage with ontologies and offers schema constraints, its standard graphs may not consistently utilize or demand structured semantic frameworks.

[TIP]
.Knowledge Graphs and Ontologies
For more on Knowledge Graphs, Ontologies, we recommend watching the
link:https://www.youtube.com/watch?v=NQqWBnyQlS4&list=PL9Hl4pk2FsvX-5QPvwChB-ni_mFF97rCE[Going Meta â€“ A Series on Graphs, Semantics and Knowledge^ series on YouTube].


With their semantic foundation, knowledge graphs can readily adapt and evolve as they assimilate new information and face domain shifts.


== Generative AI & Large Language Models

Generative AI refers to a class of algorithms and models that can generate new content, such as images, text, or even music, in response to user prompting, based on patterns and examples from existing data.
Large Language Models, most commonly referred to as LLMs, learn the underlying structure and distribution of the data and can then generate new samples that resemble the original data.

By leveraging the power of machine learning and deep learning techniques, generative AI models can generate highly realistic and creative outputs.
LLMs are trained on vast amounts of text data to understand and generate human-like text.
They can answer questions, create content, and assist with various linguistic tasks by leveraging patterns learned from the data.


=== Instructing an LLM

The response generated by an LLM is a probabilistic continuation of the instructions it is fed.

To get an LLM to perform a task, you provide it with a **prompt**, a piece of text that should specify your requirements and provide clear instructions on how to respond.
Precision in the task description, potentially combined with examples or context, ensures that the model understands the intent and produces relevant and accurate outputs.

// TODO: Add a picture/flow chart


An example prompt may be a simple question.

[source,role=nocopy]
What is the capital of Japan?

Or, it could be more descriptive. For example:

[source,role=nocopy]
----
Produce a succinct list of talking points exploring the subject of Knowledge Graphs and how they relate to LLMs.
The content should be targetted at Developers and Data Scientists.
Your readers may have English as a second language, so use simple terms and avoid colloquialissm.
Avoid Jargon at all costs.

Return the results as a list of JSON strings containing content formatted in Markdown.
----

The LLM will interpret these instructions and return a response based on the patterns it has learned from its training data.


== Potential Problems

While LLMs provide a lot of potential, they should also be used with caution.

At their core, LLMs are trained to predict the next word(s) in a sequence.
They convert words into tokens, text into complex vector embeddings; arrays of numbers that can be compared and contrasted.

Their primary goal is to be as helpful as possible, even if that means occasionally generating misleading or baseless content, a phenomenon known as **hallucination**.

While LLMs can represent the essence of words and phrases through their internal embeddings, they don't possess a genuine understanding or ethical judgment of the content.
This can lead to outputs that might be biased, devoid of context, or lack logical coherence.


== Fixing Hallucinations

Providing additional data helps in grounding the LLM's responses and making them more accurate.

When integrated with an LLM, a knowledge graph can guide the model to pull from trusted and contextually relevant information, enhancing the accuracy and reliability of its outputs.
This fusion of structured and unstructured data helps in achieving more context-aware, precise, and factually grounded responses.

When you pass contextual information to an LLM, you're primarily leveraging its ability to generate coherent and contextually relevant responses based on the new information provided.

While the LLM uses its language skills to interpret and respond to the context, it will not disregard the original training data.
Instead, the original training data shapes the foundational language skills and general knowledge of the model.

You can think of the original training data as the base knowledge and linguistic capabilities, while the contextual information guides the application of that knowledge to a specific situation.

The combination of both approaches is what enables the LLM to generate meaningful responses.



Throughout this course, we will explore how to leverage the capabilities of Neo4j and Generative AI to build intelligent, context-aware systems.
We will apply the information learned in the course to build a recommendation engine that queries a movie recommendations dataset through a conversational interface built with Langchain.


== Check Your Understanding

include::questions/1-hallucination.adoc[]

include::questions/2-fixing-hallucination.adoc[]


[.summary]
== Lesson Summary

In this lesson, you learned about LLMs, their potential drawbacks and how they can be avoided.

In the next lesson, you will learn how a knowledge graph can be used to help ground the LLM and avoid hallucinations.
