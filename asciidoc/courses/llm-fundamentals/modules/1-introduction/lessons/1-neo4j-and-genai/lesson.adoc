= Introduction to Neo4j & GenAI
:order: 1
:type: lesson

This course will get you started learning about using Neo4j with Generative AI.

You will learn about: 

* Large Language Models (LLMs)
* Knowledge Graphs
* How to use Neo4j for Retrieval Augmented Generation (RAG)
* Integrating Neo4j and LLMs using Python and Langchain

First, let's cover the basics.

== What is Neo4j?

Neo4j is a graph database and analytics system that allows us to store, manage, and query highly connected data.

// TODO - add graph image

Unlike traditional relational databases, which use tables and rows, Neo4j uses a graph-based model with nodes and relationships.

Making Neo4j particularly well-suited for representing and querying complex, interconnected data.

[TIP]
.New to Neo4j?
To learn about graph databases and Neo4j, link:/courses/neo4j-fundamentals/[check out the Neo4j Fundamentals course^].

== What are Knowledge Graphs?

Knowledge graphs are a specific implementation of a Graph Database, where information is captured and integrated from many different sources, representing the inherent knowledge of a particular domain.

They provide a structured way to represent entities, their attributes, and relationships, allowing for a comprehensive and interconnected understanding of the information within that domain.

This integration from diverse sources gives knowledge graphs a more holistic view and facilitates complex queries, analytics, and insights.

Knowledge graphs are tailored for semantic search, data retrieval, reasoning, and data amalgamation, frequently powering search engines, AI, and research.

Knowledge graphs typically lean on ontologies, offering structured concepts within a domain and their interrelations. Conversely, while Neo4j can engage with ontologies and offers schema constraints, its standard graphs may not consistently utilize or demand structured semantic frameworks.

[TIP]
.Knowledge Graphs and Ontologies
For more on Knowledge Graphs, Ontologies, we recommend watching the
link:https://www.youtube.com/watch?v=NQqWBnyQlS4&list=PL9Hl4pk2FsvX-5QPvwChB-ni_mFF97rCE[Going Meta â€“ A Series on Graphs, Semantics and Knowledge^ series on YouTube^].


With their semantic foundation, knowledge graphs can readily adapt and evolve as they assimilate new information and face domain shifts.

== Generative AI & Large Language Models

Generative AI is a class of algorithms and models that can generate new content, such as images, text, or even music, in response to user prompting, based on patterns and examples from existing data.

Large Language Models, most commonly referred to as LLMs, learn the underlying structure and distribution of the data and can then generate new samples that resemble the original data.

Generative AI models can generate highly realistic and creative outputs by leveraging the power of machine learning and deep learning techniques. LLMs are trained on vast amounts of text data to understand and generate human-like text.

They can answer questions, create content, and assist with various linguistic tasks by leveraging patterns learned from the data.

=== Instructing an LLM

The response generated by an LLM is a probabilistic continuation of the instructions it receives. The LLM provides the most likely response based on the patterns it has learned from its training data. 

To get an LLM to perform a task, you provide a **prompt**, a piece of text that should specify your requirements and provide clear instructions on how to respond.
Precision in the task description, potentially combined with examples or context, ensures that the model understands the intent and produces relevant and accurate outputs.

// TODO: Add a picture/flow chart

An example prompt may be a simple question.

    What is the capital of Japan?

Or, it could be more descriptive. For example:

    Produce a brief list of talking points exploring the subject of Knowledge Graphs and how they relate to LLMs.

    The content should be targeted at Developers and Data Scientists.
    
    Your readers may have English as a second language, so use simple terms and avoid colloquialisms.

    Avoid Jargon at all costs.

    Return the results as a list of JSON strings containing content formatted in Markdown.

The LLM will interpret these instructions and return a response based on the patterns it has learned from its training data.

== Potential Problems

While LLMs provide a lot of potential, you should also be cautious.

At their core, LLMs are trained to predict the following word(s) in a sequence.

The words that come next are based on the patterns and relationships from other text in the training data. The sources for this training data are often the internet, books, and other publicly available text. This data could be of questionable quality and maybe be incorrect. Training happens at a point in time, it may not reflect the current state of the world and would not include any private information.

LLMs are fine-tuned to be as helpful as possible, even if that means occasionally generating misleading or baseless content, a phenomenon known as **hallucination**.

While LLMs can represent the essence of words and phrases, they don't possess a genuine understanding or ethical judgment of the content.

These factors can lead to outputs that might be biased, devoid of context, or lack logical coherence.


== Fixing Hallucinations

Providing additional data helps in grounding the LLM's responses and making them more accurate.

When integrated with an LLM, a knowledge graph can guide the model to pull from trusted and contextually relevant information, enhancing the accuracy and reliability of its outputs.

This fusion of structured and unstructured data helps achieve more context-aware, precise, and factually grounded responses.

When you pass contextual information to an LLM, you primarily leverage its ability to generate coherent and contextually relevant responses based on the new information provided.

While the LLM uses its language skills to interpret and respond to the context, it will not disregard the original training data.

Instead, the original training data shapes the foundational language skills and general knowledge.

You can think of the original training data as the base knowledge and linguistic capabilities, while the contextual information provides guidance to specific situations.

The combination of both approaches enables the LLM to generate more meaningful responses.

Throughout this course, you will explore how to leverage the capabilities of Neo4j and Generative AI to build intelligent, context-aware systems.

You will apply the information and skills learned in the course to build an engine that provides recommendations and information about movies and people.


== Check Your Understanding

include::questions/1-hallucination.adoc[]

include::questions/2-fixing-hallucination.adoc[]


[.summary]
== Lesson Summary

In this lesson, you learned about LLMs, their potential drawbacks and how they can be avoided.

In the next lesson, you will learn how a knowledge graph can be used to help ground the LLM and avoid hallucinations.
