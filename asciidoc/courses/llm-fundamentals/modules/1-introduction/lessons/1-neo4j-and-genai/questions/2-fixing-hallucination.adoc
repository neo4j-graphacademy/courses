[.question]
= 2. LLM Hallucinations

Why might an LLM produce outputs that manifest as generating untrue facts or nonsensical explanations?

* [ ] LLMs are not designed to generate human-like text.
* [x] LLMs rely on patterns and sometimes overfit to the data they've been trained on.
* [ ] LLMs always provide accurate and factual information.
* [ ] LLMs are designed to be unpredictable.


[TIP,role=hint]
.Hint
====
Remember that LLMs rely heavily on patterns from their training data.
====

[TIP,role=solution]
.Solution
====
LLMs rely on patterns and sometimes overfit to the data they've been trained on.
====
