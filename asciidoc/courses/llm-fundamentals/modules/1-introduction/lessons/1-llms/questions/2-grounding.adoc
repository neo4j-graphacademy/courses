[.question]
= 2. Fixing Hallucination

Which of the below methods can be used to help avoid hallucinations?

* [*] Grounding
* [*] Prompt Engineering
* [*] Providing Few-shot Examples
* [ ] Retraining the model


[TIP,role=hint]
.Hint
====
Three of the four options above are valid options to avoid hallucination.
====

[TIP,role=solution]
.Solution
====
You can avoid hallucination by **engineering the prompt** passed to the LLM,  **providing examples** as part of the prompt and **grounding** the LLM with data from another source, such as a database.
====