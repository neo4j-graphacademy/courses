= Introduction to LLMs
:order: 1


// === Temperature

// The temperature variable passed to an LLM controls the randomness of the prediction.
// The higher the temperature, the wilder the suggesti2on may become.


// === Training Cut-off Date

// LLMs are computationally expensive to train

// GPT 3.5 was trained up until 28 August 2023.



// * for over a decade neo4j has been helping the world make sense of data
// * today neo4j the leader in graph database and analytics technology is unlocking new possibilities in generative AI
// * together, neo4j and large language models allow you to use a public pre-trained llm with your own data subject to your privacy controls
// * reduce hallucination through a powerful combination of deterministic facts and probabilistic conclusions and * enhance explainability and transparency through explicit knowledge representation for getting transparent reasoning and explainable AI
// * neo4j's scalable and flexible technology seamlessly integrates with generative AI Frameworks like Lang chain vertex AI open Ai and Beyond
// *  democratizing access to the world's information while setting a new standard for AI accuracy transparency and explainability
// * unlock new possibilities in generative AI today at


Chances are, you have ended up enrolling to this course because you have heard some hype around Neo4j and LLMs and you are wondering what the fuss is all about.
You may also be here

In this lesson, we will cover the basics of LLMs and how you can reduce their tendency to hallucinate, or make things up, their output by passing data from Neo4j.


== Large Language Models

Large Language Models, commonly abbreviated as LLMs, are advanced machine learning models designed to understand, interpret, and generate human-like text based on the input they receive. LLMs are trained on vast datasets, allowing them to have a broad understanding of various subjects, languages, and contexts.

They can perform a range of tasks, from simple text completion and translation to more complex tasks like content creation, code generation, and question-answering.

Despite their power, they do have at least one drawback.  They tend to _make things up_.

== The Hallucination Problem

At their core, LLMs are trained to predict the next word(s) in a sequence.
They convert words into tokens, text into complex vector embeddings, lists of numbers that can be compared and contrasted.
In that sense, the model is unaware of what any one word in a sequence means compared to another.
It is only aware of the similarity.

When presented with a question on a subject that wasn't included in the dataset, this prediction model can end up on a tangent and produce inaccurate statements.
This is known as hallucination.

This hallucination can occur for several reasons.


* **Bad Data** - The training data itself may be false, or the LLM may not have the ability to verify the source of the data.
* **Cut-off Date** - LLMs are computationally expensive to train, and this training takes a long time.  Furthermore, the model itself will not have access to the latest data.
* **Lack of explainability** - it is next to impossible to work out which of the trillions of parameters led the LLM to produce the result that it did
* **Prompt Sensitivity** - an LLM may respond differently to different, semantically identical, prompts
* **Lack of data/enterprise domain knowledge** - When it comes to private data, the LLM will not be aware


These problems can be solved by:

* **Prompt Engineering** - Modifying the prompt to answer
* **In-context learning** - Provide completed examples to the AI as contexts
* **Fine-tuning** - providing additional training data to better tune the LLM
* **Grounding** - Provide AI with information to use when generating the response

Grounding is where a database can help.


== Check Your Understanding

include::questions/1-hallucination.adoc[]
include::questions/2-grounding.adoc[]


[.summary]
== {1:Lesson Summary}

In this lesson, you learned about LLMs, their potential drawbacks and how they can be avoided.

In the next lesson, you will learn how a knowledge graph can be used to help ground the LLM and avoid hallucinations.
