= Vectors & Semantic Search

// TODO: Rewrite
In the last lesson, you learned about the importance of grounding to improve LLM accuracy and the concept of **Retrieval Augmented Generation** (RAG).
RAG involves providing additional information along with a prompt to help the LLM form a response.

One of the big challenges of RAG is understanding what the user is asking for and surfacing the correct information to pass to the LLM.


== Semantic Search vs. Traditional Search

At its core, semantic search aims to understand the intent and contextual meaning of search phrases, rather than just focusing on individual keywords.
Traditional search often depends on exact match keywords or proximity-based algorithms to return results.

For example, if you input "apple" in a traditional search, you might predominantly get results about the fruit.
However, in a semantic search, the engine tries to gauge the context: Are you searching about the fruit, the tech company, or something else?

The results are tailored not just based on the term but also the perceived intent.

== Vectors and embeddings


In natural language processing (NLP) and machine learning, words or phrases are often converted into numerical representations known as **vectors**.
Each dimension in a vector can represent a particular semantic aspect of the word or phrase.
With advanced models, these embeddings also contain contextual information.
This means the vector for a word can change based on its surrounding context. For instance, the word _bank_ will have a different vector in _river bank_ than in _savings bank_.
Semantic search systems can use these contextual embeddings to understand user intent.


[INFO]
.Creating Vector Embeddings
====
All LLM providers expose API endpoints that covert a _chunk_ of text into a vector embedding.
Depending on the provider, the shape of the vector may differ.

OpenAI's `text-embedding-ada-002` embedding model converts text into a vector consisting of 1,536 dimensions.
====

The _distance_ or _angle_ between vectors can be used to gauge the semantic similarity between words or phrases.
Words with similar meanings or contexts will have vectors that are close together in the high-dimensional vector space, while unrelated words will be farther apart.
This principle is employed in semantic search to find results that are contextually relevant to a user's query.

When a user submits a query, their input is converted into a vector representation.
This vector representation will be compared to an existing index of vectors to find the most similar text in terms of context and meaning.


== Vectors and Neo4j

Underlying vectors serve as the backbone of semantic search, enabling it to understand and represent the complex, multi-dimensional nature of language, context, and meaning. These vectors facilitate the matching of user queries with relevant and contextually appropriate information.

Since the v5.11 release, Neo4j has a link:https://neo4j.com/docs/cypher-manual/current/indexes-for-vector-search/[Vector search index^], so you don't necessarily need to use an additional.

In the next lesson, you will learn how you can leverage the data already stored in Neo4j to perform semantic search.


== Check Your Understanding

include::questions/1-semantic-vs-traditional.adoc[leveloffset=+1]
include::questions/2-vector-role.adoc[leveloffset=+1]
include::questions/3-query-index.adoc[leveloffset=+1]


[.summary]
== Lesson Summary

Semantic search prioritizes user intent and context over mere keyword matching, a departure from traditional search methods. Vectors, representing data numerically, are pivotal in facilitating this advanced search mechanism and are integral to the operations of contemporary machine learning algorithms like LLMs.


In the next lesson, you will learn how to implement semantic search in Neo4j using Vector Indexes.
