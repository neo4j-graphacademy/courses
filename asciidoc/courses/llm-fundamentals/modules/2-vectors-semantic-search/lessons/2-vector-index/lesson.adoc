= Vector Indexes
:order: 2
:type: lesson
:sandbox: true

In the last lesson, you learned about vectors and their role in Semantic Search.

In this lesson, you will learn how to create vector embeddings of text content in an existing Neo4j database.

== Vectorizing Movie Plots

When you enrolled in this course, a Movie Recommendation Sandbox was created. This database consists of over 9000 movies, 15000 actors, and just over 100000 user ratings.

Each movie has a `.plot` property.

.Movie Plot Example
[source,cypher]
MATCH (m:Movie {title: "Toy Story"})
RETURN m.title AS title, m.plot AS plot

    "A cowboy doll is profoundly threatened and jealous when a new spaceman figure supplants him as top toy in a boy's room."

You can use the vector index to find the most similar movies by converting this text into a vector embedding. 

You will use a pre-created link:https://data.neo4j.com/llm-fundamentals/openai-embeddings.csv[CSV file of 1000 movie plot vector embeddings^] in this lesson.

The CSV file contains:

* `movieId` - The ID of the movie
* `embedding` - The vector embedding of the movie plot generated by OpenAI

[source,csv]
----
movieId, embedding
1, [-0.0271058, -0.0242211, 0.0060390322, -0.02437703, ...]
2, [-0.001596838, -0.022397375, 0.0046575777, 0.0019427929, ...]
----

[TIP]
.Generating the Embeddings
====
link:https://platform.openai.com/docs/guides/embeddings/what-are-embeddings[OpenAI's text-embedding-ada-002 model^] was used to create the embeddings. It is a cost-effective model that can generate embeddings for text.

A simple Python script calls the embeddings endpoint served by OpenAI. The link:https://github.com/neo4j-graphacademy/llm-fundamentals/blob/main/openai_embeddings.py[code^] is available in the link:https://github.com/neo4j-graphacademy/llm-fundamentals[github.com/graphacademy/llm-fundamentals^] repository.
====

For this lesson, it doesn't necessarily matter how these embeddings were obtained, as each LLM will provide an embedding in its own shape.

== Creating the Vector Index

You will save the embeddings to a `.embedding` property on the `(:Movie)` node.

You will need to create a vector index to search across these embeddings.

You will use the `db.index.vector.createNodeIndex()` procedure to create the index:

.db.index.vector.createNodeIndex Syntax
[source,cypher, role=noplay nocopy]
----
CALL db.index.vector.createNodeIndex(
    indexName :: STRING, 
    label :: STRING, 
    propertyKey :: STRING, 
    vectorDimension :: INTEGER, 
    vectorSimilarityFunction :: STRING)
----

`db.index.vector.createNodeIndex()` expects the following parameters:

* `indexName` - The name of the index
* `label` - The node label on which to index
* `propertyKey` - The property key on which to index
* `vectorDimension` - The dimension of the embedding e.g. OpenAI embeddings consist of `1536` dimensions.
* `vectorSimilarityFunction` - The similarity function to use when comparing values in this index - this can be `euclidean` or `cosine`.

Review and run the following Cypher to create the vector index:

.Create the vector index
[source,cypher]
----
CALL db.index.vector.createNodeIndex(
    'moviePlots',
    'Movie',
    'embedding',
    1536,
    'cosine'
)
----

Note that the index is called `moviePlots`, it is against the `Movie` label, and it is on the `.embedding` property. The `vectorDimension` is `1536` (as used by OpenAI) and the similarity function is `cosine`.

[TIP]
.Choosing a Similarity Function
====
Generally, cosine will perform best for text embeddings, but you may want to experiment with other functions.

You can link:https://neo4j.com/docs/cypher-manual/current/indexes-for-vector-search/#indexes-vector-similarity[read more about similarity functions in the documentation^].

Typically, you will choose a similarity function closest to the loss function used when training the embedding model. You should refer to the model's documentation for more information.
====

=== Check the index creation status

Check that you created the index successfully using the `SHOW INDEXES` command.

.Show Indexes
[source,cypher]
----
SHOW INDEXES  YIELD id, name, type, state, populationPercent WHERE type = "VECTOR"
----

You should see a result similar to the following:

.Show Indexes Result
|===
| id | name | type | state | populationPercent

|1 | "moviePlots" | "VECTOR" | "ONLINE" | `100.0`
|===

Once the `state` is listed as online, the index will be ready to query.

The `populationPercentage` field indicates the proportion of node and property pairing.

== Loading Embeddings

You will use the `LOAD CSV` command to load the embeddings into the Neo4j Sandbox instance.

The following Cypher loads the embeddings CSV file, performs a `MATCH` query to find the `(:Movie)` node with the corresponding `movieId` property, and then sets the `.embedding` property on that node.

Review this Cypher statement before running it.

.Loading the Embeddings
[source,cypher]
----
LOAD CSV WITH HEADERS
FROM 'https://data.neo4j.com/llm-fundamentals/openai-embeddings.csv'
AS row
MATCH (m:Movie {movieId: row.movieId})
CALL db.create.setNodeVectorProperty(m, 'embedding', apoc.convert.fromJsonList(row.embedding))
RETURN count(*)
----

The statement:

* Loads the CSV file
* Matches the `(:Movie)` node with the corresponding `movieId` property
* Calls `db.create.setNodeVectorProperty()` procedure to set the `embedding` property
* The procedure also validates that the property is a valid vector


[TIP]
.LOAD CSV and Strings
====
When data is loaded using `LOAD CSV`, it is treated as a string unless specifically cast using a specific function, for example, `toInteger()` or `toFloat()`.

In this case, the embedding needs to be coerced into a JSON list link:https://neo4j.com/docs/apoc/current/overview/apoc.convert/apoc.convert.fromJsonList/[using the `apoc.convert.fromJsonList()` function^].

You can link:https://graphacademy.neo4j.com/courses/importing-cypher/[learn how to use the `LOAD CSV` command in the Importing CSV Data into Neo4j course].
====

The index will be updated asynchronously. You can check the status of the index population using the `SHOW INDEXES` statement:

[source,cypher]
----
SHOW INDEXES  YIELD id, name, type, state, populationPercent WHERE type = "VECTOR"
----

When the `populationPercent` is `100.0`, all the movie embeddings have been indexed.

== Querying Vector Indexes

You can query the index using the `db.index.vector.queryNodes()` procedure.

The procedure returns the requested number of approximate nearest neighbor nodes and their similarity score, ordered by the score.

[source,cypher,rel=norun]
.Querying a Vector Index
----
CALL db.index.vector.queryNodes(
    'moviePlots',
    10,
    $embedding
) YIELD node, score
----

The procedure accepts three parameters:

1. The name of the vector index
2. The number of results to return
3. A list of `float`s that represent an embedding

The procedure yields two arguments; a `node` and a similarity `score` ranging from `0.0` to `1.0`.

// TODO - MH comment - should we also show how to search for a user question embedding? should we prepare a few in a file?

You can use this procedure to find the closest embedding value to a given embedding.

For example, find movies with a similar plot to another.

Review this Cypher before running it.

[source,cypher]
.Similar Plots
----
MATCH (m:Movie {title: 'Toy Story'})
WITH m LIMIT 1

CALL db.index.vector.queryNodes('moviePlots', 6, m.embedding)
YIELD node, score

RETURN node.title AS title, node.plot AS plot, score
----

The query finds the _Toy Story_ `Movie` node and uses the `.embedding` property to find the most similar plots. The `db.index.vector.queryNodes()` procedure uses the `moviePlots` vector index to find similar embeddings.

The procedure returns the requested number of approximate nearest neighbor nodes and their similarity score, ordered by the score.

.Similar Plots Results
|===
| title | plot | score
| "Toy Story" | "A cowboy doll is profoundly threatened and jealous when a new spaceman figure supplants him as top toy in a boy's room." | 1.0
| "Little Rascals, The" | "Alfalfa is wooing Darla and his He-Man-Woman-Hating friends attempt to sabotage the relationship." | 0.9214372634887695
| "NeverEnding Story III, The" | "A young boy must restore order when a group of bullies steal the magical book that acts as a portal between Earth and the imaginary world of Fantasia." | 0.9206198453903198
|  "Drop Dead Fred" | "A young woman finds her already unstable life rocked by the presence of a rambunctious imaginary friend from childhood." | 0.9199690818786621
| "E.T. the Extra-Terrestrial" | "A troubled child summons the courage to help a friendly alien escape Earth and return to his home-world." | 0.919100284576416
| "Gumby: The Movie" | "In this offshoot of the 1950s claymation cartoon series, the crazy Blockheads threaten to ruin Gumby's benefit concert by replacing the entire city of Clokeytown with robots." | 0.9180967211723328
|===

The similarity score is between `0.0` and `1.0`, with `1.0` being the most similar. Note how the most similar plot is that of the _Toy Story_ movie itself!

== Considerations

As you can see, this approach is relatively straightforward and can quickly yield results. The downside to this approach is that it relies heavily on the embeddings and similarity function to produce valid results.

This approach is also a black box - with 1536 dimensions, it would be impossible to determine how the vectors are structured and what factors were considered when calculating the similarity. 

The movies returned look similar, but you would have no way of verifying that the results are correct.

== Check your understanding

include::questions/1-create-index.adoc[leveloffset=+1]
include::questions/2-query-index.adoc[leveloffset=+1]

[.summary]
== Lesson Summary

In this lesson, you learned how to create, populate, and use a Vector index in Neo4j.

In the next lesson, you will learn how to use feedback to improve the suggestions provided by Semantic Search.
