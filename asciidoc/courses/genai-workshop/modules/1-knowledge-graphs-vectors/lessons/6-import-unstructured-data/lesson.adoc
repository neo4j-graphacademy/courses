= Import Unstructured Data
:order: 6
:type: challenge
:sandbox: false
:branch: main
:slides: true

[.slide.discrete]
== Unstructured data

Unstructured data is often rich in information but challenging to analyze.
Graphs are a powerful tool for representing unstructured data, and vectors and embeddings can help you identify similarities and search for related data.

You will use Python and LangChain to create embeddings and load the unstructured content into a Neo4j graph database.

You will load the content from the GraphAcademy course link:https://graphacademy.neo4j.com/courses/llm-fundamentals/[Neo4j & LLM Fundamentals^].

image:https://cdn.graphacademy.neo4j.com/courses/llm-fundamentals/banner.png[Neo4j & LLM Fundamentals course banner, width=400, align=center]

[.slide]
== The course content

The `1-knowledge-graphs-vectors\data` directory in the workshop repository contains the course data.

Open the directory and note the following structure:

* `asciidoc` - the course content in asciidoc format
** `courses` - the course content
*** `llm-fundamentals` - the course name
**** `modules` - numbered directories for each module
***** `01-name` - the module name
****** `lessons` - numbered directories for each lesson
******* `01-name` - the lesson name
******** `lesson.adoc` - the lesson content

[.slide]
== Load the content and chunk it

You will load the content and _chunk_ it using Python and LangChain.

[.transcript-only]
====
[%collapsible]
.More on chunking
=====
When dealing with large amounts of data, breaking it into smaller, more manageable parts is helpful. This process is called chunking.

Smaller pieces of data are easier to work with and process. Embedding models also have size (token) limits and can only handle a certain amount of data.

Embedding large amounts of text may also be less valuable. For example, if you are trying to find a document that references a specific topic, the meaning maybe lost in the whole document. Instead, you may only need the paragraph or sentence that contains the relevant information. Conversely, small amounts of data may not contain enough context to be useful.

There are countless strategies for splitting data into chunks, and the best approach depends on the data and the problem you are trying to solve.
=====
====

Open the `1-knowledge-graphs-vectors/create_vector.py` file and review the program.

[.transcript-only]
====
[source,python]
----
include::{repository-raw}/{branch}/1-knowledge-graphs-vectors/create_vector.py[]
----
====

The program uses the link:https://api.python.langchain.com/en/latest/document_loaders/langchain_community.document_loaders.directory.DirectoryLoader.html[`DirectoryLoader`^] class to load the content from the `data/asciidoc` directory.

Your task is to add the code to:

. Create a link:https://api.python.langchain.com/en/latest/text_splitter/langchain.text_splitter.CharacterTextSplitter.html[`CharacterTextSplitter`^] object to split the content into chunks of text.
. Use the `split_documents` method to split the documents into chunks of text based on the existence of `\n\n` and a chunk size of 1500 characters.

[.slide.discrete]
== Create the text splitter
Create the CharacterTextSplitter object to split the content into paragraphs (`\n\n`).

[source,python]
----
include::{repository-raw}/{branch}/1-knowledge-graphs-vectors/solutions/create_vector.py[tag=import_splitter]

include::{repository-raw}/{branch}/1-knowledge-graphs-vectors/solutions/create_vector.py[tag=splitter]
----

The `text_splitter` will create chunks of text, around 1500 characters long, each containing one or more paragraphs. 

[.slide.discrete]
== Split the documents
Split the documents into chunks of text.

[source,python]
----
include::{repository-raw}/{branch}/1-knowledge-graphs-vectors/solutions/create_vector.py[tag=split]
----

[TIP]
You can run your code now to see the chunks of text.

[.transcript-only]
====
[%collapsible]
.More on splitting
=====
The content isn't split simply by a character (`\n\n`) or on a fixed number of characters.
The process is more complicated. 
Chunks should be up to maximum size but conform to the character split.

In this example, the `split_documents` method does the following:

. Splits the documents into paragraphs (using the `separator` - `\n\n`)
. Combines the paragraphs into chunks of text that are up 1500 characters (`chunk_size`)
** if a single paragraph is longer than 1500 characters, the method will not split the paragraph but create a chunk larger than 1500 characters
. Adds the last paragraph in a chunk to the start of the next paragraph to create an overlap between chunks. 
** if the last paragraph in a chunk is more than 200 characters (`chunk_overlap`) it will *not* be added to the next chunk

This process ensures that:

* Chunks are never too small.
* That a paragraph is never split between chunks.
* That chunks are significantly different, and the overlap doesn't result in a lot of repeated content.

Investigate what happens when you modify the `separator`, `chunk_size` and `chunk_overlap` parameters.
=====
====

[.slide]
== Create vector index

Once you have chunked the content, you can use the LangChain link:https://python.langchain.com/docs/integrations/vectorstores/neo4jvector[`Neo4jVector`^] class to create *embeddings*, a *vector index*, and *store the chunks* in a Neo4j graph database.

You will need to modify your Python program to:

. Connect to the Neo4j database.
. Create an embedding provider.
. Create the nodes and vector index.

[.slide.discrete]
== Connect
Connect to the Neo4j database:

[source,python]
----
include::{repository-raw}/{branch}/1-knowledge-graphs-vectors/solutions/create_vector.py[tag=import_graph]

include::{repository-raw}/{branch}/1-knowledge-graphs-vectors/solutions/create_vector.py[tag=graph]
----

[.slide.discrete]
== Embeddings
Create an embedding provider:

[source, python]
----
include::{repository-raw}/{branch}/1-knowledge-graphs-vectors/solutions/create_vector.py[tag=import_embedding]

include::{repository-raw}/{branch}/1-knowledge-graphs-vectors/solutions/create_vector.py[tag=embedding]
----

[.slide.discrete.col-2]
== Create
[.col]
====
Create the nodes and vector index:

[source, python]
----
include::{repository-raw}/{branch}/1-knowledge-graphs-vectors/solutions/create_vector.py[tag=import_vector]

include::{repository-raw}/{branch}/1-knowledge-graphs-vectors/solutions/create_vector.py[tag=vector]
----
====

[.col]
====
The code will create `Chunk` nodes with `text` and `embedding` properties and a vector index called `chunkVector`.

[.transcript-only]
=====
[%collapsible]
.View the complete code
======
[source]
----
include::{repository-raw}/{branch}/1-knowledge-graphs-vectors/solutions/create_vector.py[tag=**]
----
======
=====

Run the program to create the chunk nodes and vector index. 

It may take a minute or two to complete.
====

[.slide]
== View chunks in the sandbox

You can now view the chunks in the Neo4j sandbox.

[source,cypher]
----
MATCH (c:Chunk) RETURN c LIMIT 25
----

[.slide.discrete]
== Query the vector index
You can also query the vector index to find similar chunks.
For example, you can find lesson chunks relating to a specific question, "What does Hallucination mean?":

[source,cypher]
----
WITH genai.vector.encode(
    "What does Hallucination mean?", 
    "OpenAI", 
    { token: "sk-..." }) AS userEmbedding
CALL db.index.vector.queryNodes('chunkVector', 6, userEmbedding)
YIELD node, score
RETURN node.text, score
----

[.transcript-only]
====
[IMPORTANT]
=====
Remember to replace `sk-...` with your OpenAI API key.
=====
====

Experiment with different questions and see how the vector index can find similar chunks.

[.next]
== Continue

When you are ready, you can move on to the next task.

read::Move on[]

[.summary]
== Summary

You learned to use Python and LangChain to load, chunk, and vectorize unstructured data into a Neo4j graph database.