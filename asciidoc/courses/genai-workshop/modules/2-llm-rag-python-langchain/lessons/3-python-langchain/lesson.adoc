= Using Python and LangChain
:order: 3
:type: challenge
:branch: main
:slides: true

[.slide]
== Invoke an LLM

You will use Python and link:https://www.langchain.com/[LangChain^] to get responses from an LLM.

Open and run the `2-llm-rag-python-langchain\llm_invoke.py` program.

[.transcript-only]
====
.llm_invoke.py
[source, python]
----
include::{repository-raw}/{branch}/2-llm-rag-python-langchain/llm_invoke.py[]
----
====

This program will use LangChain to invoke (call) the OpenAI LLM and print the response.

The phrase "What is Neo4j?" is passed to the LLM and the response is printed.

[.transcript-only]
====
You should see something similar to:

_"Neo4j is a highly scalable, native graph database that is designed to store, process, and query large networks of highly connected data. It is based on the property graph model, which allows for the representation of complex relationships between data entities. Neo4j is known for its fast performance and ability to handle complex queries efficiently, making it a popular choice for applications that require real-time data processing and analysis. It is commonly used for a variety of use cases, such as social networks, fraud detection, recommendation engines, and network and IT operations."_
====

Try changing the phrase and rerun the program.

[.slide]
== Prompts

Prompt templates allow you to create reusable instructions or questions. 

This prompt template would give context to the LLM and instruct it to respond as a cockney fruit and vegetable seller.

[source, python]
----
"""
You are a cockney fruit and vegetable seller.
Your role is to assist your customer with their fruit and vegetable needs.
Respond using cockney rhyming slang.

Tell me about the following fruit: {fruit}
"""
----

You can define parameters within the template using braces `{}` e.g. `{fruit}`. These parameters will be replaced with values when the prompt is formatted.

[.slide.discrete.col-2]
== Create a Prompt Template

[.col]
====
Open the `2-llm-rag-python-langchain\llm_prompt.py` file.

[.transcript-only]
=====
.llm_prompt.py
[source, python]
----
include::{repository-raw}/{branch}/2-llm-rag-python-langchain/llm_prompt.py[]
----
=====

Modify the program to create a prompt template:

[source, python]
----
include::{repository-raw}/{branch}/2-llm-rag-python-langchain/solutions/llm_prompt.py[tag=import_prompt]

include::{repository-raw}/{branch}/2-llm-rag-python-langchain/solutions/llm_prompt.py[tag=template]
----
====

[.col]
====
Call the LLM, passing the formatted prompt template as the input:

[source,python]
----
include::{repository-raw}/{branch}/2-llm-rag-python-langchain/solutions/llm_prompt.py[tag=invoke]
----

You use the `format` method to pass the parameters to the prompt e.g. `fruit="apple"`. 

[.transcript-only]
=====
The input variables will be validated when the prompt is formatted, and a `KeyError` will be raised if any variables are missing from the input.

[%collapsible]
.Click to view the complete code
======
[source]
----
include::{repository-raw}/{branch}/2-llm-rag-python-langchain/solutions/llm_prompt.py[tag=**]
----
======
=====
====

[.slide.discrete]
== PromptTemplate.format

When the program is run, the prompt is formatted using the parameters:

    You are a cockney fruit and vegetable seller.
    Your role is to assist your customer with their fruit and vegetable needs
    Respond using cockney rhyming slang.
    Tell me about the following fruit: apple

The LLM will then create a response based on the formatted prompt:

    Well, apples is a right corker - they come in all shapes and sizes from 
    Granny Smiths to Royal Galas. Got 'em right 'ere, two a penny - come and
    grab a pick of the barrel!

[.transcript-only]
====
[NOTE]
.Differing Results
=====
If you run the program multiple times, you will notice you get different responses because the LLM is generating the answer each time.
=====
====

Create a new prompt template and use it to get a response from the LLM.

[.slide]
== Chains

Chains are reusable components that allow you to combine language models with different data sources and third-party APIs.
You can combine a prompt and llm into a chain to create a reusable component.

The simplest chain combines a prompt template with an LLM and returns a response.

You can create a chain using LangChain Expression Language (LCEL).
LCEL is a declarative way to chain LangChain components together.

Components are chained together using the `|` operator.

[source, python, role=nocopy noplay]
----
chain = prompt | llm
----

[.slide.discrete]
== Create a Chain

Open the `2-llm-rag-python-langchain\llm_chain.py` file 

Review the code and run the program.

[.transcript-only]
====
.llm_chain.py
[source,python]
----
include::{repository-raw}/{branch}/2-llm-rag-python-langchain/llm_chain.py[]
----
====

The output from the chain is typically a string, and you can specify an link:https://python.langchain.com/docs/modules/model_io/output_parsers/[output parser^] to parse the output.

Adding a `StrOutputParser` to the chain would ensure a string is returned.

[source,python]
----
include::{repository-raw}/{branch}/2-llm-rag-python-langchain/solutions/llm_chain_string.py[tag=import_str_parser]

include::{repository-raw}/{branch}/2-llm-rag-python-langchain/solutions/llm_chain_string.py[tag=llm_chain]
----

[.slide.discrete.col-2]
== Output parsers

[.col]
====
You can change the prompt to instruct the LLM to return a specific output type. 

For example, specifying `Output JSON` in a specific format:

[source,python]
----
include::{repository-raw}/{branch}/2-llm-rag-python-langchain/solutions/llm_chain_json.py[tag=prompt]
----
====

[.col]
====

You can ensure LangChain parses the response as JSON by specifying `SimpleJsonOutputParser` as the `output_parser`:

[source,python]
----
include::{repository-raw}/{branch}/2-llm-rag-python-langchain/solutions/llm_chain_json.py[tag=import_json_parser]

include::{repository-raw}/{branch}/2-llm-rag-python-langchain/solutions/llm_chain_json.py[tag=llm_chain]
----

Experiment with the prompt to see how the response changes.
====

[.next]
== Continue

When you are ready, you can move on to the next task.

read::Move on[]

[.summary]
== Lesson Summary

You learned how to invoke an LLM using Python and LangChain.

Next, you will learn about strategies for grounding an LLM.