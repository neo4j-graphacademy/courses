= Using Python and LangChain
:order: 3
:type: challenge

You will use Python and link:https://www.langchain.com/[LangChain^] to get responses from an LLM.

== Invoke an LLM

Open the `2-llm-rag-python-langchain\llm_invoke.py` file.

You will see the following code:

.llm_invoke.py
[source, python]
----
include::{repository-raw}/main/2-llm-rag-python-langchain/llm_invoke.py[]
----

This program will use LangChain to invoke (call) the OpenAI LLM and print the response.

The phrase "What is Neo4j?" is passed to the LLM. 
Run the program and observe the response.

You should see something similar to:

_"Neo4j is a highly scalable, native graph database that is designed to store, process, and query large networks of highly connected data. It is based on the property graph model, which allows for the representation of complex relationships between data entities. Neo4j is known for its fast performance and ability to handle complex queries efficiently, making it a popular choice for applications that require real-time data processing and analysis. It is commonly used for a variety of use cases, such as social networks, fraud detection, recommendation engines, and network and IT operations."_

Try changing the phrase and rerun the program.

== Prompts

Prompt templates allow you to create reusable instructions or questions. 

Below is an example of a prompt template:

[source, python]
----
"""
You are a cockney fruit and vegetable seller.
Your role is to assist your customer with their fruit and vegetable needs.
Respond using cockney rhyming slang.

Tell me about the following fruit: {fruit}
"""
----

This prompt template would give context to the LLM and instruct it to respond as a cockney fruit and vegetable seller.

You can define parameters within the template using braces `{}` e.g. `{fruit}`. These parameters will be replaced with values when the prompt is formatted.

Open the `2-llm-rag-python-langchain\llm_prompt.py` file.

.llm_prompt.py
[source, python]
----
include::{repository-raw}/main/2-llm-rag-python-langchain/llm_prompt.py[]
----

Modify the program to create a prompt template:

[source, python]
----
include::{repository-raw}/main/2-llm-rag-python-langchain/solutions/llm_prompt.py[tag=import_prompt]

include::{repository-raw}/main/2-llm-rag-python-langchain/solutions/llm_prompt.py[tag=template]
----

Call the LLM, passing the formatted prompt template as the input:

[source,python]
----
include::{repository-raw}/main/2-llm-rag-python-langchain/solutions/llm_prompt.py[tag=invoke]
----

[%collapsible]
.Click to view the complete code
====
[source]
----
include::{repository-raw}/main/2-llm-rag-python-langchain/solutions/llm_prompt.py[]
----
====

You use the `format` method to pass the parameters to the prompt e.g. `fruit="apple"`. The input variables will be validated when the prompt is formatted, and a `KeyError` will be raised if any variables are missing from the input.

The prompt will be formatted as follows:

    You are a cockney fruit and vegetable seller.
    Your role is to assist your customer with their fruit and vegetable needs
    Respond using cockney rhyming slang.
    Tell me about the following fruit: apple

When running the program, you should see a response similar to:

    Well, apples is a right corker - they come in all shapes and sizes from Granny Smiths to Royal Galas. Got 'em right 'ere, two a penny - come and grab a pick of the barrel!

[NOTE]
.Differing Results
If you run the program multiple times, you will notice you get different responses because the LLM is generating the answer each time.

Before moving on, create a new prompt template and use it to get a response from the LLM.

[TIP]
.Creating PromptTemplates
You can create a prompt from a string by calling the `PromptTemplate.from_template()` static method or load a prompt from a file using the `PromptTemplate.from_file()` static method.

== Chains

Chains are reusable components that allow you to combine language models with different data sources and third-party APIs.

The simplest chain is an `LLMChain`. An `LLMChain` combines a prompt template with an LLM and returns a response.

You can combine a prompt and llm into a chain to create a reusable component.

Open the `2-llm-rag-python-langchain\llm_chain.py` file.

.llm_chain.py
[source,python]
----
include::{repository-raw}/main/2-llm-rag-python-langchain/llm_chain.py[]
----

Run the program and note that the `response` is a dictionary containing the template parameters and a `text` key containing the response from the LLM.

    {'fruit': 'apple', 'text': 'Well, mate, apples and pears are what I sell...'}

You can specify that a chain uses a specific link:https://python.langchain.com/docs/modules/model_io/output_parsers/[output parser^] to parse the output of the LLM.

Setting the `output_parser` parameter to `StrOutputParser` on the `LLMChain` would ensure the response is a string.

[source,python]
----
include::{repository-raw}/main/2-llm-rag-python-langchain/solutions/llm_chain_string.py[tag=import_str_parser]

include::{repository-raw}/main/2-llm-rag-python-langchain/solutions/llm_chain_string.py[tag=llm_chain]
----

You can change the prompt to instruct the LLM to return a specific output type. 
For example, return JSON by specifying `Output JSON` and give a format in the prompt:

[source,python]
----
include::{repository-raw}/main/2-llm-rag-python-langchain/solutions/llm_chain_json.py[tag=prompt]
----

You can ensure LangChain parses the response as JSON by specifying `SimpleJsonOutputParser` as the `output_parser`:

[source,python]
----
include::{repository-raw}/main/2-llm-rag-python-langchain/solutions/llm_chain_json.py[tag=import_json_parser]

include::{repository-raw}/main/2-llm-rag-python-langchain/solutions/llm_chain_json.py[tag=llm_chain]
----

Experiment with the `output_parser` and prompt to see how the response changes.

== Continue

When you are ready, you can move on to the next task.

read::Move on[]

[.summary],
== Lesson Summary

You learned how to invoke an LLM using Python and LangChain.

Next, you will learn about strategies for grounding an LLM.