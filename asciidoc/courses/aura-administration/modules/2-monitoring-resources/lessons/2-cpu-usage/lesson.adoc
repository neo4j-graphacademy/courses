= Monitoring CPU Usage
:type: lesson
:order: 3


[.discrete]
== Introduction

CPU is a critical resource for database performance. Your database uses it for planning and executing queries.

In this lesson, you'll learn how to monitor CPU usage, identify bottlenecks, and determine when you need to scale your instance.


== Understanding CPU Metrics

The CPU Usage metric shows the overall usage of the CPU's assigned to your instance.

// UI Description: CPU usage (cores). CPU is used for planning and serving queries. 
// If this metric is constantly spiking or at its limits, consider increasing the size of your instance.

CPU is consumed by several key processes in Neo4j.


=== Query Planning and Execution

The query planner analyzes your Cypher statements and creates an execution plan - a series of operations to retrieve your data. This planning process consumes CPU, especially for complex queries.

Once planned, Neo4j executes your query using various operators, each with different CPU costs:

* **Index seeks** - The most efficient operations, directly accessing specific nodes through indexes
* **Label scans** - Scanning all nodes with a particular label consumes moderate CPU
* **Full node scans** - The most expensive operation, fetching every node in your database
* **Relationship traversals** - Expanding relationships by type and direction between nodes, with CPU cost proportional to the number of relationships that are expanded
* **Filtering operations** - Applying `WHERE` clauses to the data held in memory at that time
* **Sorting and aggregations** - Organizing results can be CPU-intensive for large datasets


=== Connection and Thread Management  

Every client connection to your database uses CPU through Neo4j's Bolt protocol:

* **Worker threads** execute your queries and process client requests
* **I/O threads** manage network communication with clients
* **Transaction threads** handle the transaction lifecycle

Each active connection or transaction holds a thread, and that thread consumes CPU while processing. When you have hundreds of concurrent connections, this can put significant pressure on the CPU which subsequently can cause performance issues.


=== Background Operations

Neo4j periodically performs maintenance tasks that consume CPU:

* **Checkpointing** - Writing modified data from memory to disk
* **Index updates** - Keeping indexes synchronized as the data held in the database changes
* **Statistics collection** - Gathering counts of nodes, relationships and properties that will be used by the query planner to create the most efficient execution plan
* **Garbage collection** - The Java Virtual Machine (JVM) reclaiming memory, indicating high workloads that lead to memory pressure
* **Transaction log management** - Processing and rotating transaction logs


== Reading the CPU Usage Chart

The CPU Usage chart displays:

* **Current usage**: Number of cores currently in use
* **Usage over time**: Historical CPU consumption
* **Available cores**: Total cores allocated to your instance (varies by tier and size)


=== Normal CPU Usage Patterns

You'll typically see a **steady baseline** from background operations and regular queries, **periodic peaks** from batch jobs or scheduled tasks, **gradual increases** as your workload grows over time, and **sudden spikes** from large queries or unexpected load. 

Once you identify these patterns, you can tell the difference between normal behavior and actual issues.


== Identifying CPU Issues

Not all database operations cost the same, and understanding what drives CPU usage helps you identify the root cause of performance issues.

Watch for the following warning signs:


=== Constantly High CPU

When your CPU usage stays consistently near or at your instance limits, queries queue for available CPU time rather than executing immediately. This causes increased latency and eventual timeouts as queries wait longer than their configured timeout thresholds.

Before assuming you need more capacity, identify what's consuming CPU. 

Inefficient queries performing expensive operations are the most common cause - full database scans examining every node, label scans without indexes, Cartesian products creating large intermediate result sets, or complex pattern matching that doesn't use indexes. An indexed lookup performs approximately 10 database hits, while a full scan performs millions - this translates directly to CPU cycles consumed.

**Connection overhead** can also drive sustained high CPU. With 100 concurrent connections executing queries on an 8-core instance, 100 threads compete for 8 cores. The CPU cycles are spent on **context-switching overhead** (saving and restoring thread states) rather than executing query operations.

**Diagnostic checklist**:

* Profile your most frequently executed queries using `PROFILE` to measure database hits per operator
* Review execution plans for AllNodesScan, NodeByLabelScan, or high-cost Expand operations
* Check active connection count against available CPU cores (more than 10-15 connections per core indicates contention)
* Verify indexes exist for common filter predicates in `WHERE` clauses
* Identify missing indexes where Filter operators appear instead of NodeIndexSeek
* Optimize expensive queries before scaling instance size


=== Frequent Spikes

Regular CPU spikes typically indicate periodic batch operations, repeatedly executed inefficient queries, or lock contention. But they can also signal garbage collection pressure.

Neo4j runs on the Java Virtual Machine (JVM), which periodically stops everything to reclaim unused memory. During these _"stop-the-world" pauses_, reclaiming memory is given priority, and no actual work gets done. 

Normal garbage collection should consume less than 1% of CPU time. If you're seeing 5% or more, you're likely experiencing memory pressure - your heap is too small for your workload, forcing the JVM to constantly clean up memory.

High garbage collection activity shows up as CPU spikes with degraded query performance across all queries, not just specific ones.

**Diagnostic checklist**:

* Review query logs to correlate spikes with specific queries or scheduled operations
* Check garbage collection metrics - if GC time exceeds 1%, investigate memory pressure
* Monitor heap utilization during spike periods - consistently near 100% indicates insufficient memory
* Identify batch operations or scheduled jobs that coincide with spike timing
* Examine long-running queries that may be holding memory resources
* If GC metrics show more than 5% time in garbage collection, prioritize increasing instance size for more memory over query optimization


=== Sustained 100% Usage

Sustained 100% CPU usage causes query failures or timeouts, degrades transaction processing, and indicates the system has reached capacity limits.

**Immediate actions**:

* Identify currently running queries using query monitoring
* Check for long-running queries that can be safely terminated
* Review recent application deployments that may have introduced inefficient queries
* Determine if this is a temporary spike or sustained load requiring scaling
* If caused by specific queries, terminate them to restore service
* If caused by sustained workload growth, initiate instance scaling
* Implement query timeouts in application code to prevent runaway queries


== When to Scale Your Instance

You should consider scaling your instance when your CPU usage consistently exceeds 70-80% during normal operations, query performance remains degraded after optimization efforts, peak usage regularly hits 100% causing query queuing, or growth trends indicate you'll exceed capacity within your planning window.

But not all scaling is the same. You need to match the scaling strategy to your workload type.


=== Scaling for Write-Heavy Workloads

Writes consume more CPU because Neo4j must write transaction logs, update data structures on disk, maintain indexes for all indexed properties, update statistics for the query planner, and perform consistency checks. 

Write operations must go through the primary instance to maintain consistency in clustered deployments. For sustained write load consuming high CPU, scale the instance vertically by increasing its size to get more CPU cores. 

**Horizontal scaling with secondaries does not reduce write-related CPU load on the primary.**


=== Scaling for Read-Heavy Workloads

Read operations generally consume less CPU than writes, though complex queries can still be expensive. Simple lookups with indexes use minimal CPU, while analytics queries scanning large portions of the graph can spike usage to 100%.

For temporary read spikes - like end-of-quarter reporting or unexpected traffic surges - you can scale horizontally by adding **secondary** servers. Secondaries distribute read queries across multiple instances, reducing the CPU load on any single server.

// [CAUTION]
// .Secondaries for temporary loads
// ====
// Use secondaries for temporary or variable read loads. If your read load is consistently high, scale the instance size instead - it's more cost-effective than running multiple replicas permanently.

// Only maintain multiple replicas when you need high availability or have genuinely variable traffic patterns.
// ====


=== Scaling for Mixed Workloads

Production databases typically serve both transactional writes (Online Transactional Processing - OLTP) and analytical reads (Online Analytical Processing - OLAP) with different resource consumption patterns. Analyze your metrics to determine whether writes (requiring vertical scaling) or reads (allowing horizontal scaling with replicas) drive your CPU consumption, then apply the appropriate scaling strategy.

[TIP]
.Proactive scaling
====
Proactive scaling based on trend analysis prevents performance degradation by adding capacity before constraints impact operations. Reactive scaling adds capacity after performance has already degraded, meaning users experience slow queries and timeouts during the scaling transition.

Monitor your metrics over weeks and months. Scale when trends indicate you'll reach 80% sustained utilization within your planning horizon, rather than waiting until 100% utilization causes active performance problems.
====


== Optimizing CPU Usage

Before scaling, consider these optimization strategies:


=== Query Optimization

Start by identifying your most expensive queries. Use `PROFILE` to see actual CPU costs measured in database hits:

[source,cypher]
----
PROFILE MATCH (p:Person {name: 'Alice'})-[:KNOWS]->(f)
RETURN f.name
----

The **execution plan** shows you exactly where CPU is being spent. Query operations have vastly different costs - an **indexed lookup** might perform 10 **database hits**, while a **full scan** can perform millions. Look for:

* **Full scans** - `AllNodesScan` or `NodeByLabelScan` operations examining every node or every node of a label
* **High DB hits** - The cumulative number of database operations per operator
* **Missing indexes** - `Filter` operations happening after data retrieval instead of `NodeIndexSeek` operations that go directly to matching nodes
* **Expensive traversals** - `Expand` operations following many relationships without selective starting points

**Add strategic indexes** for your most common query patterns:

[source,cypher]
----
CREATE INDEX person_name FOR (p:Person) ON (p.name)
----

This single index can reduce a query from millions of operations to just a few.

**Use query parameters** to enable **plan caching** - Neo4j won't need to replan the same query structure repeatedly:

[source,cypher]
----
// Avoid - replanned every time
MATCH (p:Person {name: 'Alice'}) RETURN p;
----

Instead, use parameters to enable plan caching:

[source,cypher]
----
// Good - plan is cached
MATCH (p:Person {name: $name}) RETURN p;
----


**Specify node labels** in your patterns - this helps Neo4j narrow down the search space:

[source,cypher]
.Avoid - must check all nodes
----
MATCH (p)-[:KNOWS]->(f)
----

Instead, specify the node label used to find the starting point(s) of the query:

[source,cypher]
.Good - Neo4j knows to check only Person nodes
----
MATCH (p:Person)-[:KNOWS]->(f:Person)
----



// === Workload Management

// **Schedule batch jobs during off-peak hours** - Large data imports, analytics queries, or report generation should run when normal traffic is low. A single **bulk operation** can monopolize CPU resources.

// **Break large operations into smaller chunks** - Instead of processing 100,000 records in one transaction, **batch** them in groups of 1,000. This gives other queries a chance to execute and prevents **thread starvation**.

// **Implement connection pooling** in your applications. Creating new database connections requires CPU for authentication, session initialization, and **thread pool assignment**. **Connection pools** maintain reusable connections, eliminating repeated setup overhead. Configure your pool size based on actual **concurrent query execution**, not total user count.

// For example, an application pool of 50 connections serving thousands of users reduces CPU overhead compared to 500 connections. The 500 connection scenario forces 500 threads to compete for CPU cores, spending cycles on **context switching** rather than query execution.

// === Application Changes

// * Cache frequently accessed data at the application level
// * Implement pagination for large result sets
// * Reduce query frequency when possible
// * Use background jobs for non-urgent operations


== Monitoring CPU Patterns by Workload Type

Different workload types create distinct CPU patterns that help you identify what's happening in your database.


=== Read-Heavy Workload Patterns

Watch for regular spikes during complex queries or aggregations These workloads benefit most from page cache optimization - when your frequently accessed data fits in memory, CPU is spent on actual query processing rather than waiting for disk I/O.

If complex read queries are frequent and unavoidable, consider caching results at the application layer or using secondaries to distribute the load.


=== Write-Heavy Workload Patterns  

Sustained high CPU during peak write periods is normal - **index updates** are a major contributor. Each indexed property on modified nodes requires CPU to update.

Look for optimization opportunities in **batching**. Processing 1,000 updates in a single **transaction** is much more efficient than 1,000 individual transactions, reducing both CPU overhead and index update frequency.


=== Mixed Workload Patterns

Production environments typically combine reads and writes with competing resource requirements. Watch for high CPU with many active connections but low **query throughput** - this indicates queries are waiting for threads or resources rather than actually executing.

Background writes consume CPU while holding **locks** that block concurrent reads. Long-running analytics queries hold **thread pool threads** for extended periods, preventing short transactional queries from acquiring threads to execute, causing those queries to queue despite available CPU capacity.

Consider separating workloads by query complexity. Use different **connection pools** for short **transactional queries** versus long-running **analytics queries**, or schedule heavy **batch operations** during dedicated time windows.


== Practical Example: Diagnosing High CPU

Let's walk through a real scenario. Your CPU usage has jumped to 90% sustained, and users are reporting slow queries.

**Step 1: Check active connections**

Look at your metrics - you see 200 **active connections** on an 8-core instance. That's 25 connections per core, which creates significant **context-switching overhead**.

**Step 2: Identify expensive queries**

Review **query logs** for operations with high **execution times** and **database hits**. You find queries like:

[source,cypher]
----
MATCH (p:Person)-[:KNOWS]-(f)
WHERE f.name = 'Alice'
RETURN p.name
----

This query performs **expensive operations** - it scans all Person nodes (potentially millions), follows all KNOWS relationships, then filters for Alice. The **filter** happens after the expensive traversal instead of using an **index seek** to start from Alice directly.

**Step 3: Analyze the execution plan**

Running `PROFILE` shows:

* `NodeByLabelScan` on Person (scanning millions of nodes)
* `Expand(All)` following relationships
* `Filter` operation checking names
* Total: 15 million **database hits**

**Step 4: Apply optimizations**

Restructure the query to **filter early**:

[source,cypher]
----
MATCH (f:Person {name: 'Alice'})-[:KNOWS]-(p)
RETURN p.name
----

Add an **index** if one doesn't exist:

[source,cypher]
----
CREATE INDEX person_name FOR (p:Person) ON (p.name)
----

The new **execution plan** shows:

* `NodeIndexSeek` on Person using person_name
* `Expand(All)` from only matching nodes
* Total: 150 **database hits**

You've reduced the cost by 99.999%, and CPU usage drops to 30%.


[.quiz]
== Check Your Understanding

include::questions/1-cpu-action.adoc[leveloffset=+1]


[.summary]
== Summary

You now understand how CPU is actually used in Neo4j and how to monitor CPU usage for your Aura instances.

You've learned:

* What specific operations consume CPU - from efficient **index seeks** to expensive **full scans**
* How **query execution operators** differ in CPU cost (sometimes by millions of operations)
* The role of **thread pools** in managing **concurrent connections**
* How to use `PROFILE` to identify **expensive operations**
* Specific optimization techniques to reduce CPU consumption
* How to recognize and diagnose CPU issues through concrete examples

With this knowledge, you can identify whether high CPU usage is due to **inefficient queries**, too many **connections**, or genuine **capacity limits** - and take appropriate action.

In the next lesson, you'll learn how to monitor storage consumption and query rates.

