[.question]
= CPU Crisis Response

How should you respond to restore normal service?

* [ ] Scale the instance to get more CPU capacity

* [ ] Wait for the spike to resolve naturally

* [x] Rollback or stop the Friday deployment causing the queries

* [ ] Restart the instance to clear the issue

[TIP,role=hint]
.Hint
====
Consider what's causing the CPU spike and the fastest way to return to normal operation.
====

[TIP,role=solution]
.Solution
====
**Rollback or stop the Friday deployment causing the queries** is correct.

This is the fastest path to recovery because:

**Root cause identified**: New query pattern started at 9:00 AM (same as CPU spike), query is executing 500+ times/minute (very high frequency), each execution takes 2-5 seconds (very slow), and it was introduced in Friday deployment (known cause).

**Immediate relief**: Stop the problematic application/deployment. CPU frees up within seconds, normal operations resume, no downtime is required, and there's no data loss.

**After stopping the deployment**:
1. Work with developers to fix the query
2. Test in non-production environment
3. Deploy fixed version
4. Monitor during re-deployment

Why other approaches fall short: **Scaling** is expensive, doesn't fix the bad query, and the problem persists. **Waiting** means users suffer with no guarantee it resolves. **Restarting** doesn't stop the bad queries as they'll restart immediately.

**The problematic query likely has**:
[source,cypher]
----
// Bad - Cartesian product
MATCH (u:User) MATCH (p:Product) WHERE u.region = p.region
----

**Should be**:
[source,cypher]
----
// Good - Proper relationship or WITH clause
MATCH (u:User) WITH u MATCH (p:Product) WHERE u.region = p.region
----

**Key lesson**: When a specific deployment causes resource exhaustion, stopping that deployment is the fastest recovery path. Fix, test, redeploy properly.
====

