[.question]
= OOM Error Response

Your Aura instance shows the following metrics:

. OOM Error Rate: 0.5 errors per minute (sustained for 10 minutes)
. OOM Error Count: 15 (and increasing)
. Heap Usage: 96%
. Total GC Time: 18%
. Query Rate: Normal

What should be your immediate priority?

* [ ] Wait for garbage collection to resolve the issue

* [ ] Restart the instance to clear memory

* [ ] Review query logs to find the problematic query

* [x] Scale the instance immediately to provide more heap memory

[TIP,role=hint]
.Hint
====
Consider the severity of OOM errors and their impact. What needs to happen first - investigation or stabilization?
====

[TIP,role=solution]
.Solution
====
**Scale the instance immediately to provide more heap memory** is correct.

This is a critical, ongoing incident that requires immediate action:

**Why immediate scaling is priority #1**: OOM errors are ongoing (0.5 per minute means queries are actively failing), the count is increasing (problem is not resolving itself), heap is at 96% (dangerously close to maximum capacity), GC is at 18% (critical level, should be <2%), and there's user impact (applications are experiencing failures NOW).

**What happens after scaling**: More heap memory becomes available, heap usage percentage drops (e.g., 96% â†’ 65%), GC pressure decreases immediately, OOM errors stop occurring, and queries can execute successfully.

**Then investigate**: Once the instance is stable, you can review query logs to identify memory-intensive queries, optimize problematic queries, and implement preventive measures.

Why the other answers are wrong: **Waiting for GC** - GC is already running at 18% (critical level) and failing to prevent OOM errors, so the problem won't resolve itself. **Restarting** - would cause downtime and doesn't address the root cause (insufficient memory for workload). **Query log review** - important but secondary; first stop the bleeding (ongoing failures), then investigate the cause.

**The principle**: When OOM errors are ongoing, stabilize first by adding capacity, then investigate to prevent recurrence.
====

