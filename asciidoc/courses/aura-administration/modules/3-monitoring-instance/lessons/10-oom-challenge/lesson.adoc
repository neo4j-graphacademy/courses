= OOM Crisis Management
:type: challenge
:order: 10


== Scenario

Your e-commerce platform's Neo4j Aura instance starts throwing errors at 2 PM on a busy Friday.

Customer-facing features are failing:

* Product recommendations not loading
* Shopping cart operations timing out
* Order processing errors

Your monitoring dashboard shows:

**Current metrics (2:15 PM)**:

* **OOM Error Rate**: 2.5 errors per minute
* **OOM Error Count**: 150 (started at 0 this morning)
* **Heap Usage**: 98%
* **Total GC Time**: 22%
* **Old Gen GC**: Running almost continuously
* **Query Rate**: 3,200 per minute (normal is 2,500)
* **Failed Query Rate**: 85 per minute (normally 0-2)
* **CPU Usage**: 85%
* **Page Cache Hit Ratio**: 97%


**What happened**:

Reviewing recent changes, you discover:

* Marketing launched a "similar products" feature at 2 PM
* New feature shows 20 recommendations per product page
* Product pages receiving 3x normal traffic due to promotion

**Query logs reveal** (top memory consumer):

```cypher
MATCH (p:Product {id: $productId})
MATCH (p)-[:IN_CATEGORY]->(c:Category)
MATCH (similar:Product)-[:IN_CATEGORY]->(c)
MATCH (similar)-[:PURCHASED_WITH]->(other:Product)
RETURN DISTINCT similar, collect(other) as suggestions
```

This query:

* Runs 800 times per minute
* Average allocation: 45 MB per query
* No `LIMIT` clause
* Returns 200-500 products per execution


== The Challenge

What's your incident response plan?


include::questions/1-oom-triage.adoc[leveloffset=+1]


[.summary]
== Summary

OOM incidents require a structured response: stabilize, investigate, and prevent recurrence.

**The three-phase response**:


=== Phase 1: Stabilize (Minutes)

**Immediate action - Scale the instance**:

* Provides immediate relief from OOM errors
* Stops ongoing query failures
* Restores application functionality
* Buys time for investigation


**Why this comes first**:

* OOM errors mean active failures
* Users are impacted RIGHT NOW
* Cannot investigate effectively while system is failing
* Scaling takes 5-10 minutes vs hours of optimization


=== Phase 2: Mitigate (Within an hour)

**Temporary fix - Optimize the problematic query**:

The query was consuming 36 GB per minute (45 MB × 800 queries):

```cypher
// Original (memory hungry)
MATCH (p:Product {id: $productId})
MATCH (p)-[:IN_CATEGORY]->(c:Category)
MATCH (similar:Product)-[:IN_CATEGORY]->(c)
MATCH (similar)-[:PURCHASED_WITH]->(other:Product)
RETURN DISTINCT similar, collect(other) as suggestions

// Optimized (memory efficient)
MATCH (p:Product {id: $productId})
MATCH (p)-[:IN_CATEGORY]->(c:Category)
MATCH (similar:Product)-[:IN_CATEGORY]->(c)
WHERE similar.id <> p.id
MATCH (similar)-[:PURCHASED_WITH]->(other:Product)
WITH similar, other
LIMIT 20
RETURN similar, collect(other) as suggestions
```

**Key optimizations**:

* Added `LIMIT 20` to match feature requirements
* Added `WHERE` clause to exclude the product itself
* Limited before collecting to reduce memory allocation
* Reduces allocation from 45 MB to ~200 KB per query


=== Phase 3: Prevent (Ongoing)

**Long-term improvements**:

* **Code review process**: Require `LIMIT` clauses on queries returning collections
* **Load testing**: Test new features at production scale before launch
* **Monitoring alerts**: Alert on heap >85% and any OOM errors
* **Capacity planning**: Review metrics weekly, scale proactively
* **Query guidelines**: Document memory-efficient query patterns


**The wrong approach**:

Trying to optimize the query while OOM errors are occurring:

* Users continue experiencing failures
* System instability makes testing difficult
* Team is under high pressure
* Risk of making things worse
* Delays resolution


**The right approach**:

Scale → Stabilize → Investigate → Optimize → Prevent

This incident demonstrates why monitoring is critical:

* Early warning metrics (heap, GC) were showing problems
* OOM errors indicated critical failure
* Query logs identified the root cause
* Multiple metrics together told the complete story


**Key lessons**:

* New features need load testing at production scale
* Queries should always include `LIMIT` when returning collections
* Memory allocation (`alloc bytes`) is a critical metric
* Scaling buys time to fix problems properly
* Optimization should happen after stabilization


**Capacity planning**:

Even after optimization, consider:

* Keeping the larger instance size
* Traffic is 3x normal due to promotion
* Future features may add workload
* Headroom prevents future incidents


**Monitoring setup**:

Configure alerts to catch issues earlier:

* Heap usage >80%: Warning
* Heap usage >85%: Critical
* GC time >5%: Investigation needed
* Any OOM errors: Page on-call
* Failed query rate >10/min: Alert


This scenario shows how operational monitoring, query optimization, and capacity planning work together to maintain a healthy production system.

