[.question]
= Incident Triage

With OOM errors occurring at 2.5 per minute and customers unable to complete purchases, what's the correct response order?

* [ ] 1. Optimize the query, 2. Scale instance, 3. Monitor results

* [ ] 1. Disable the new feature, 2. Restart instance, 3. Optimize query

* [x] 1. Scale instance immediately, 2. Verify stabilization, 3. Optimize query

* [ ] 1. Contact development team, 2. Plan optimization, 3. Schedule scaling

[TIP,role=hint]
.Hint
====
Consider what needs to happen first when users are experiencing active failures. What provides the fastest path to stability?
====

[TIP,role=solution]
.Solution
====
**1. Scale instance immediately, 2. Verify stabilization, 3. Optimize query** is correct.

This is the proper incident response for ongoing OOM errors:


**Step 1: Scale instance immediately (5-10 minutes)**

Why this comes first: OOM errors are causing active failures RIGHT NOW. With 85 queries failing per minute, you're looking at 5,100 failed operations in an hour. Customers cannot complete purchases, causing revenue loss. Scaling provides immediate relief and takes only 5-10 minutes to complete.

**What scaling accomplishes**: More heap memory becomes available (e.g., 8GB → 16GB), heap usage drops from 98% to ~60%, GC pressure decreases immediately, OOM errors stop occurring, queries can execute successfully, and application functionality is restored.


**Step 2: Verify stabilization (5 minutes)**

Check metrics to confirm the immediate crisis is resolved: OOM error rate returns to 0, heap usage drops below 80%, GC time returns to <5%, failed query rate returns to normal, and application features are working.


**Step 3: Optimize the query (30-60 minutes)**

Now that the system is stable, you can review query logs safely, identify memory allocation patterns, test optimizations in a stable environment, deploy query improvements, and monitor their impact.

**The query optimization**:

Original query allocating 45 MB × 800 queries/min = 36 GB/min memory allocation:

```cypher
MATCH (p:Product {id: $productId})
MATCH (p)-[:IN_CATEGORY]->(c:Category)
MATCH (similar:Product)-[:IN_CATEGORY]->(c)
MATCH (similar)-[:PURCHASED_WITH]->(other:Product)
RETURN DISTINCT similar, collect(other) as suggestions
```

Optimized query with `LIMIT`:

```cypher
MATCH (p:Product {id: $productId})
MATCH (p)-[:IN_CATEGORY]->(c:Category)
MATCH (similar:Product)-[:IN_CATEGORY]->(c)
WHERE similar.id <> p.id
MATCH (similar)-[:PURCHASED_WITH]->(other:Product)
WITH similar, other
LIMIT 20
RETURN similar, collect(other) as suggestions
```

**Optimization results**: Memory allocation drops from 45 MB to ~200 KB (a 225x reduction) while maintaining the same functionality (feature still shows 20 recommendations) with a better user experience (faster queries).


Why the other approaches are wrong:

**Option 1: Optimize first, then scale** - Users continue experiencing failures while you work, optimization takes 30-60 minutes, more failed transactions equals more revenue loss, it's harder to test optimizations with an unstable system, and this approach delays resolution by an hour.


**Option 2: Disable feature, restart, optimize** - Disabling the feature removes revenue-generating functionality, restart causes additional downtime, restart doesn't address the underlying capacity issue, and OOM errors would likely return when the feature is re-enabled.


**Option 3: Contact team, plan, schedule** - This is too slow for a critical incident, users would be experiencing failures for hours, coordination takes too long during an incident, and scaling can happen immediately without planning.


**The incident response principle**:

For critical incidents with active failures:

1. **Stop the bleeding** (scale/add capacity)
2. **Verify stability** (confirm metrics)
3. **Fix the cause** (optimize queries)
4. **Prevent recurrence** (monitoring, guidelines)


**Cost considerations**:

Scaling a production instance during an incident is always justified: Revenue loss from failed transactions far exceeds infrastructure cost, customer trust and experience are critical, you can optimize and potentially scale back down after resolution, and infrastructure cost is minimal compared to incident impact.


**Real-world timing**: Scale instance (10 minutes), verify stability (5 minutes), optimize query (45 minutes) - **Total resolution: ~60 minutes**.

Compare this to trying to optimize first: Hours of investigation during failures, continued user impact, risk of incomplete optimization under pressure - **Total resolution: 2-4+ hours with sustained impact**.


This scenario demonstrates why **scaling is not a failure** - it's a valid and often necessary incident response tool that enables proper investigation and optimization in a stable environment.
====

