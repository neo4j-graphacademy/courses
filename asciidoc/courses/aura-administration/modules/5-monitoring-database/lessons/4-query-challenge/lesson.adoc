=  Query Performance Investigation
:type: challenge
:order: 4


== Scenario

Users are complaining that "reports are slow" in your application.

Investigating the Database metrics for the past 24 hours:

* **Queries per minute**: 800 (normal)
* **Failed queries per minute**: 2-3 (< 1%, normal)
* **Query Latency 50th percentile**: 45ms (acceptable)
* **Query Latency 75th percentile**: 120ms (acceptable)
* **Query Latency 99th percentile**: 8,500ms (concerning)

The reports that users mention are generated by a dashboard that runs every hour.

Looking at query logs filtered for "duration > 5000ms":

* Same 3 queries appear repeatedly
* All 3 are dashboard queries
* Each returns 50,000+ rows
* Average duration: 7,000-9,000ms
* These queries run 24 times per day (hourly)


== The Challenge

Based on this analysis, how should you address the performance complaints?


include::questions/1-percentile-analysis.adoc[leveloffset=+1]


[.summary]
== Summary

When 99th percentile latency is much higher than median, investigate and optimize the specific slow queries rather than scaling. Most queries are performing well—focus on the outliers
3. ✅ Filtered query logs for slow queries (>5000ms)
4. ✅ Found specific problematic queries
5. ✅ Ready to optimize those queries

**Common optimizations for dashboard queries:**

* Add appropriate indexes
* Use aggregations instead of returning all rows
* Implement pagination or limiting
* Pre-compute results for frequently run reports
* Cache results at application level

**When to scale vs. optimize:**

* **Optimize first**: Specific slow queries, high 99th percentile, low 50th percentile
* **Scale instance**: All percentiles high, resources maxed, queries already optimized

The 99th percentile is crucial for identifying outlier queries that need attention.

