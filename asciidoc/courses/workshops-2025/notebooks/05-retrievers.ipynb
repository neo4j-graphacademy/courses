{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 5: Retrievers for Graph-Enhanced RAG\n",
    "\n",
    "This notebook focuses on implementing retrieval patterns for Retrieval-Augmented Generation (RAG) using graph databases, combining semantic search with graph traversal for enhanced context retrieval.\n",
    "\n",
    "## Learning Objectives\n",
    "- Implement vector-based semantic retrieval in Neo4j\n",
    "- Design graph traversal retrieval patterns\n",
    "- Combine vector and graph retrieval strategies\n",
    "- Optimize retrieval performance and relevance\n",
    "- Build hybrid retrievers for complex queries\n",
    "\n",
    "## Prerequisites\n",
    "- Completion of Module 4: Graph Analytics\n",
    "- Understanding of vector embeddings and RAG concepts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install neo4j pandas numpy openai sentence-transformers langchain tiktoken faiss-cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "from neo4j import GraphDatabase\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import openai\n",
    "from typing import List, Dict, Tuple, Optional\n",
    "import tiktoken\n",
    "from datetime import datetime\n",
    "import re\n",
    "from dataclasses import dataclass\n",
    "import time\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Initialize models\n",
    "embedding_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "tokenizer = tiktoken.get_encoding(\"cl100k_base\")\n",
    "\n",
    "print(\"Libraries and models loaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neo4j Connection and Data Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Neo4j connection settings\n",
    "NEO4J_URI = os.getenv('NEO4J_URI', 'bolt://localhost:7687')\n",
    "NEO4J_USERNAME = os.getenv('NEO4J_USERNAME', 'neo4j')\n",
    "NEO4J_PASSWORD = os.getenv('NEO4J_PASSWORD', 'password')\n",
    "\n",
    "# Create Neo4j driver\n",
    "driver = GraphDatabase.driver(NEO4J_URI, auth=(NEO4J_USERNAME, NEO4J_PASSWORD))\n",
    "\n",
    "def run_query(query, parameters=None):\n",
    "    \"\"\"Execute a Cypher query and return results\"\"\"\n",
    "    with driver.session() as session:\n",
    "        result = session.run(query, parameters or {})\n",
    "        return [record.data() for record in result]\n",
    "\n",
    "# Test connection\n",
    "print(\"Testing Neo4j connection...\")\n",
    "result = run_query(\"RETURN 'Connected to Neo4j!' as message\")\n",
    "print(result[0]['message'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clear existing data and create sample knowledge base\n",
    "run_query(\"MATCH (n) DETACH DELETE n\")\n",
    "\n",
    "# Sample knowledge base: AI/ML research papers and concepts\n",
    "knowledge_base = [\n",
    "    {\n",
    "        \"id\": \"paper1\",\n",
    "        \"title\": \"Attention Is All You Need\",\n",
    "        \"authors\": [\"Vaswani\", \"Shazeer\", \"Parmar\", \"Uszkoreit\"],\n",
    "        \"year\": 2017,\n",
    "        \"venue\": \"NeurIPS\",\n",
    "        \"abstract\": \"The dominant sequence transduction models are based on complex recurrent or convolutional neural networks that include an encoder and a decoder. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely.\",\n",
    "        \"concepts\": [\"Transformer\", \"Attention Mechanism\", \"Self-Attention\", \"Multi-Head Attention\", \"Neural Machine Translation\"],\n",
    "        \"citations\": 50000,\n",
    "        \"field\": \"Natural Language Processing\"\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"paper2\",\n",
    "        \"title\": \"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\",\n",
    "        \"authors\": [\"Devlin\", \"Chang\", \"Lee\", \"Toutanova\"],\n",
    "        \"year\": 2018,\n",
    "        \"venue\": \"NAACL\",\n",
    "        \"abstract\": \"We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models, BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers.\",\n",
    "        \"concepts\": [\"BERT\", \"Bidirectional Transformer\", \"Pre-training\", \"Masked Language Modeling\", \"Fine-tuning\"],\n",
    "        \"citations\": 75000,\n",
    "        \"field\": \"Natural Language Processing\"\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"paper3\",\n",
    "        \"title\": \"Language Models are Few-Shot Learners\",\n",
    "        \"authors\": [\"Brown\", \"Mann\", \"Ryder\", \"Subbiah\"],\n",
    "        \"year\": 2020,\n",
    "        \"venue\": \"NeurIPS\",\n",
    "        \"abstract\": \"Recent work has demonstrated substantial gains on many NLP tasks and benchmarks by pre-training on a large corpus of text followed by fine-tuning on a specific task. While typically task-agnostic in architecture, this method still requires task-specific fine-tuning datasets of thousands or tens of thousands of examples. By contrast, humans can generally perform a new language task from only a few examples or from simple instructions.\",\n",
    "        \"concepts\": [\"GPT-3\", \"Few-Shot Learning\", \"In-Context Learning\", \"Large Language Models\", \"Emergent Abilities\"],\n",
    "        \"citations\": 25000,\n",
    "        \"field\": \"Natural Language Processing\"\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"paper4\",\n",
    "        \"title\": \"Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks\",\n",
    "        \"authors\": [\"Lewis\", \"Perez\", \"Piktus\", \"Petroni\"],\n",
    "        \"year\": 2020,\n",
    "        \"venue\": \"NeurIPS\",\n",
    "        \"abstract\": \"Large pre-trained language models have been shown to store factual knowledge in their parameters, and achieve state-of-the-art results when fine-tuned on downstream NLP tasks. However, their ability to access and precisely manipulate knowledge is still limited, and hence on knowledge-intensive tasks, their performance lags behind task-specific architectures. Additionally, providing provenance for their decisions is challenging.\",\n",
    "        \"concepts\": [\"RAG\", \"Retrieval-Augmented Generation\", \"Knowledge-Intensive Tasks\", \"Dense Passage Retrieval\", \"Hybrid Models\"],\n",
    "        \"citations\": 8000,\n",
    "        \"field\": \"Natural Language Processing\"\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"paper5\",\n",
    "        \"title\": \"Graph Neural Networks: A Review of Methods and Applications\",\n",
    "        \"authors\": [\"Zhou\", \"Cui\", \"Hu\", \"Zhang\"],\n",
    "        \"year\": 2020,\n",
    "        \"venue\": \"AI Open\",\n",
    "        \"abstract\": \"Lots of learning tasks require dealing with graph data which contains rich relation information among elements. Modeling physics systems, learning molecular fingerprints, predicting protein interface, and classifying diseases require a model to learn from graph inputs. In other domains such as learning from non-Euclidean data, graph neural networks (GNNs) have been demonstrated to be effective.\",\n",
    "        \"concepts\": [\"Graph Neural Networks\", \"GNN\", \"Graph Convolution\", \"Message Passing\", \"Graph Attention\"],\n",
    "        \"citations\": 15000,\n",
    "        \"field\": \"Machine Learning\"\n",
    "    }\n",
    "]\n",
    "\n",
    "print(f\"Loaded {len(knowledge_base)} research papers for the knowledge base\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lesson 1: Vector Similarity Search Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create knowledge graph with embeddings\n",
    "def create_knowledge_graph(papers):\n",
    "    \"\"\"Create a knowledge graph from research papers with embeddings\"\"\"\n",
    "    \n",
    "    for paper in papers:\n",
    "        # Create paper node\n",
    "        paper_query = \"\"\"\n",
    "        CREATE (p:Paper {\n",
    "            id: $id,\n",
    "            title: $title,\n",
    "            year: $year,\n",
    "            venue: $venue,\n",
    "            abstract: $abstract,\n",
    "            citations: $citations,\n",
    "            field: $field\n",
    "        })\n",
    "        \"\"\"\n",
    "        \n",
    "        run_query(paper_query, {\n",
    "            'id': paper['id'],\n",
    "            'title': paper['title'],\n",
    "            'year': paper['year'],\n",
    "            'venue': paper['venue'],\n",
    "            'abstract': paper['abstract'],\n",
    "            'citations': paper['citations'],\n",
    "            'field': paper['field']\n",
    "        })\n",
    "        \n",
    "        # Create author nodes and relationships\n",
    "        for author in paper['authors']:\n",
    "            author_query = \"\"\"\n",
    "            MERGE (a:Author {name: $author_name})\n",
    "            WITH a\n",
    "            MATCH (p:Paper {id: $paper_id})\n",
    "            CREATE (a)-[:AUTHORED]->(p)\n",
    "            \"\"\"\n",
    "            \n",
    "            run_query(author_query, {\n",
    "                'author_name': author,\n",
    "                'paper_id': paper['id']\n",
    "            })\n",
    "        \n",
    "        # Create concept nodes and relationships\n",
    "        for concept in paper['concepts']:\n",
    "            concept_query = \"\"\"\n",
    "            MERGE (c:Concept {name: $concept_name})\n",
    "            WITH c\n",
    "            MATCH (p:Paper {id: $paper_id})\n",
    "            CREATE (p)-[:DISCUSSES]->(c)\n",
    "            \"\"\"\n",
    "            \n",
    "            run_query(concept_query, {\n",
    "                'concept_name': concept,\n",
    "                'paper_id': paper['id']\n",
    "            })\n",
    "        \n",
    "        # Create field node and relationship\n",
    "        field_query = \"\"\"\n",
    "        MERGE (f:Field {name: $field_name})\n",
    "        WITH f\n",
    "        MATCH (p:Paper {id: $paper_id})\n",
    "        CREATE (p)-[:BELONGS_TO]->(f)\n",
    "        \"\"\"\n",
    "        \n",
    "        run_query(field_query, {\n",
    "            'field_name': paper['field'],\n",
    "            'paper_id': paper['id']\n",
    "        })\n",
    "\n",
    "# Create the knowledge graph\n",
    "create_knowledge_graph(knowledge_base)\n",
    "print(\"Knowledge graph created with papers, authors, concepts, and fields\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate and store embeddings for papers\n",
    "def add_embeddings_to_papers():\n",
    "    \"\"\"Generate embeddings for paper titles and abstracts\"\"\"\n",
    "    \n",
    "    # Get all papers\n",
    "    papers = run_query(\"MATCH (p:Paper) RETURN p.id as id, p.title as title, p.abstract as abstract\")\n",
    "    \n",
    "    for paper in papers:\n",
    "        # Combine title and abstract for embedding\n",
    "        text_content = f\"{paper['title']} {paper['abstract']}\"\n",
    "        \n",
    "        # Generate embedding\n",
    "        embedding = embedding_model.encode([text_content])[0]\n",
    "        \n",
    "        # Store embedding in Neo4j\n",
    "        embedding_query = \"\"\"\n",
    "        MATCH (p:Paper {id: $paper_id})\n",
    "        SET p.embedding = $embedding\n",
    "        \"\"\"\n",
    "        \n",
    "        run_query(embedding_query, {\n",
    "            'paper_id': paper['id'],\n",
    "            'embedding': embedding.tolist()\n",
    "        })\n",
    "        \n",
    "        print(f\"Generated embedding for: {paper['title'][:50]}...\")\n",
    "\n",
    "add_embeddings_to_papers()\n",
    "print(\"\\nEmbeddings added to all papers in the knowledge graph\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create vector search index\n",
    "def create_vector_index():\n",
    "    \"\"\"Create vector search index for papers\"\"\"\n",
    "    \n",
    "    index_query = \"\"\"\n",
    "    CREATE VECTOR INDEX paper_embeddings IF NOT EXISTS\n",
    "    FOR (p:Paper) ON (p.embedding)\n",
    "    OPTIONS {\n",
    "      indexConfig: {\n",
    "        `vector.dimensions`: 384,\n",
    "        `vector.similarity_function`: 'cosine'\n",
    "      }\n",
    "    }\n",
    "    \"\"\"\n",
    "    \n",
    "    try:\n",
    "        run_query(index_query)\n",
    "        print(\"Vector index created successfully\")\n",
    "        \n",
    "        # Wait for index to be ready\n",
    "        time.sleep(2)\n",
    "        \n",
    "        # Check index status\n",
    "        status = run_query(\"SHOW INDEXES YIELD name, state WHERE name = 'paper_embeddings'\")\n",
    "        if status:\n",
    "            print(f\"Index status: {status[0]['state']}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Vector index creation failed: {e}\")\n",
    "        print(\"Proceeding with fallback similarity search\")\n",
    "\n",
    "create_vector_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement vector similarity search\n",
    "@dataclass\n",
    "class RetrievalResult:\n",
    "    \"\"\"Container for retrieval results\"\"\"\n",
    "    paper_id: str\n",
    "    title: str\n",
    "    abstract: str\n",
    "    score: float\n",
    "    retrieval_method: str\n",
    "    additional_info: Dict = None\n",
    "\n",
    "def vector_similarity_search(query: str, top_k: int = 3) -> List[RetrievalResult]:\n",
    "    \"\"\"Perform vector similarity search\"\"\"\n",
    "    \n",
    "    # Generate query embedding\n",
    "    query_embedding = embedding_model.encode([query])[0]\n",
    "    \n",
    "    try:\n",
    "        # Try using vector index first\n",
    "        search_query = \"\"\"\n",
    "        CALL db.index.vector.queryNodes('paper_embeddings', $top_k, $query_embedding)\n",
    "        YIELD node AS paper, score\n",
    "        RETURN paper.id as paper_id,\n",
    "               paper.title as title,\n",
    "               paper.abstract as abstract,\n",
    "               score\n",
    "        ORDER BY score DESC\n",
    "        \"\"\"\n",
    "        \n",
    "        results = run_query(search_query, {\n",
    "            'query_embedding': query_embedding.tolist(),\n",
    "            'top_k': top_k\n",
    "        })\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Vector index search failed: {e}\")\n",
    "        print(\"Falling back to manual similarity calculation\")\n",
    "        \n",
    "        # Fallback: manual similarity calculation\n",
    "        papers = run_query(\"MATCH (p:Paper) RETURN p.id as paper_id, p.title as title, p.abstract as abstract, p.embedding as embedding\")\n",
    "        \n",
    "        similarities = []\n",
    "        for paper in papers:\n",
    "            if paper['embedding']:\n",
    "                paper_embedding = np.array(paper['embedding'])\n",
    "                similarity = cosine_similarity([query_embedding], [paper_embedding])[0][0]\n",
    "                similarities.append({\n",
    "                    'paper_id': paper['paper_id'],\n",
    "                    'title': paper['title'],\n",
    "                    'abstract': paper['abstract'],\n",
    "                    'score': float(similarity)\n",
    "                })\n",
    "        \n",
    "        # Sort by similarity and take top k\n",
    "        results = sorted(similarities, key=lambda x: x['score'], reverse=True)[:top_k]\n",
    "    \n",
    "    # Convert to RetrievalResult objects\n",
    "    return [\n",
    "        RetrievalResult(\n",
    "            paper_id=r['paper_id'],\n",
    "            title=r['title'],\n",
    "            abstract=r['abstract'],\n",
    "            score=r['score'],\n",
    "            retrieval_method='vector_similarity'\n",
    "        )\n",
    "        for r in results\n",
    "    ]\n",
    "\n",
    "# Test vector similarity search\n",
    "test_query = \"transformer models for language understanding\"\n",
    "vector_results = vector_similarity_search(test_query, top_k=3)\n",
    "\n",
    "print(f\"Vector Similarity Search Results for: '{test_query}'\\n\")\n",
    "for i, result in enumerate(vector_results, 1):\n",
    "    print(f\"{i}. {result.title} (Score: {result.score:.4f})\")\n",
    "    print(f\"   Abstract: {result.abstract[:100]}...\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lesson 2: Graph Traversal Retrieval Patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement graph traversal retrieval\n",
    "def graph_traversal_search(query: str, max_depth: int = 2) -> List[RetrievalResult]:\n",
    "    \"\"\"Retrieve papers using graph traversal patterns\"\"\"\n",
    "    \n",
    "    # Extract key terms from query\n",
    "    query_terms = [term.strip().lower() for term in query.split() if len(term.strip()) > 3]\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    # Pattern 1: Find papers by concept similarity\n",
    "    for term in query_terms:\n",
    "        concept_query = f\"\"\"\n",
    "        MATCH (c:Concept)\n",
    "        WHERE toLower(c.name) CONTAINS '{term}'\n",
    "        MATCH (p:Paper)-[:DISCUSSES]->(c)\n",
    "        RETURN DISTINCT p.id as paper_id,\n",
    "               p.title as title,\n",
    "               p.abstract as abstract,\n",
    "               c.name as matched_concept,\n",
    "               'concept_match' as match_type\n",
    "        \"\"\"\n",
    "        \n",
    "        concept_results = run_query(concept_query)\n",
    "        \n",
    "        for result in concept_results:\n",
    "            results.append(RetrievalResult(\n",
    "                paper_id=result['paper_id'],\n",
    "                title=result['title'],\n",
    "                abstract=result['abstract'],\n",
    "                score=0.8,  # High score for direct concept match\n",
    "                retrieval_method='graph_traversal',\n",
    "                additional_info={'matched_concept': result['matched_concept'], 'match_type': result['match_type']}\n",
    "            ))\n",
    "    \n",
    "    # Pattern 2: Find papers by author collaboration\n",
    "    author_query = f\"\"\"\n",
    "    MATCH (a:Author)-[:AUTHORED]->(p1:Paper)\n",
    "    WHERE toLower(p1.title) CONTAINS '{query_terms[0]}' OR toLower(p1.abstract) CONTAINS '{query_terms[0]}'\n",
    "    MATCH (a)-[:AUTHORED]->(p2:Paper)\n",
    "    WHERE p1 <> p2\n",
    "    RETURN DISTINCT p2.id as paper_id,\n",
    "           p2.title as title,\n",
    "           p2.abstract as abstract,\n",
    "           a.name as connecting_author,\n",
    "           'author_connection' as match_type\n",
    "    LIMIT 3\n",
    "    \"\"\"\n",
    "    \n",
    "    author_results = run_query(author_query)\n",
    "    \n",
    "    for result in author_results:\n",
    "        results.append(RetrievalResult(\n",
    "            paper_id=result['paper_id'],\n",
    "            title=result['title'],\n",
    "            abstract=result['abstract'],\n",
    "            score=0.6,  # Medium score for author connection\n",
    "            retrieval_method='graph_traversal',\n",
    "            additional_info={'connecting_author': result['connecting_author'], 'match_type': result['match_type']}\n",
    "        ))\n",
    "    \n",
    "    # Pattern 3: Find papers in the same field with shared concepts\n",
    "    field_query = f\"\"\"\n",
    "    MATCH (p1:Paper)-[:DISCUSSES]->(c:Concept)\n",
    "    WHERE toLower(p1.title) CONTAINS '{query_terms[0]}' OR toLower(c.name) CONTAINS '{query_terms[0]}'\n",
    "    MATCH (p1)-[:BELONGS_TO]->(f:Field)<-[:BELONGS_TO]-(p2:Paper)\n",
    "    MATCH (p2)-[:DISCUSSES]->(c2:Concept)\n",
    "    WHERE p1 <> p2\n",
    "    RETURN DISTINCT p2.id as paper_id,\n",
    "           p2.title as title,\n",
    "           p2.abstract as abstract,\n",
    "           f.name as field,\n",
    "           collect(DISTINCT c2.name)[0..3] as shared_concepts,\n",
    "           'field_connection' as match_type\n",
    "    LIMIT 3\n",
    "    \"\"\"\n",
    "    \n",
    "    field_results = run_query(field_query)\n",
    "    \n",
    "    for result in field_results:\n",
    "        results.append(RetrievalResult(\n",
    "            paper_id=result['paper_id'],\n",
    "            title=result['title'],\n",
    "            abstract=result['abstract'],\n",
    "            score=0.7,  # Good score for field connection\n",
    "            retrieval_method='graph_traversal',\n",
    "            additional_info={'field': result['field'], 'shared_concepts': result['shared_concepts'], 'match_type': result['match_type']}\n",
    "        ))\n",
    "    \n",
    "    # Remove duplicates and sort by score\n",
    "    unique_results = {}\n",
    "    for result in results:\n",
    "        if result.paper_id not in unique_results or result.score > unique_results[result.paper_id].score:\n",
    "            unique_results[result.paper_id] = result\n",
    "    \n",
    "    return sorted(unique_results.values(), key=lambda x: x.score, reverse=True)\n",
    "\n",
    "# Test graph traversal search\n",
    "graph_results = graph_traversal_search(\"attention transformer neural networks\", max_depth=2)\n",
    "\n",
    "print(f\"Graph Traversal Search Results for: 'attention transformer neural networks'\\n\")\n",
    "for i, result in enumerate(graph_results[:5], 1):\n",
    "    print(f\"{i}. {result.title} (Score: {result.score:.2f})\")\n",
    "    if result.additional_info:\n",
    "        info = result.additional_info\n",
    "        if 'matched_concept' in info:\n",
    "            print(f\"   Matched Concept: {info['matched_concept']}\")\n",
    "        if 'connecting_author' in info:\n",
    "            print(f\"   Connected via Author: {info['connecting_author']}\")\n",
    "        if 'field' in info:\n",
    "            print(f\"   Field: {info['field']}, Concepts: {', '.join(info['shared_concepts'])}\")\n",
    "    print(f\"   Abstract: {result.abstract[:80]}...\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lesson 3: Hybrid Retrieval Strategies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement hybrid retrieval combining vector and graph methods\n",
    "def hybrid_retrieval(query: str, top_k: int = 5, weights: Dict[str, float] = None) -> List[RetrievalResult]:\n",
    "    \"\"\"Combine vector similarity and graph traversal for optimal retrieval\"\"\"\n",
    "    \n",
    "    if weights is None:\n",
    "        weights = {'vector': 0.6, 'graph': 0.4}\n",
    "    \n",
    "    # Get results from both methods\n",
    "    vector_results = vector_similarity_search(query, top_k=top_k*2)\n",
    "    graph_results = graph_traversal_search(query, max_depth=2)\n",
    "    \n",
    "    # Combine and re-score results\n",
    "    combined_results = {}\n",
    "    \n",
    "    # Add vector results with weighted scores\n",
    "    for result in vector_results:\n",
    "        combined_results[result.paper_id] = RetrievalResult(\n",
    "            paper_id=result.paper_id,\n",
    "            title=result.title,\n",
    "            abstract=result.abstract,\n",
    "            score=result.score * weights['vector'],\n",
    "            retrieval_method='hybrid',\n",
    "            additional_info={'vector_score': result.score, 'graph_score': 0.0, 'methods': ['vector']}\n",
    "        )\n",
    "    \n",
    "    # Add or update with graph results\n",
    "    for result in graph_results:\n",
    "        if result.paper_id in combined_results:\n",
    "            # Update existing result\n",
    "            existing = combined_results[result.paper_id]\n",
    "            existing.score += result.score * weights['graph']\n",
    "            existing.additional_info['graph_score'] = result.score\n",
    "            existing.additional_info['methods'].append('graph')\n",
    "            if result.additional_info:\n",
    "                existing.additional_info.update(result.additional_info)\n",
    "        else:\n",
    "            # Add new result\n",
    "            combined_results[result.paper_id] = RetrievalResult(\n",
    "                paper_id=result.paper_id,\n",
    "                title=result.title,\n",
    "                abstract=result.abstract,\n",
    "                score=result.score * weights['graph'],\n",
    "                retrieval_method='hybrid',\n",
    "                additional_info={\n",
    "                    'vector_score': 0.0, \n",
    "                    'graph_score': result.score, \n",
    "                    'methods': ['graph'],\n",
    "                    **(result.additional_info or {})\n",
    "                }\n",
    "            )\n",
    "    \n",
    "    # Sort by combined score and return top k\n",
    "    return sorted(combined_results.values(), key=lambda x: x.score, reverse=True)[:top_k]\n",
    "\n",
    "# Test hybrid retrieval\n",
    "hybrid_results = hybrid_retrieval(\"pre-training language models with attention\", top_k=4)\n",
    "\n",
    "print(f\"Hybrid Retrieval Results for: 'pre-training language models with attention'\\n\")\n",
    "for i, result in enumerate(hybrid_results, 1):\n",
    "    print(f\"{i}. {result.title}\")\n",
    "    print(f\"   Combined Score: {result.score:.4f}\")\n",
    "    info = result.additional_info\n",
    "    print(f\"   Vector Score: {info['vector_score']:.4f}, Graph Score: {info['graph_score']:.4f}\")\n",
    "    print(f\"   Methods Used: {', '.join(info['methods'])}\")\n",
    "    if 'matched_concept' in info:\n",
    "        print(f\"   Matched Concept: {info['matched_concept']}\")\n",
    "    print(f\"   Abstract: {result.abstract[:100]}...\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Advanced hybrid retrieval with query expansion\n",
    "def advanced_hybrid_retrieval(query: str, top_k: int = 5) -> List[RetrievalResult]:\n",
    "    \"\"\"Advanced hybrid retrieval with query expansion and contextual scoring\"\"\"\n",
    "    \n",
    "    # Step 1: Query expansion using graph structure\n",
    "    expanded_terms = expand_query_with_graph(query)\n",
    "    print(f\"Query expanded to include: {', '.join(expanded_terms)}\")\n",
    "    \n",
    "    # Step 2: Multi-hop graph traversal for contextual retrieval\n",
    "    contextual_results = contextual_graph_retrieval(query, expanded_terms)\n",
    "    \n",
    "    # Step 3: Vector similarity with expanded query\n",
    "    expanded_query = f\"{query} {' '.join(expanded_terms)}\"\n",
    "    vector_results = vector_similarity_search(expanded_query, top_k=top_k*2)\n",
    "    \n",
    "    # Step 4: Intelligent score fusion\n",
    "    return fuse_retrieval_results(vector_results, contextual_results, top_k)\n",
    "\n",
    "def expand_query_with_graph(query: str) -> List[str]:\n",
    "    \"\"\"Expand query using related concepts from the knowledge graph\"\"\"\n",
    "    \n",
    "    # Find concepts related to query terms\n",
    "    query_terms = [term.strip().lower() for term in query.split() if len(term.strip()) > 3]\n",
    "    expanded_terms = set()\n",
    "    \n",
    "    for term in query_terms:\n",
    "        expansion_query = f\"\"\"\n",
    "        MATCH (c1:Concept)\n",
    "        WHERE toLower(c1.name) CONTAINS '{term}'\n",
    "        MATCH (p:Paper)-[:DISCUSSES]->(c1)\n",
    "        MATCH (p)-[:DISCUSSES]->(c2:Concept)\n",
    "        WHERE c1 <> c2\n",
    "        RETURN DISTINCT c2.name as related_concept\n",
    "        LIMIT 3\n",
    "        \"\"\"\n",
    "        \n",
    "        results = run_query(expansion_query)\n",
    "        for result in results:\n",
    "            expanded_terms.add(result['related_concept'].lower())\n",
    "    \n",
    "    return list(expanded_terms)[:5]  # Limit expansion\n",
    "\n",
    "def contextual_graph_retrieval(query: str, expanded_terms: List[str]) -> List[RetrievalResult]:\n",
    "    \"\"\"Perform contextual retrieval using multi-hop graph patterns\"\"\"\n",
    "    \n",
    "    # Multi-hop pattern: Paper -> Concept -> Paper -> Author -> Paper\n",
    "    contextual_query = \"\"\"\n",
    "    MATCH (p1:Paper)-[:DISCUSSES]->(c:Concept)<-[:DISCUSSES]-(p2:Paper)\n",
    "    MATCH (p2)<-[:AUTHORED]-(a:Author)-[:AUTHORED]->(p3:Paper)\n",
    "    WHERE p1 <> p2 AND p2 <> p3 AND p1 <> p3\n",
    "      AND (toLower(p1.title) CONTAINS $query_term OR toLower(p1.abstract) CONTAINS $query_term)\n",
    "    WITH p3, c, a, \n",
    "         p3.citations as citations,\n",
    "         p3.year as year,\n",
    "         count(*) as connection_strength\n",
    "    RETURN DISTINCT p3.id as paper_id,\n",
    "           p3.title as title,\n",
    "           p3.abstract as abstract,\n",
    "           connection_strength,\n",
    "           citations,\n",
    "           year,\n",
    "           c.name as connecting_concept,\n",
    "           a.name as connecting_author\n",
    "    ORDER BY connection_strength DESC, citations DESC\n",
    "    LIMIT 5\n",
    "    \"\"\"\n",
    "    \n",
    "    results = []\n",
    "    query_terms = [term.strip().lower() for term in query.split() if len(term.strip()) > 3]\n",
    "    \n",
    "    for term in query_terms[:2]:  # Use first 2 terms\n",
    "        contextual_results = run_query(contextual_query, {'query_term': term})\n",
    "        \n",
    "        for result in contextual_results:\n",
    "            # Calculate contextual score based on connection strength and citations\n",
    "            base_score = min(result['connection_strength'] * 0.2, 1.0)\n",
    "            citation_bonus = min(result['citations'] / 10000, 0.3)\n",
    "            recency_bonus = max(0, (result['year'] - 2015) * 0.05)\n",
    "            \n",
    "            contextual_score = base_score + citation_bonus + recency_bonus\n",
    "            \n",
    "            results.append(RetrievalResult(\n",
    "                paper_id=result['paper_id'],\n",
    "                title=result['title'],\n",
    "                abstract=result['abstract'],\n",
    "                score=contextual_score,\n",
    "                retrieval_method='contextual_graph',\n",
    "                additional_info={\n",
    "                    'connection_strength': result['connection_strength'],\n",
    "                    'connecting_concept': result['connecting_concept'],\n",
    "                    'connecting_author': result['connecting_author'],\n",
    "                    'citations': result['citations']\n",
    "                }\n",
    "            ))\n",
    "    \n",
    "    return results\n",
    "\n",
    "def fuse_retrieval_results(vector_results: List[RetrievalResult], \n",
    "                          contextual_results: List[RetrievalResult], \n",
    "                          top_k: int) -> List[RetrievalResult]:\n",
    "    \"\"\"Intelligently fuse results from different retrieval methods\"\"\"\n",
    "    \n",
    "    fused_results = {}\n",
    "    \n",
    "    # Add vector results\n",
    "    for result in vector_results:\n",
    "        fused_results[result.paper_id] = RetrievalResult(\n",
    "            paper_id=result.paper_id,\n",
    "            title=result.title,\n",
    "            abstract=result.abstract,\n",
    "            score=result.score * 0.7,  # Weight vector similarity\n",
    "            retrieval_method='advanced_hybrid',\n",
    "            additional_info={\n",
    "                'vector_score': result.score,\n",
    "                'contextual_score': 0.0,\n",
    "                'methods': ['vector']\n",
    "            }\n",
    "        )\n",
    "    \n",
    "    # Add contextual results\n",
    "    for result in contextual_results:\n",
    "        if result.paper_id in fused_results:\n",
    "            # Boost existing result\n",
    "            existing = fused_results[result.paper_id]\n",
    "            existing.score += result.score * 0.5  # Weight contextual score\n",
    "            existing.additional_info['contextual_score'] = result.score\n",
    "            existing.additional_info['methods'].append('contextual')\n",
    "            existing.additional_info.update(result.additional_info)\n",
    "        else:\n",
    "            # Add new result\n",
    "            fused_results[result.paper_id] = RetrievalResult(\n",
    "                paper_id=result.paper_id,\n",
    "                title=result.title,\n",
    "                abstract=result.abstract,\n",
    "                score=result.score * 0.5,\n",
    "                retrieval_method='advanced_hybrid',\n",
    "                additional_info={\n",
    "                    'vector_score': 0.0,\n",
    "                    'contextual_score': result.score,\n",
    "                    'methods': ['contextual'],\n",
    "                    **result.additional_info\n",
    "                }\n",
    "            )\n",
    "    \n",
    "    return sorted(fused_results.values(), key=lambda x: x.score, reverse=True)[:top_k]\n",
    "\n",
    "# Test advanced hybrid retrieval\n",
    "print(\"Testing Advanced Hybrid Retrieval...\\n\")\n",
    "advanced_results = advanced_hybrid_retrieval(\"bidirectional encoder representations\", top_k=3)\n",
    "\n",
    "print(f\"\\nAdvanced Hybrid Retrieval Results for: 'bidirectional encoder representations'\\n\")\n",
    "for i, result in enumerate(advanced_results, 1):\n",
    "    print(f\"{i}. {result.title}\")\n",
    "    print(f\"   Final Score: {result.score:.4f}\")\n",
    "    info = result.additional_info\n",
    "    print(f\"   Component Scores - Vector: {info['vector_score']:.4f}, Contextual: {info['contextual_score']:.4f}\")\n",
    "    print(f\"   Methods: {', '.join(info['methods'])}\")\n",
    "    if 'connecting_concept' in info:\n",
    "        print(f\"   Connected via: {info['connecting_concept']} (Author: {info['connecting_author']})\")\n",
    "    print(f\"   Abstract: {result.abstract[:100]}...\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lesson 4: Retrieval Optimization and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieval evaluation and optimization\n",
    "def evaluate_retrieval_methods(test_queries: List[Dict[str, any]]) -> Dict[str, Dict[str, float]]:\n",
    "    \"\"\"Evaluate different retrieval methods on test queries\"\"\"\n",
    "    \n",
    "    methods = {\n",
    "        'vector_only': lambda q: vector_similarity_search(q, top_k=3),\n",
    "        'graph_only': lambda q: graph_traversal_search(q, max_depth=2)[:3],\n",
    "        'hybrid': lambda q: hybrid_retrieval(q, top_k=3),\n",
    "        'advanced_hybrid': lambda q: advanced_hybrid_retrieval(q, top_k=3)\n",
    "    }\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    for method_name, method_func in methods.items():\n",
    "        print(f\"Evaluating {method_name}...\")\n",
    "        \n",
    "        method_results = {\n",
    "            'precision_at_1': 0.0,\n",
    "            'precision_at_3': 0.0,\n",
    "            'avg_score': 0.0,\n",
    "            'coverage': 0.0\n",
    "        }\n",
    "        \n",
    "        for query_data in test_queries:\n",
    "            query = query_data['query']\n",
    "            relevant_papers = set(query_data['relevant_papers'])\n",
    "            \n",
    "            try:\n",
    "                retrieved = method_func(query)\n",
    "                retrieved_papers = {r.paper_id for r in retrieved}\n",
    "                \n",
    "                # Calculate metrics\n",
    "                if retrieved:\n",
    "                    # Precision@1\n",
    "                    if retrieved[0].paper_id in relevant_papers:\n",
    "                        method_results['precision_at_1'] += 1.0\n",
    "                    \n",
    "                    # Precision@3\n",
    "                    relevant_in_top3 = len(retrieved_papers.intersection(relevant_papers))\n",
    "                    method_results['precision_at_3'] += relevant_in_top3 / min(3, len(retrieved))\n",
    "                    \n",
    "                    # Average score\n",
    "                    method_results['avg_score'] += sum(r.score for r in retrieved) / len(retrieved)\n",
    "                    \n",
    "                    # Coverage\n",
    "                    method_results['coverage'] += len(retrieved_papers.intersection(relevant_papers)) / len(relevant_papers)\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"Error with {method_name} on query '{query}': {e}\")\n",
    "        \n",
    "        # Average the metrics\n",
    "        num_queries = len(test_queries)\n",
    "        for metric in method_results:\n",
    "            method_results[metric] /= num_queries\n",
    "        \n",
    "        results[method_name] = method_results\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Define test queries with relevant papers\n",
    "test_queries = [\n",
    "    {\n",
    "        'query': 'transformer attention mechanism',\n",
    "        'relevant_papers': ['paper1', 'paper2']  # Attention paper and BERT\n",
    "    },\n",
    "    {\n",
    "        'query': 'large language models few shot learning',\n",
    "        'relevant_papers': ['paper3']  # GPT-3 paper\n",
    "    },\n",
    "    {\n",
    "        'query': 'retrieval augmented generation knowledge',\n",
    "        'relevant_papers': ['paper4']  # RAG paper\n",
    "    },\n",
    "    {\n",
    "        'query': 'graph neural networks message passing',\n",
    "        'relevant_papers': ['paper5']  # GNN paper\n",
    "    }\n",
    "]\n",
    "\n",
    "# Run evaluation\n",
    "evaluation_results = evaluate_retrieval_methods(test_queries)\n",
    "\n",
    "# Display results\n",
    "print(\"\\n=== RETRIEVAL METHOD EVALUATION RESULTS ===\")\n",
    "print(f\"{'Method':<20} {'P@1':<8} {'P@3':<8} {'Avg Score':<10} {'Coverage':<10}\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "for method, metrics in evaluation_results.items():\n",
    "    print(f\"{method:<20} {metrics['precision_at_1']:.3f}    {metrics['precision_at_3']:.3f}    {metrics['avg_score']:.3f}      {metrics['coverage']:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Token-aware retrieval optimization\n",
    "def optimize_context_window(query: str, max_tokens: int = 2000) -> List[RetrievalResult]:\n",
    "    \"\"\"Optimize retrieval results to fit within token constraints\"\"\"\n",
    "    \n",
    "    # Get comprehensive results\n",
    "    results = advanced_hybrid_retrieval(query, top_k=10)\n",
    "    \n",
    "    # Calculate token usage for each result\n",
    "    optimized_results = []\n",
    "    current_tokens = 0\n",
    "    \n",
    "    # Reserve tokens for query and system prompt\n",
    "    query_tokens = len(tokenizer.encode(query))\n",
    "    system_tokens = 200  # Estimate for system prompt\n",
    "    available_tokens = max_tokens - query_tokens - system_tokens\n",
    "    \n",
    "    for result in results:\n",
    "        # Calculate tokens for this result\n",
    "        content = f\"Title: {result.title}\\nAbstract: {result.abstract}\"\n",
    "        result_tokens = len(tokenizer.encode(content))\n",
    "        \n",
    "        if current_tokens + result_tokens <= available_tokens:\n",
    "            optimized_results.append(result)\n",
    "            current_tokens += result_tokens\n",
    "        else:\n",
    "            # Try to truncate abstract to fit\n",
    "            truncated_abstract = result.abstract\n",
    "            while len(tokenizer.encode(f\"Title: {result.title}\\nAbstract: {truncated_abstract}\")) > available_tokens - current_tokens and len(truncated_abstract) > 50:\n",
    "                truncated_abstract = truncated_abstract[:-50]\n",
    "            \n",
    "            if len(truncated_abstract) > 50:\n",
    "                truncated_result = RetrievalResult(\n",
    "                    paper_id=result.paper_id,\n",
    "                    title=result.title,\n",
    "                    abstract=truncated_abstract + \"...\",\n",
    "                    score=result.score,\n",
    "                    retrieval_method=result.retrieval_method,\n",
    "                    additional_info=result.additional_info\n",
    "                )\n",
    "                optimized_results.append(truncated_result)\n",
    "                current_tokens += len(tokenizer.encode(f\"Title: {result.title}\\nAbstract: {truncated_abstract}...\"))\n",
    "            \n",
    "            break\n",
    "    \n",
    "    return optimized_results, current_tokens\n",
    "\n",
    "# Test token optimization\n",
    "optimized_results, token_count = optimize_context_window(\"transformer models with attention mechanisms\", max_tokens=1500)\n",
    "\n",
    "print(f\"Token-Optimized Retrieval Results (Total tokens: {token_count})\\n\")\n",
    "for i, result in enumerate(optimized_results, 1):\n",
    "    content = f\"Title: {result.title}\\nAbstract: {result.abstract}\"\n",
    "    result_tokens = len(tokenizer.encode(content))\n",
    "    print(f\"{i}. {result.title} ({result_tokens} tokens)\")\n",
    "    print(f\"   Score: {result.score:.4f}\")\n",
    "    print(f\"   Abstract: {result.abstract[:100]}...\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hands-on Exercise: Complete GraphRAG Retrieval System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complete GraphRAG retrieval system\n",
    "class GraphRAGRetriever:\n",
    "    \"\"\"Complete GraphRAG retrieval system with multiple strategies\"\"\"\n",
    "    \n",
    "    def __init__(self, neo4j_driver, embedding_model):\n",
    "        self.driver = neo4j_driver\n",
    "        self.embedding_model = embedding_model\n",
    "        self.tokenizer = tiktoken.get_encoding(\"cl100k_base\")\n",
    "    \n",
    "    def retrieve(self, query: str, method: str = 'advanced_hybrid', \n",
    "                top_k: int = 5, max_tokens: int = 2000) -> Dict[str, any]:\n",
    "        \"\"\"Main retrieval method with multiple strategies\"\"\"\n",
    "        \n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Choose retrieval method\n",
    "        if method == 'vector':\n",
    "            results = vector_similarity_search(query, top_k)\n",
    "        elif method == 'graph':\n",
    "            results = graph_traversal_search(query, max_depth=2)[:top_k]\n",
    "        elif method == 'hybrid':\n",
    "            results = hybrid_retrieval(query, top_k)\n",
    "        elif method == 'advanced_hybrid':\n",
    "            results = advanced_hybrid_retrieval(query, top_k)\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown method: {method}\")\n",
    "        \n",
    "        # Optimize for token constraints\n",
    "        optimized_results, token_count = optimize_context_window(query, max_tokens)\n",
    "        \n",
    "        # Prepare context for RAG\n",
    "        context = self._prepare_context(optimized_results)\n",
    "        \n",
    "        retrieval_time = time.time() - start_time\n",
    "        \n",
    "        return {\n",
    "            'query': query,\n",
    "            'method': method,\n",
    "            'results': optimized_results,\n",
    "            'context': context,\n",
    "            'token_count': token_count,\n",
    "            'retrieval_time': retrieval_time,\n",
    "            'metadata': {\n",
    "                'num_results': len(optimized_results),\n",
    "                'avg_score': sum(r.score for r in optimized_results) / len(optimized_results) if optimized_results else 0\n",
    "            }\n",
    "        }\n",
    "    \n",
    "    def _prepare_context(self, results: List[RetrievalResult]) -> str:\n",
    "        \"\"\"Prepare formatted context for RAG\"\"\"\n",
    "        \n",
    "        context_parts = []\n",
    "        \n",
    "        for i, result in enumerate(results, 1):\n",
    "            context_part = f\"\"\"[Document {i}]\n",
    "Title: {result.title}\n",
    "Abstract: {result.abstract}\n",
    "Retrieval Score: {result.score:.4f}\n",
    "Method: {result.retrieval_method}\n",
    "\"\"\"\n",
    "            \n",
    "            if result.additional_info:\n",
    "                info = result.additional_info\n",
    "                if 'matched_concept' in info:\n",
    "                    context_part += f\"Matched Concept: {info['matched_concept']}\\n\"\n",
    "                if 'connecting_author' in info:\n",
    "                    context_part += f\"Connected via Author: {info['connecting_author']}\\n\"\n",
    "                if 'field' in info:\n",
    "                    context_part += f\"Field: {info['field']}\\n\"\n",
    "            \n",
    "            context_parts.append(context_part)\n",
    "        \n",
    "        return \"\\n\".join(context_parts)\n",
    "    \n",
    "    def batch_retrieve(self, queries: List[str], method: str = 'advanced_hybrid') -> List[Dict[str, any]]:\n",
    "        \"\"\"Batch retrieval for multiple queries\"\"\"\n",
    "        return [self.retrieve(query, method) for query in queries]\n",
    "    \n",
    "    def explain_retrieval(self, query: str, method: str = 'advanced_hybrid') -> Dict[str, any]:\n",
    "        \"\"\"Explain how retrieval worked for debugging\"\"\"\n",
    "        \n",
    "        result = self.retrieve(query, method)\n",
    "        \n",
    "        explanation = {\n",
    "            'query_analysis': {\n",
    "                'query': query,\n",
    "                'query_terms': [term.strip().lower() for term in query.split() if len(term.strip()) > 3],\n",
    "                'query_tokens': len(self.tokenizer.encode(query))\n",
    "            },\n",
    "            'retrieval_details': {\n",
    "                'method_used': method,\n",
    "                'num_results': len(result['results']),\n",
    "                'total_tokens': result['token_count'],\n",
    "                'retrieval_time': result['retrieval_time']\n",
    "            },\n",
    "            'result_breakdown': []\n",
    "        }\n",
    "        \n",
    "        for result_obj in result['results']:\n",
    "            breakdown = {\n",
    "                'paper_id': result_obj.paper_id,\n",
    "                'title': result_obj.title,\n",
    "                'final_score': result_obj.score,\n",
    "                'retrieval_method': result_obj.retrieval_method\n",
    "            }\n",
    "            \n",
    "            if result_obj.additional_info:\n",
    "                breakdown['score_components'] = result_obj.additional_info\n",
    "            \n",
    "            explanation['result_breakdown'].append(breakdown)\n",
    "        \n",
    "        return explanation\n",
    "\n",
    "# Initialize the complete GraphRAG retriever\n",
    "graphrag_retriever = GraphRAGRetriever(driver, embedding_model)\n",
    "\n",
    "# Test the complete system\n",
    "test_query = \"How do attention mechanisms work in transformer models?\"\n",
    "retrieval_result = graphrag_retriever.retrieve(test_query, method='advanced_hybrid', top_k=3)\n",
    "\n",
    "print(\"=== COMPLETE GRAPHRAG RETRIEVAL SYSTEM TEST ===\")\n",
    "print(f\"Query: {test_query}\")\n",
    "print(f\"Method: {retrieval_result['method']}\")\n",
    "print(f\"Results: {retrieval_result['metadata']['num_results']}\")\n",
    "print(f\"Tokens: {retrieval_result['token_count']}\")\n",
    "print(f\"Time: {retrieval_result['retrieval_time']:.3f}s\")\n",
    "print(f\"Avg Score: {retrieval_result['metadata']['avg_score']:.4f}\")\n",
    "\n",
    "print(\"\\nRetrieved Context:\")\n",
    "print(\"=\" * 50)\n",
    "print(retrieval_result['context'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test explanation functionality\n",
    "explanation = graphrag_retriever.explain_retrieval(\"bidirectional language model pre-training\", method='advanced_hybrid')\n",
    "\n",
    "print(\"\\n=== RETRIEVAL EXPLANATION ===\")\n",
    "print(f\"Query: {explanation['query_analysis']['query']}\")\n",
    "print(f\"Query Terms: {', '.join(explanation['query_analysis']['query_terms'])}\")\n",
    "print(f\"Query Tokens: {explanation['query_analysis']['query_tokens']}\")\n",
    "\n",
    "print(f\"\\nRetrieval Method: {explanation['retrieval_details']['method_used']}\")\n",
    "print(f\"Results Retrieved: {explanation['retrieval_details']['num_results']}\")\n",
    "print(f\"Total Context Tokens: {explanation['retrieval_details']['total_tokens']}\")\n",
    "print(f\"Retrieval Time: {explanation['retrieval_details']['retrieval_time']:.3f}s\")\n",
    "\n",
    "print(\"\\nResult Breakdown:\")\n",
    "for i, result in enumerate(explanation['result_breakdown'], 1):\n",
    "    print(f\"\\n{i}. {result['title']}\")\n",
    "    print(f\"   Paper ID: {result['paper_id']}\")\n",
    "    print(f\"   Final Score: {result['final_score']:.4f}\")\n",
    "    print(f\"   Method: {result['retrieval_method']}\")\n",
    "    \n",
    "    if 'score_components' in result:\n",
    "        components = result['score_components']\n",
    "        print(f\"   Score Components:\")\n",
    "        if 'vector_score' in components:\n",
    "            print(f\"     - Vector Score: {components['vector_score']:.4f}\")\n",
    "        if 'contextual_score' in components:\n",
    "            print(f\"     - Contextual Score: {components['contextual_score']:.4f}\")\n",
    "        if 'methods' in components:\n",
    "            print(f\"     - Methods Used: {', '.join(components['methods'])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Module Summary and Next Steps\n",
    "\n",
    "In this module, you learned to:\n",
    "- Implement vector-based semantic retrieval with Neo4j\n",
    "- Design sophisticated graph traversal retrieval patterns\n",
    "- Combine multiple retrieval strategies for optimal results\n",
    "- Optimize retrieval for token constraints and performance\n",
    "- Build complete GraphRAG systems with explanation capabilities\n",
    "\n",
    "### Key Takeaways\n",
    "- **Vector Similarity**: Provides semantic understanding and content-based matching\n",
    "- **Graph Traversal**: Reveals structural relationships and contextual connections\n",
    "- **Hybrid Approaches**: Combine strengths of multiple methods for better results\n",
    "- **Token Optimization**: Essential for real-world LLM applications with context limits\n",
    "- **Explainability**: Critical for debugging and improving retrieval systems\n",
    "\n",
    "### Business Applications\n",
    "- **Research Discovery**: Find related papers and concepts through graph relationships\n",
    "- **Document Retrieval**: Combine semantic and structural search for enterprise knowledge\n",
    "- **Recommendation Systems**: Use graph connections to suggest relevant content\n",
    "- **Question Answering**: Provide rich context for LLM-powered Q&A systems\n",
    "\n",
    "### Performance Insights\n",
    "- Vector similarity excels at semantic matching but may miss structural relationships\n",
    "- Graph traversal finds unexpected connections but can be computationally expensive\n",
    "- Hybrid methods consistently outperform single approaches\n",
    "- Token optimization is crucial for production deployments\n",
    "\n",
    "### Next Module\n",
    "Module 6: Agents - Learn how to build intelligent agents that can reason over graph knowledge and take actions based on retrieved information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final performance benchmark\n",
    "print(\"\\n=== FINAL PERFORMANCE BENCHMARK ===\")\n",
    "\n",
    "benchmark_queries = [\n",
    "    \"transformer attention mechanisms neural networks\",\n",
    "    \"large language models few shot learning\",\n",
    "    \"graph neural networks message passing\",\n",
    "    \"retrieval augmented generation knowledge\",\n",
    "    \"bidirectional encoder representations\"\n",
    "]\n",
    "\n",
    "methods_to_test = ['vector', 'graph', 'hybrid', 'advanced_hybrid']\n",
    "performance_data = []\n",
    "\n",
    "for method in methods_to_test:\n",
    "    total_time = 0\n",
    "    total_results = 0\n",
    "    total_score = 0\n",
    "    \n",
    "    for query in benchmark_queries:\n",
    "        try:\n",
    "            result = graphrag_retriever.retrieve(query, method=method, top_k=3)\n",
    "            total_time += result['retrieval_time']\n",
    "            total_results += result['metadata']['num_results']\n",
    "            total_score += result['metadata']['avg_score']\n",
    "        except Exception as e:\n",
    "            print(f\"Error with {method}: {e}\")\n",
    "    \n",
    "    avg_time = total_time / len(benchmark_queries)\n",
    "    avg_results = total_results / len(benchmark_queries)\n",
    "    avg_score = total_score / len(benchmark_queries)\n",
    "    \n",
    "    performance_data.append({\n",
    "        'method': method,\n",
    "        'avg_time': avg_time,\n",
    "        'avg_results': avg_results,\n",
    "        'avg_score': avg_score\n",
    "    })\n",
    "\n",
    "print(f\"{'Method':<18} {'Avg Time (s)':<12} {'Avg Results':<12} {'Avg Score':<10}\")\n",
    "print(\"-\" * 55)\n",
    "for perf in performance_data:\n",
    "    print(f\"{perf['method']:<18} {perf['avg_time']:<12.3f} {perf['avg_results']:<12.1f} {perf['avg_score']:<10.4f}\")\n",
    "\n",
    "print(\"\\n Module 5: Retrievers completed successfully!\")\n",
    "print(\"You're now ready to move on to Module 6: Agents\")\n",
    "\n",
    "# Cleanup (optional)\n",
    "# driver.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}