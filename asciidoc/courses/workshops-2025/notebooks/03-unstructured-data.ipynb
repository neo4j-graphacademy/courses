{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 3: Unstructured Data Processing with Neo4j\n",
    "\n",
    "This notebook covers extracting, processing, and storing unstructured data in Neo4j for knowledge graph construction and AI applications.\n",
    "\n",
    "## Learning Objectives\n",
    "- Extract entities and relationships from text using NLP\n",
    "- Process documents and create knowledge graphs\n",
    "- Generate embeddings for semantic analysis\n",
    "- Build document-based graph structures\n",
    "\n",
    "## Prerequisites\n",
    "- Completion of Module 2: Structured Data\n",
    "- Basic understanding of NLP concepts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install neo4j pandas numpy spacy openai sentence-transformers nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "## Install Required NLP and ML Packages\n\nFor unstructured data processing, we need specialized packages:\n\n- **spacy**: Advanced NLP library for entity extraction and language processing\n- **openai**: API access for advanced language models (optional)\n- **sentence-transformers**: Pre-trained models for creating semantic embeddings\n- **nltk**: Natural Language Toolkit for text preprocessing\n- **neo4j, pandas, numpy**: Core data processing and graph database connectivity\n\nThese packages enable sophisticated text analysis and knowledge graph construction.",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "## Import NLP Libraries and Download Language Resources\n\n**Library Setup:**\n- **spacy & nltk**: For natural language processing\n- **sentence_transformers**: For generating semantic embeddings\n- **numpy**: For numerical operations on embeddings\n- **typing**: For type hints and better code documentation\n\n**NLTK Downloads:**\n- **punkt**: Sentence tokenization models\n- **stopwords**: Common words to filter out during processing\n\n**Important Note:** The `nltk.download()` commands download necessary language models and data files for text processing.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from neo4j import GraphDatabase\n",
    "import spacy\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import openai\n",
    "import json\n",
    "import re\n",
    "from typing import List, Dict, Tuple\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Download NLTK data\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "## Establish Neo4j Connection for Knowledge Graphs\n\n**Connection Setup for Text Processing:**\n- Load credentials from environment variables for security\n- Create a reusable connection function for graph operations\n- Test the connection to ensure we can store extracted knowledge\n\n**Knowledge Graph Storage:**\nUnlike structured data import, unstructured data processing creates more complex graph structures:\n- Documents become nodes\n- Extracted entities become nodes\n- Relationships between entities are discovered through text analysis\n- Embeddings enable semantic search and similarity analysis",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neo4j Connection Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "## Initialize NLP Models for Text Processing\n\n**SpaCy Model Setup:**\n- Download and load the English language model (`en_core_web_sm`)\n- This model includes pre-trained capabilities for named entity recognition\n- Provides part-of-speech tagging, dependency parsing, and more\n\n**Sentence Transformer Setup:**\n- Load `all-MiniLM-L6-v2` - a lightweight but effective embedding model\n- This model converts text into 384-dimensional vectors\n- Enables semantic similarity calculations and vector search\n\n**Why These Models:**\n- SpaCy excels at extracting structured information from text\n- Sentence transformers capture semantic meaning for similarity search\n- Together they enable both symbolic and semantic knowledge extraction",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "## Create Sample Document Collection\n\n**Document Data Structure:**\nEach document contains:\n- **id**: Unique identifier for tracking\n- **title**: Document title for human reference\n- **content**: The actual text content to be processed\n\n**Sample Content Themes:**\n- **AI Research**: Academic and technical content with people, organizations, and technologies\n- **Business Partnerships**: Corporate entities, partnerships, and strategic relationships  \n- **Technology Innovation**: Products, companies, and technical developments\n\n**Why This Data:**\nThese documents contain rich entity relationships perfect for demonstrating knowledge graph construction from unstructured text.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Neo4j connection settings\n",
    "NEO4J_URI = os.getenv('NEO4J_URI', 'bolt://localhost:7687')\n",
    "NEO4J_USERNAME = os.getenv('NEO4J_USERNAME', 'neo4j')\n",
    "NEO4J_PASSWORD = os.getenv('NEO4J_PASSWORD', 'password')\n",
    "\n",
    "# Create Neo4j driver\n",
    "driver = GraphDatabase.driver(NEO4J_URI, auth=(NEO4J_USERNAME, NEO4J_PASSWORD))\n",
    "\n",
    "def run_query(query, parameters=None):\n",
    "    \"\"\"Execute a Cypher query and return results\"\"\"\n",
    "    with driver.session() as session:\n",
    "        result = session.run(query, parameters or {})\n",
    "        return [record.data() for record in result]\n",
    "\n",
    "# Test connection\n",
    "print(\"Testing Neo4j connection...\")\n",
    "result = run_query(\"RETURN 'Connected to Neo4j!' as message\")\n",
    "print(result[0]['message'])"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "## Define Entity and Relationship Extraction Functions\n\n**Entity Extraction with SpaCy:**\nThe `extract_entities()` function uses SpaCy's named entity recognition to find:\n- **PERSON**: Names of people (Dr. Sarah Johnson, Prof. Michael Chen)\n- **ORG**: Organizations (Stanford University, Google AI, Microsoft)\n- **GPE**: Geographic/political entities (countries, cities, states)\n- **PRODUCT**: Products and technologies (GPT models, neural networks)\n\n**Relationship Extraction via Dependency Parsing:**\nThe `extract_relationships()` function identifies grammatical relationships:\n- **nsubj**: Nominal subject (who is doing something)\n- **dobj**: Direct object (what is being done to)\n- **pobj**: Object of preposition (relationships through prepositions)\n\n**Learning Point:** \nNLP enables automatic discovery of entities and relationships that would be impossible to extract manually from large document collections.",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "## Create Knowledge Graph from Extracted Entities\n\n**Graph Construction Pattern:**\nThe `create_document_graph()` function demonstrates how to build knowledge graphs from NLP output:\n\n1. **Document Nodes**: Each document becomes a node with metadata\n2. **Entity Nodes**: Each extracted entity becomes a separate node\n3. **CONTAINS_ENTITY Relationships**: Connect documents to their entities\n4. **Position Tracking**: Store where entities appear in the source text\n\n**Graph Design Benefits:**\n- **Searchable entities**: Find all documents mentioning specific people/organizations\n- **Entity relationships**: Discover connections between entities across documents\n- **Provenance tracking**: Know exactly where information came from\n- **Scalable structure**: Easy to add new documents and entities",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lesson 1: Text Processing and NLP Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "## Generate Vector Embeddings for Semantic Search\n\n**Text Chunking Strategy:**\n- Break documents into smaller, overlapping chunks (100 words with 20-word overlap)\n- Overlapping ensures important concepts aren't split across chunks\n- Smaller chunks provide more precise search results\n\n**Embedding Generation:**\n- Convert each text chunk into a 384-dimensional vector\n- These vectors capture semantic meaning beyond keyword matching\n- Similar concepts have similar vectors (high cosine similarity)\n\n**Graph Storage:**\n- **Chunk nodes**: Store text chunks as separate entities\n- **HAS_CHUNK relationships**: Connect documents to their chunks\n- **Embedding vectors**: Store as array properties for vector search\n\n**Why This Approach:**\nChunking + embeddings enables finding relevant content even when exact keywords don't match the query.",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "## Create Vector Search Index for Fast Similarity Queries\n\n**Vector Index Configuration:**\n- **384 dimensions**: Matches our sentence transformer model output\n- **Cosine similarity**: Best for text embeddings (measures angle between vectors)\n- **Index name**: `chunk_embeddings` for easy reference\n\n**Why Vector Indexes Matter:**\n- **Performance**: Enables sub-second search across thousands of vectors\n- **Similarity function**: Cosine similarity is optimal for text semantics\n- **Scalability**: Index structure supports millions of embeddings\n\n**Error Handling:**\nThe try-catch block handles cases where the index already exists, which is common when re-running notebook cells.",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "## Implement and Test Semantic Search\n\n**Semantic Search Process:**\n1. **Query embedding**: Convert search query to vector using same model\n2. **Vector similarity**: Use Neo4j's vector search to find similar chunks\n3. **Graph enrichment**: Include document context and metadata\n4. **Ranking**: Results ordered by similarity score\n\n**Search Query Breakdown:**\n- `db.index.vector.queryNodes()`: Neo4j's vector search function\n- Returns chunks ranked by cosine similarity to query\n- Joins with document metadata for context\n\n**Test Search:**\nQuery \"AI research and neural networks\" should find relevant content even if exact phrases don't appear in the text, demonstrating semantic understanding beyond keyword matching.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load spaCy model for NLP processing\n",
    "!python -m spacy download en_core_web_sm\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Initialize sentence transformer for embeddings\n",
    "embedding_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "print(\"NLP models loaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "## Discover Entity Co-occurrence Patterns\n\n**Co-occurrence Analysis:**\nThis function finds entities that appear together in the same documents:\n- **Pattern detection**: Entities mentioned in the same document likely have relationships\n- **Frequency counting**: How often entities co-occur indicates relationship strength\n- **Graph relationships**: Create CO_OCCURS_WITH edges between related entities\n\n**Knowledge Discovery:**\n- **Automatic relationship detection**: No manual relationship definition needed\n- **Statistical significance**: Frequency indicates relationship importance\n- **Network effects**: Entities become connected through shared contexts\n\n**Applications:**\n- **Recommendation systems**: Find related entities to suggest\n- **Knowledge exploration**: Discover unexpected connections\n- **Graph analysis**: Enable centrality and community detection algorithms",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "## Create Entity Similarity Using Context Embeddings\n\n**Context-Based Similarity:**\nInstead of just co-occurrence counting, this approach uses semantic similarity:\n1. **Context extraction**: Gather text around each entity mention\n2. **Context embeddings**: Generate vectors representing each entity's typical context\n3. **Similarity calculation**: Use cosine similarity between context vectors\n4. **Relationship creation**: Connect entities with high semantic similarity\n\n**Advanced Concept:**\nThis creates a more nuanced understanding of entity relationships:\n- Entities are similar if they appear in similar contexts\n- Goes beyond simple co-occurrence to semantic relatedness\n- Enables discovery of conceptually related entities even from different documents\n\n**Threshold Tuning:**\nThe 0.7 similarity threshold can be adjusted based on desired precision vs. recall.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample unstructured text data\n",
    "sample_documents = [\n",
    "    {\n",
    "        \"id\": \"doc1\",\n",
    "        \"title\": \"AI Research Breakthrough\",\n",
    "        \"content\": \"Dr. Sarah Johnson from Stanford University published groundbreaking research on neural networks. The study, conducted in collaboration with Google AI, demonstrates significant improvements in natural language processing. The research team, including Prof. Michael Chen from MIT, used advanced transformer architectures to achieve state-of-the-art results.\"\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"doc2\",\n",
    "        \"title\": \"Tech Company Partnership\",\n",
    "        \"content\": \"Microsoft announced a strategic partnership with OpenAI to develop advanced AI systems. The collaboration focuses on integrating GPT models into Microsoft's suite of productivity tools. CEO Satya Nadella emphasized the importance of responsible AI development in the announcement.\"\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"doc3\",\n",
    "        \"title\": \"Database Innovation\",\n",
    "        \"content\": \"Neo4j released new graph database capabilities for AI applications. The update includes enhanced vector search functionality and improved integration with machine learning workflows. CTO Jim Webber highlighted the benefits for knowledge graph construction and graph neural networks.\"\n",
    "    }\n",
    "]\n",
    "\n",
    "print(f\"Loaded {len(sample_documents)} sample documents\")\n",
    "for doc in sample_documents[:1]:\n",
    "    print(f\"\\nDocument: {doc['title']}\")\n",
    "    print(f\"Content: {doc['content'][:100]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "## Complete Knowledge Graph Construction\n\n**Scale Up Processing:**\nNow we process all remaining documents to build a comprehensive knowledge graph:\n- Extract entities from each document\n- Create document and entity nodes\n- Build relationships between entities\n\n**Knowledge Graph Growth:**\nAs we add more documents:\n- New entities are discovered\n- Existing entities gain more connections\n- Co-occurrence patterns become more reliable\n- The graph becomes richer and more connected\n\n**Processing Feedback:**\nThe loop provides feedback on entities found per document, helping you understand the richness of your knowledge extraction process.",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "## Comprehensive Knowledge Graph Analysis\n\n**Multi-Perspective Analysis:**\nThe `analyze_knowledge_graph()` function provides insights from multiple angles:\n\n1. **Document Statistics**: How many entities and chunks per document\n2. **Entity Connectivity**: Which entities are most connected (central in the graph)\n3. **Graph Structure**: Overall composition of nodes by type\n\n**Key Metrics:**\n- **Entity count**: Richness of information extraction\n- **Connection count**: How well entities are linked (graph density)\n- **Node distribution**: Balance of different node types\n\n**Business Value:**\nThese metrics help assess the quality and completeness of your knowledge extraction pipeline and identify the most important entities in your domain.",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "## Advanced Hybrid Search: Combining Semantic and Graph Intelligence\n\n**Multi-Modal Search Approach:**\nThe `hybrid_search()` function demonstrates how to combine different search methods:\n\n1. **Semantic search**: Find content similar to the query using embeddings\n2. **Graph traversal**: Discover related entities through graph relationships\n3. **Combined results**: Merge both approaches for comprehensive results\n\n**Hybrid Search Benefits:**\n- **Semantic matching**: Finds conceptually related content\n- **Graph enrichment**: Discovers connected entities and concepts\n- **Comprehensive coverage**: Captures both direct matches and related information\n\n**Real-World Application:**\nThis pattern is used in advanced search engines, recommendation systems, and knowledge discovery platforms to provide more complete and contextual results than either approach alone.",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lesson 2: Entity Extraction and Relationship Discovery"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "## Module Completion and Resource Cleanup\n\n**Optional Cleanup:**\nThe commented `driver.close()` line shows how to properly close the Neo4j connection when you're completely done with the session.\n\n**Why Keep Connection Open:**\nIn Jupyter notebooks, you might want to keep the connection open for additional experimentation and queries after completing the main module exercises.\n\n**Production Consideration:**\nIn production applications, always ensure proper resource cleanup with try-finally blocks or context managers.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_entities(text: str) -> List[Dict]:\n",
    "    \"\"\"Extract named entities from text using spaCy\"\"\"\n",
    "    doc = nlp(text)\n",
    "    entities = []\n",
    "    \n",
    "    for ent in doc.ents:\n",
    "        entities.append({\n",
    "            \"text\": ent.text,\n",
    "            \"label\": ent.label_,\n",
    "            \"start\": ent.start_char,\n",
    "            \"end\": ent.end_char,\n",
    "            \"description\": spacy.explain(ent.label_)\n",
    "        })\n",
    "    \n",
    "    return entities\n",
    "\n",
    "def extract_relationships(text: str) -> List[Dict]:\n",
    "    \"\"\"Extract basic relationships using dependency parsing\"\"\"\n",
    "    doc = nlp(text)\n",
    "    relationships = []\n",
    "    \n",
    "    for token in doc:\n",
    "        if token.dep_ in ['nsubj', 'dobj', 'pobj'] and token.head.pos_ == 'VERB':\n",
    "            relationships.append({\n",
    "                \"subject\": token.text,\n",
    "                \"predicate\": token.head.text,\n",
    "                \"object\": token.head.text,\n",
    "                \"dependency\": token.dep_\n",
    "            })\n",
    "    \n",
    "    return relationships\n",
    "\n",
    "# Process sample document\n",
    "sample_text = sample_documents[0]['content']\n",
    "entities = extract_entities(sample_text)\n",
    "relationships = extract_relationships(sample_text)\n",
    "\n",
    "print(\"Extracted Entities:\")\n",
    "for entity in entities:\n",
    "    print(f\"- {entity['text']} ({entity['label']}) - {entity['description']}\")\n",
    "\n",
    "print(\"\\nExtracted Relationships:\")\n",
    "for rel in relationships[:5]:  # Show first 5\n",
    "    print(f\"- {rel['subject']} --{rel['dependency']}--> {rel['predicate']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create document nodes and entity relationships in Neo4j\n",
    "def create_document_graph(doc_data: Dict, entities: List[Dict]):\n",
    "    \"\"\"Create document and entity nodes with relationships\"\"\"\n",
    "    \n",
    "    # Create document node\n",
    "    doc_query = \"\"\"\n",
    "    MERGE (d:Document {id: $doc_id})\n",
    "    SET d.title = $title,\n",
    "        d.content = $content,\n",
    "        d.created_at = datetime()\n",
    "    \"\"\"\n",
    "    \n",
    "    run_query(doc_query, {\n",
    "        'doc_id': doc_data['id'],\n",
    "        'title': doc_data['title'],\n",
    "        'content': doc_data['content']\n",
    "    })\n",
    "    \n",
    "    # Create entity nodes and relationships\n",
    "    for entity in entities:\n",
    "        entity_query = \"\"\"\n",
    "        MATCH (d:Document {id: $doc_id})\n",
    "        MERGE (e:Entity {text: $entity_text})\n",
    "        SET e.label = $entity_label,\n",
    "            e.description = $entity_description\n",
    "        MERGE (d)-[:CONTAINS_ENTITY {\n",
    "            start_pos: $start_pos,\n",
    "            end_pos: $end_pos\n",
    "        }]->(e)\n",
    "        \"\"\"\n",
    "        \n",
    "        run_query(entity_query, {\n",
    "            'doc_id': doc_data['id'],\n",
    "            'entity_text': entity['text'],\n",
    "            'entity_label': entity['label'],\n",
    "            'entity_description': entity['description'],\n",
    "            'start_pos': entity['start'],\n",
    "            'end_pos': entity['end']\n",
    "        })\n",
    "\n",
    "# Process first document\n",
    "create_document_graph(sample_documents[0], entities)\n",
    "print(f\"Created graph for document: {sample_documents[0]['title']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lesson 3: Vector Embeddings and Semantic Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_embeddings(texts: List[str]) -> np.ndarray:\n",
    "    \"\"\"Generate embeddings for a list of texts\"\"\"\n",
    "    embeddings = embedding_model.encode(texts)\n",
    "    return embeddings\n",
    "\n",
    "def chunk_text(text: str, chunk_size: int = 200, overlap: int = 50) -> List[str]:\n",
    "    \"\"\"Split text into overlapping chunks\"\"\"\n",
    "    words = text.split()\n",
    "    chunks = []\n",
    "    \n",
    "    for i in range(0, len(words), chunk_size - overlap):\n",
    "        chunk = ' '.join(words[i:i + chunk_size])\n",
    "        if chunk.strip():\n",
    "            chunks.append(chunk)\n",
    "    \n",
    "    return chunks\n",
    "\n",
    "# Process all documents with embeddings\n",
    "for doc in sample_documents:\n",
    "    # Create text chunks\n",
    "    chunks = chunk_text(doc['content'], chunk_size=100, overlap=20)\n",
    "    \n",
    "    # Generate embeddings for chunks\n",
    "    chunk_embeddings = generate_embeddings(chunks)\n",
    "    \n",
    "    # Store chunks and embeddings in Neo4j\n",
    "    for i, (chunk, embedding) in enumerate(zip(chunks, chunk_embeddings)):\n",
    "        chunk_query = \"\"\"\n",
    "        MATCH (d:Document {id: $doc_id})\n",
    "        MERGE (c:Chunk {id: $chunk_id})\n",
    "        SET c.text = $chunk_text,\n",
    "            c.position = $position,\n",
    "            c.embedding = $embedding\n",
    "        MERGE (d)-[:HAS_CHUNK]->(c)\n",
    "        \"\"\"\n",
    "        \n",
    "        run_query(chunk_query, {\n",
    "            'doc_id': doc['id'],\n",
    "            'chunk_id': f\"{doc['id']}_chunk_{i}\",\n",
    "            'chunk_text': chunk,\n",
    "            'position': i,\n",
    "            'embedding': embedding.tolist()\n",
    "        })\n",
    "    \n",
    "    print(f\"Processed {len(chunks)} chunks for document: {doc['title']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create vector search index\n",
    "index_query = \"\"\"\n",
    "CREATE VECTOR INDEX chunk_embeddings IF NOT EXISTS\n",
    "FOR (c:Chunk) ON (c.embedding)\n",
    "OPTIONS {\n",
    "  indexConfig: {\n",
    "    `vector.dimensions`: 384,\n",
    "    `vector.similarity_function`: 'cosine'\n",
    "  }\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "try:\n",
    "    run_query(index_query)\n",
    "    print(\"Vector index created successfully\")\n",
    "except Exception as e:\n",
    "    print(f\"Vector index may already exist: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Semantic search function\n",
    "def semantic_search(query_text: str, top_k: int = 3) -> List[Dict]:\n",
    "    \"\"\"Perform semantic search using vector similarity\"\"\"\n",
    "    # Generate query embedding\n",
    "    query_embedding = generate_embeddings([query_text])[0]\n",
    "    \n",
    "    # Search similar chunks\n",
    "    search_query = \"\"\"\n",
    "    CALL db.index.vector.queryNodes('chunk_embeddings', $top_k, $query_embedding)\n",
    "    YIELD node AS chunk, score\n",
    "    MATCH (d:Document)-[:HAS_CHUNK]->(chunk)\n",
    "    RETURN d.title as document_title, \n",
    "           chunk.text as chunk_text,\n",
    "           score,\n",
    "           chunk.position as position\n",
    "    ORDER BY score DESC\n",
    "    \"\"\"\n",
    "    \n",
    "    results = run_query(search_query, {\n",
    "        'query_embedding': query_embedding.tolist(),\n",
    "        'top_k': top_k\n",
    "    })\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Test semantic search\n",
    "query = \"AI research and neural networks\"\n",
    "search_results = semantic_search(query, top_k=3)\n",
    "\n",
    "print(f\"Search results for: '{query}'\\n\")\n",
    "for i, result in enumerate(search_results, 1):\n",
    "    print(f\"{i}. Document: {result['document_title']}\")\n",
    "    print(f\"   Score: {result['score']:.4f}\")\n",
    "    print(f\"   Text: {result['chunk_text'][:100]}...\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lesson 4: Advanced Text Processing and Knowledge Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Advanced entity linking and co-occurrence analysis\n",
    "def find_entity_cooccurrences() -> List[Dict]:\n",
    "    \"\"\"Find entities that frequently co-occur in documents\"\"\"\n",
    "    cooccurrence_query = \"\"\"\n",
    "    MATCH (d:Document)-[:CONTAINS_ENTITY]->(e1:Entity),\n",
    "          (d)-[:CONTAINS_ENTITY]->(e2:Entity)\n",
    "    WHERE e1 <> e2 AND e1.text < e2.text  // Avoid duplicates\n",
    "    WITH e1, e2, count(d) as cooccurrence_count\n",
    "    WHERE cooccurrence_count > 0\n",
    "    MERGE (e1)-[r:CO_OCCURS_WITH]-(e2)\n",
    "    SET r.frequency = cooccurrence_count\n",
    "    RETURN e1.text as entity1, e2.text as entity2, cooccurrence_count\n",
    "    ORDER BY cooccurrence_count DESC\n",
    "    \"\"\"\n",
    "    \n",
    "    return run_query(cooccurrence_query)\n",
    "\n",
    "# Create entity co-occurrence relationships\n",
    "cooccurrences = find_entity_cooccurrences()\n",
    "print(\"Entity Co-occurrences:\")\n",
    "for co in cooccurrences[:5]:\n",
    "    print(f\"- {co['entity1']} <-> {co['entity2']} (frequency: {co['cooccurrence_count']})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create entity similarity based on embeddings\n",
    "def create_entity_similarities():\n",
    "    \"\"\"Create entity similarity relationships based on context embeddings\"\"\"\n",
    "    \n",
    "    # Get all entities and their contexts\n",
    "    entity_query = \"\"\"\n",
    "    MATCH (d:Document)-[r:CONTAINS_ENTITY]->(e:Entity)\n",
    "    WITH e, collect(substring(d.content, r.start_pos - 50, r.end_pos + 50)) as contexts\n",
    "    RETURN e.text as entity_text, contexts\n",
    "    \"\"\"\n",
    "    \n",
    "    entities_data = run_query(entity_query)\n",
    "    \n",
    "    # Generate embeddings for entity contexts\n",
    "    entity_embeddings = {}\n",
    "    for entity_data in entities_data:\n",
    "        entity_text = entity_data['entity_text']\n",
    "        contexts = entity_data['contexts']\n",
    "        \n",
    "        # Combine contexts and generate embedding\n",
    "        combined_context = ' '.join(contexts)\n",
    "        embedding = generate_embeddings([combined_context])[0]\n",
    "        entity_embeddings[entity_text] = embedding\n",
    "    \n",
    "    # Calculate similarities and create relationships\n",
    "    entities = list(entity_embeddings.keys())\n",
    "    for i, entity1 in enumerate(entities):\n",
    "        for entity2 in entities[i+1:]:\n",
    "            # Calculate cosine similarity\n",
    "            similarity = np.dot(\n",
    "                entity_embeddings[entity1], \n",
    "                entity_embeddings[entity2]\n",
    "            ) / (\n",
    "                np.linalg.norm(entity_embeddings[entity1]) * \n",
    "                np.linalg.norm(entity_embeddings[entity2])\n",
    "            )\n",
    "            \n",
    "            # Create similarity relationship if above threshold\n",
    "            if similarity > 0.7:\n",
    "                similarity_query = \"\"\"\n",
    "                MATCH (e1:Entity {text: $entity1}),\n",
    "                      (e2:Entity {text: $entity2})\n",
    "                MERGE (e1)-[r:SIMILAR_TO]-(e2)\n",
    "                SET r.similarity = $similarity\n",
    "                \"\"\"\n",
    "                \n",
    "                run_query(similarity_query, {\n",
    "                    'entity1': entity1,\n",
    "                    'entity2': entity2,\n",
    "                    'similarity': float(similarity)\n",
    "                })\n",
    "\n",
    "create_entity_similarities()\n",
    "print(\"Created entity similarity relationships\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hands-on Exercise: Complete Knowledge Graph Construction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process all remaining documents\n",
    "for doc in sample_documents[1:]:\n",
    "    # Extract entities\n",
    "    entities = extract_entities(doc['content'])\n",
    "    \n",
    "    # Create document graph\n",
    "    create_document_graph(doc, entities)\n",
    "    \n",
    "    print(f\"Processed document: {doc['title']} ({len(entities)} entities)\")\n",
    "\n",
    "# Recreate co-occurrences with all documents\n",
    "cooccurrences = find_entity_cooccurrences()\n",
    "print(f\"\\nTotal entity co-occurrences: {len(cooccurrences)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Knowledge graph analysis and insights\n",
    "def analyze_knowledge_graph():\n",
    "    \"\"\"Analyze the constructed knowledge graph\"\"\"\n",
    "    \n",
    "    # Document statistics\n",
    "    doc_stats = run_query(\"\"\"\n",
    "    MATCH (d:Document)\n",
    "    OPTIONAL MATCH (d)-[:CONTAINS_ENTITY]->(e:Entity)\n",
    "    OPTIONAL MATCH (d)-[:HAS_CHUNK]->(c:Chunk)\n",
    "    RETURN d.title as document,\n",
    "           count(DISTINCT e) as entity_count,\n",
    "           count(DISTINCT c) as chunk_count\n",
    "    ORDER BY entity_count DESC\n",
    "    \"\"\")\n",
    "    \n",
    "    print(\"Document Analysis:\")\n",
    "    for stat in doc_stats:\n",
    "        print(f\"- {stat['document']}: {stat['entity_count']} entities, {stat['chunk_count']} chunks\")\n",
    "    \n",
    "    # Most connected entities\n",
    "    entity_stats = run_query(\"\"\"\n",
    "    MATCH (e:Entity)\n",
    "    OPTIONAL MATCH (e)-[r:CO_OCCURS_WITH]-(other)\n",
    "    WITH e, count(r) as connection_count\n",
    "    RETURN e.text as entity, e.label as type, connection_count\n",
    "    ORDER BY connection_count DESC\n",
    "    LIMIT 10\n",
    "    \"\"\")\n",
    "    \n",
    "    print(\"\\nMost Connected Entities:\")\n",
    "    for stat in entity_stats:\n",
    "        print(f\"- {stat['entity']} ({stat['type']}): {stat['connection_count']} connections\")\n",
    "    \n",
    "    # Graph structure overview\n",
    "    graph_stats = run_query(\"\"\"\n",
    "    MATCH (n)\n",
    "    RETURN labels(n)[0] as node_type, count(n) as count\n",
    "    ORDER BY count DESC\n",
    "    \"\"\")\n",
    "    \n",
    "    print(\"\\nGraph Structure:\")\n",
    "    for stat in graph_stats:\n",
    "        print(f\"- {stat['node_type']}: {stat['count']} nodes\")\n",
    "\n",
    "analyze_knowledge_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test comprehensive search combining text and graph\n",
    "def hybrid_search(query: str, top_k: int = 5):\n",
    "    \"\"\"Combine semantic search with graph traversal\"\"\"\n",
    "    \n",
    "    # First, find semantically similar chunks\n",
    "    semantic_results = semantic_search(query, top_k)\n",
    "    \n",
    "    # Then, find related entities through graph traversal\n",
    "    graph_query = \"\"\"\n",
    "    CALL db.index.vector.queryNodes('chunk_embeddings', $top_k, $query_embedding)\n",
    "    YIELD node AS chunk, score\n",
    "    MATCH (d:Document)-[:HAS_CHUNK]->(chunk)\n",
    "    MATCH (d)-[:CONTAINS_ENTITY]->(e:Entity)\n",
    "    OPTIONAL MATCH (e)-[:CO_OCCURS_WITH]-(related:Entity)\n",
    "    RETURN DISTINCT d.title as document,\n",
    "           e.text as entity,\n",
    "           e.label as entity_type,\n",
    "           collect(DISTINCT related.text) as related_entities,\n",
    "           max(score) as relevance_score\n",
    "    ORDER BY relevance_score DESC\n",
    "    LIMIT $top_k\n",
    "    \"\"\"\n",
    "    \n",
    "    query_embedding = generate_embeddings([query])[0]\n",
    "    graph_results = run_query(graph_query, {\n",
    "        'query_embedding': query_embedding.tolist(),\n",
    "        'top_k': top_k\n",
    "    })\n",
    "    \n",
    "    return {\n",
    "        'semantic_results': semantic_results,\n",
    "        'graph_results': graph_results\n",
    "    }\n",
    "\n",
    "# Test hybrid search\n",
    "hybrid_results = hybrid_search(\"machine learning and AI partnerships\")\n",
    "\n",
    "print(\"Hybrid Search Results:\")\n",
    "print(\"\\nSemantic Matches:\")\n",
    "for result in hybrid_results['semantic_results'][:3]:\n",
    "    print(f\"- {result['document_title']}: {result['chunk_text'][:80]}... (score: {result['score']:.3f})\")\n",
    "\n",
    "print(\"\\nGraph-Enhanced Results:\")\n",
    "for result in hybrid_results['graph_results'][:3]:\n",
    "    related = ', '.join(result['related_entities'][:3]) if result['related_entities'] else 'None'\n",
    "    print(f\"- Entity: {result['entity']} ({result['entity_type']})\")\n",
    "    print(f\"  Document: {result['document']}\")\n",
    "    print(f\"  Related: {related}\")\n",
    "    print(f\"  Score: {result['relevance_score']:.3f}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Module Summary and Next Steps\n",
    "\n",
    "In this module, you learned to:\n",
    "- Extract entities and relationships from unstructured text\n",
    "- Create knowledge graphs from document collections\n",
    "- Generate and use vector embeddings for semantic analysis\n",
    "- Implement hybrid search combining text similarity and graph traversal\n",
    "- Analyze and visualize knowledge graph structures\n",
    "\n",
    "### Key Takeaways\n",
    "- NLP techniques enable automated knowledge extraction from text\n",
    "- Vector embeddings provide semantic understanding beyond keyword matching\n",
    "- Graph structures reveal relationships and patterns in unstructured data\n",
    "- Hybrid approaches combine the strengths of multiple search methods\n",
    "\n",
    "### Next Module\n",
    "Module 4: Graph Analytics - Learn how to apply graph algorithms for advanced analytics and insights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleanup (optional)\n",
    "# driver.close()\n",
    "print(\"Module 3 completed successfully!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}