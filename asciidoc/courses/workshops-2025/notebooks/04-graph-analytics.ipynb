{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 4: Graph Analytics with Neo4j GDS\n",
    "\n",
    "This notebook introduces Graph Data Science (GDS) algorithms and their applications in AI/ML workflows, focusing on practical analytics for business insights.\n",
    "\n",
    "## Learning Objectives\n",
    "- Apply centrality algorithms to identify important entities\n",
    "- Use community detection to find clusters and groups\n",
    "- Implement similarity algorithms for recommendations\n",
    "- Perform fraud detection and risk assessment analytics\n",
    "- Integrate graph features into ML pipelines\n",
    "\n",
    "## Prerequisites\n",
    "- Completion of Module 3: Unstructured Data\n",
    "- Basic understanding of statistics and ML concepts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install neo4j pandas numpy matplotlib seaborn scikit-learn networkx faker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from neo4j import GraphDatabase\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "import networkx as nx\n",
    "from faker import Faker\n",
    "import random\n",
    "from datetime import datetime, timedelta\n",
    "from typing import List, Dict, Tuple\n",
    "\n",
    "# Set style for plots\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "fake = Faker()\n",
    "random.seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neo4j Connection and GDS Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Neo4j connection settings\n",
    "NEO4J_URI = os.getenv('NEO4J_URI', 'bolt://localhost:7687')\n",
    "NEO4J_USERNAME = os.getenv('NEO4J_USERNAME', 'neo4j')\n",
    "NEO4J_PASSWORD = os.getenv('NEO4J_PASSWORD', 'password')\n",
    "\n",
    "# Create Neo4j driver\n",
    "driver = GraphDatabase.driver(NEO4J_URI, auth=(NEO4J_USERNAME, NEO4J_PASSWORD))\n",
    "\n",
    "def run_query(query, parameters=None):\n",
    "    \"\"\"Execute a Cypher query and return results\"\"\"\n",
    "    with driver.session() as session:\n",
    "        result = session.run(query, parameters or {})\n",
    "        return [record.data() for record in result]\n",
    "\n",
    "# Test connection and GDS availability\n",
    "print(\"Testing Neo4j connection...\")\n",
    "connection_test = run_query(\"RETURN 'Connected to Neo4j!' as message\")\n",
    "print(connection_test[0]['message'])\n",
    "\n",
    "# Check GDS availability\n",
    "try:\n",
    "    gds_test = run_query(\"RETURN gds.version() as version\")\n",
    "    print(f\"GDS Version: {gds_test[0]['version']}\")\n",
    "except Exception as e:\n",
    "    print(f\"GDS not available: {e}\")\n",
    "    print(\"Note: Some examples will use native Cypher instead of GDS\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lesson 1: GDS Overview and Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clear existing data for fresh start\n",
    "run_query(\"MATCH (n) DETACH DELETE n\")\n",
    "\n",
    "# Generate sample financial network data\n",
    "def generate_financial_network(num_customers=100, num_accounts=150, num_transactions=500):\n",
    "    \"\"\"Generate sample financial network for fraud detection analysis\"\"\"\n",
    "    \n",
    "    # Create customers\n",
    "    customers = []\n",
    "    for i in range(num_customers):\n",
    "        customer = {\n",
    "            'id': f'C{i:03d}',\n",
    "            'name': fake.name(),\n",
    "            'email': fake.email(),\n",
    "            'phone': fake.phone_number(),\n",
    "            'address': fake.address().replace('\\n', ', '),\n",
    "            'age': random.randint(18, 80),\n",
    "            'risk_score': random.uniform(0.1, 0.9),\n",
    "            'is_fraudster': random.random() < 0.05  # 5% fraudsters\n",
    "        }\n",
    "        customers.append(customer)\n",
    "    \n",
    "    # Create accounts\n",
    "    accounts = []\n",
    "    for i in range(num_accounts):\n",
    "        account = {\n",
    "            'id': f'A{i:03d}',\n",
    "            'account_type': random.choice(['checking', 'savings', 'credit']),\n",
    "            'balance': random.uniform(100, 50000),\n",
    "            'opened_date': fake.date_between(start_date='-5y', end_date='today'),\n",
    "            'status': random.choice(['active', 'inactive', 'frozen']),\n",
    "            'owner_id': random.choice(customers)['id']\n",
    "        }\n",
    "        accounts.append(account)\n",
    "    \n",
    "    # Create transactions\n",
    "    transactions = []\n",
    "    for i in range(num_transactions):\n",
    "        from_account = random.choice(accounts)\n",
    "        to_account = random.choice(accounts)\n",
    "        \n",
    "        if from_account['id'] != to_account['id']:\n",
    "            transaction = {\n",
    "                'id': f'T{i:04d}',\n",
    "                'amount': random.uniform(10, 10000),\n",
    "                'timestamp': fake.date_time_between(start_date='-1y', end_date='now'),\n",
    "                'transaction_type': random.choice(['transfer', 'payment', 'withdrawal']),\n",
    "                'from_account': from_account['id'],\n",
    "                'to_account': to_account['id'],\n",
    "                'is_suspicious': random.random() < 0.1  # 10% suspicious\n",
    "            }\n",
    "            transactions.append(transaction)\n",
    "    \n",
    "    return customers, accounts, transactions\n",
    "\n",
    "# Generate data\n",
    "customers, accounts, transactions = generate_financial_network()\n",
    "print(f\"Generated {len(customers)} customers, {len(accounts)} accounts, {len(transactions)} transactions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data into Neo4j\n",
    "def load_financial_data(customers, accounts, transactions):\n",
    "    \"\"\"Load financial network data into Neo4j\"\"\"\n",
    "    \n",
    "    # Create customers\n",
    "    for customer in customers:\n",
    "        customer_query = \"\"\"\n",
    "        CREATE (c:Customer {\n",
    "            id: $id,\n",
    "            name: $name,\n",
    "            email: $email,\n",
    "            phone: $phone,\n",
    "            address: $address,\n",
    "            age: $age,\n",
    "            risk_score: $risk_score,\n",
    "            is_fraudster: $is_fraudster\n",
    "        })\n",
    "        \"\"\"\n",
    "        run_query(customer_query, customer)\n",
    "    \n",
    "    # Create accounts\n",
    "    for account in accounts:\n",
    "        account_query = \"\"\"\n",
    "        CREATE (a:Account {\n",
    "            id: $id,\n",
    "            account_type: $account_type,\n",
    "            balance: $balance,\n",
    "            opened_date: date($opened_date),\n",
    "            status: $status\n",
    "        })\n",
    "        \"\"\"\n",
    "        account_data = account.copy()\n",
    "        account_data['opened_date'] = account['opened_date'].strftime('%Y-%m-%d')\n",
    "        del account_data['owner_id']\n",
    "        run_query(account_query, account_data)\n",
    "    \n",
    "    # Create customer-account relationships\n",
    "    for account in accounts:\n",
    "        owner_query = \"\"\"\n",
    "        MATCH (c:Customer {id: $customer_id}),\n",
    "              (a:Account {id: $account_id})\n",
    "        CREATE (c)-[:OWNS]->(a)\n",
    "        \"\"\"\n",
    "        run_query(owner_query, {\n",
    "            'customer_id': account['owner_id'],\n",
    "            'account_id': account['id']\n",
    "        })\n",
    "    \n",
    "    # Create transactions\n",
    "    for transaction in transactions:\n",
    "        transaction_query = \"\"\"\n",
    "        MATCH (from_acc:Account {id: $from_account}),\n",
    "              (to_acc:Account {id: $to_account})\n",
    "        CREATE (from_acc)-[:TRANSFERS {\n",
    "            id: $id,\n",
    "            amount: $amount,\n",
    "            timestamp: datetime($timestamp),\n",
    "            transaction_type: $transaction_type,\n",
    "            is_suspicious: $is_suspicious\n",
    "        }]->(to_acc)\n",
    "        \"\"\"\n",
    "        transaction_data = transaction.copy()\n",
    "        transaction_data['timestamp'] = transaction['timestamp'].isoformat()\n",
    "        del transaction_data['from_account']\n",
    "        del transaction_data['to_account']\n",
    "        run_query(transaction_query, transaction_data)\n",
    "\n",
    "load_financial_data(customers, accounts, transactions)\n",
    "print(\"Financial network data loaded successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify data loading\n",
    "network_stats = run_query(\"\"\"\n",
    "MATCH (c:Customer)\n",
    "OPTIONAL MATCH (c)-[:OWNS]->(a:Account)\n",
    "OPTIONAL MATCH (a)-[t:TRANSFERS]-()\n",
    "RETURN \n",
    "    count(DISTINCT c) as customers,\n",
    "    count(DISTINCT a) as accounts,\n",
    "    count(t) as transactions\n",
    "\"\"\")\n",
    "\n",
    "print(\"Network Statistics:\")\n",
    "for key, value in network_stats[0].items():\n",
    "    print(f\"- {key.title()}: {value:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lesson 2: Centrality Algorithms for Importance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create graph projection for GDS algorithms\n",
    "def create_graph_projection():\n",
    "    \"\"\"Create a graph projection for GDS algorithms\"\"\"\n",
    "    \n",
    "    try:\n",
    "        # Drop existing projection if it exists\n",
    "        run_query(\"CALL gds.graph.drop('financial-network', false)\")\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    # Create projection\n",
    "    projection_query = \"\"\"\n",
    "    CALL gds.graph.project(\n",
    "        'financial-network',\n",
    "        ['Customer', 'Account'],\n",
    "        {\n",
    "            OWNS: { orientation: 'UNDIRECTED' },\n",
    "            TRANSFERS: { \n",
    "                orientation: 'UNDIRECTED',\n",
    "                properties: ['amount']\n",
    "            }\n",
    "        }\n",
    "    )\n",
    "    \"\"\"\n",
    "    \n",
    "    try:\n",
    "        result = run_query(projection_query)\n",
    "        print(f\"Graph projection created: {result[0]['nodeCount']} nodes, {result[0]['relationshipCount']} relationships\")\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"Could not create GDS projection: {e}\")\n",
    "        print(\"Will use native Cypher for centrality calculations\")\n",
    "        return False\n",
    "\n",
    "gds_available = create_graph_projection()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Degree Centrality - Find most connected accounts\n",
    "def calculate_degree_centrality():\n",
    "    \"\"\"Calculate degree centrality for accounts\"\"\"\n",
    "    \n",
    "    if gds_available:\n",
    "        # Using GDS\n",
    "        gds_query = \"\"\"\n",
    "        CALL gds.degree.stream('financial-network')\n",
    "        YIELD nodeId, score\n",
    "        RETURN gds.util.asNode(nodeId).id as node_id,\n",
    "               labels(gds.util.asNode(nodeId))[0] as node_type,\n",
    "               score as degree_centrality\n",
    "        ORDER BY score DESC\n",
    "        LIMIT 10\n",
    "        \"\"\"\n",
    "        return run_query(gds_query)\n",
    "    else:\n",
    "        # Using native Cypher\n",
    "        cypher_query = \"\"\"\n",
    "        MATCH (a:Account)-[r]-()\n",
    "        WITH a, count(r) as degree\n",
    "        RETURN a.id as node_id, \n",
    "               'Account' as node_type,\n",
    "               degree as degree_centrality\n",
    "        ORDER BY degree DESC\n",
    "        LIMIT 10\n",
    "        \"\"\"\n",
    "        return run_query(cypher_query)\n",
    "\n",
    "degree_results = calculate_degree_centrality()\n",
    "print(\"Top 10 Accounts by Degree Centrality:\")\n",
    "for result in degree_results:\n",
    "    print(f\"- {result['node_id']} ({result['node_type']}): {result['degree_centrality']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Betweenness Centrality - Find accounts that act as bridges\n",
    "def calculate_betweenness_centrality():\n",
    "    \"\"\"Calculate betweenness centrality for key intermediaries\"\"\"\n",
    "    \n",
    "    if gds_available:\n",
    "        # Using GDS\n",
    "        gds_query = \"\"\"\n",
    "        CALL gds.betweenness.stream('financial-network')\n",
    "        YIELD nodeId, score\n",
    "        WHERE score > 0\n",
    "        RETURN gds.util.asNode(nodeId).id as node_id,\n",
    "               labels(gds.util.asNode(nodeId))[0] as node_type,\n",
    "               score as betweenness_centrality\n",
    "        ORDER BY score DESC\n",
    "        LIMIT 10\n",
    "        \"\"\"\n",
    "        return run_query(gds_query)\n",
    "    else:\n",
    "        # Simplified version using path-based analysis\n",
    "        cypher_query = \"\"\"\n",
    "        MATCH path = (a1:Account)-[:TRANSFERS*2..3]-(a2:Account)\n",
    "        WHERE a1 <> a2\n",
    "        UNWIND nodes(path)[1..-1] as intermediate\n",
    "        WITH intermediate, count(*) as paths_through\n",
    "        WHERE intermediate:Account\n",
    "        RETURN intermediate.id as node_id,\n",
    "               'Account' as node_type,\n",
    "               paths_through as betweenness_centrality\n",
    "        ORDER BY paths_through DESC\n",
    "        LIMIT 10\n",
    "        \"\"\"\n",
    "        return run_query(cypher_query)\n",
    "\n",
    "betweenness_results = calculate_betweenness_centrality()\n",
    "print(\"\\nTop 10 Accounts by Betweenness Centrality:\")\n",
    "for result in betweenness_results:\n",
    "    print(f\"- {result['node_id']} ({result['node_type']}): {result['betweenness_centrality']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PageRank - Find most influential accounts in the network\n",
    "def calculate_pagerank():\n",
    "    \"\"\"Calculate PageRank for account influence\"\"\"\n",
    "    \n",
    "    if gds_available:\n",
    "        # Using GDS\n",
    "        gds_query = \"\"\"\n",
    "        CALL gds.pageRank.stream('financial-network')\n",
    "        YIELD nodeId, score\n",
    "        RETURN gds.util.asNode(nodeId).id as node_id,\n",
    "               labels(gds.util.asNode(nodeId))[0] as node_type,\n",
    "               score as pagerank_score\n",
    "        ORDER BY score DESC\n",
    "        LIMIT 10\n",
    "        \"\"\"\n",
    "        return run_query(gds_query)\n",
    "    else:\n",
    "        # Simplified version using transaction volume weighting\n",
    "        cypher_query = \"\"\"\n",
    "        MATCH (a:Account)-[t:TRANSFERS]-(other:Account)\n",
    "        WITH a, sum(t.amount) as total_volume, count(t) as transaction_count\n",
    "        WITH a, total_volume * transaction_count as influence_score\n",
    "        RETURN a.id as node_id,\n",
    "               'Account' as node_type,\n",
    "               influence_score as pagerank_score\n",
    "        ORDER BY influence_score DESC\n",
    "        LIMIT 10\n",
    "        \"\"\"\n",
    "        return run_query(cypher_query)\n",
    "\n",
    "pagerank_results = calculate_pagerank()\n",
    "print(\"\\nTop 10 Accounts by PageRank/Influence:\")\n",
    "for result in pagerank_results:\n",
    "    print(f\"- {result['node_id']} ({result['node_type']}): {result['pagerank_score']:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lesson 3: Community Detection for Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Louvain Community Detection\n",
    "def detect_communities():\n",
    "    \"\"\"Detect communities using Louvain algorithm\"\"\"\n",
    "    \n",
    "    if gds_available:\n",
    "        # Using GDS Louvain\n",
    "        gds_query = \"\"\"\n",
    "        CALL gds.louvain.stream('financial-network')\n",
    "        YIELD nodeId, communityId\n",
    "        RETURN gds.util.asNode(nodeId).id as node_id,\n",
    "               labels(gds.util.asNode(nodeId))[0] as node_type,\n",
    "               communityId as community\n",
    "        ORDER BY communityId, node_id\n",
    "        \"\"\"\n",
    "        return run_query(gds_query)\n",
    "    else:\n",
    "        # Simplified community detection using connected components\n",
    "        cypher_query = \"\"\"\n",
    "        MATCH (a:Account)\n",
    "        OPTIONAL MATCH (a)-[:TRANSFERS*1..2]-(connected:Account)\n",
    "        WITH a, collect(DISTINCT connected.id) as connected_accounts\n",
    "        RETURN a.id as node_id,\n",
    "               'Account' as node_type,\n",
    "               size(connected_accounts) as community\n",
    "        ORDER BY community DESC, node_id\n",
    "        \"\"\"\n",
    "        return run_query(cypher_query)\n",
    "\n",
    "community_results = detect_communities()\n",
    "\n",
    "# Analyze community structure\n",
    "community_df = pd.DataFrame(community_results)\n",
    "community_stats = community_df.groupby('community').size().sort_values(ascending=False)\n",
    "\n",
    "print(\"Community Detection Results:\")\n",
    "print(f\"Total communities found: {len(community_stats)}\")\n",
    "print(\"\\nTop 5 communities by size:\")\n",
    "for community, size in community_stats.head().items():\n",
    "    print(f\"- Community {community}: {size} nodes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze community characteristics\n",
    "def analyze_community_characteristics():\n",
    "    \"\"\"Analyze characteristics of detected communities\"\"\"\n",
    "    \n",
    "    # Get community information with account details\n",
    "    if gds_available:\n",
    "        analysis_query = \"\"\"\n",
    "        CALL gds.louvain.stream('financial-network')\n",
    "        YIELD nodeId, communityId\n",
    "        WITH gds.util.asNode(nodeId) as node, communityId\n",
    "        WHERE node:Account\n",
    "        MATCH (c:Customer)-[:OWNS]->(node)\n",
    "        OPTIONAL MATCH (node)-[t:TRANSFERS]-()\n",
    "        WITH communityId, \n",
    "             collect(DISTINCT c.risk_score) as risk_scores,\n",
    "             collect(DISTINCT t.amount) as transaction_amounts,\n",
    "             count(DISTINCT c) as customer_count,\n",
    "             count(DISTINCT node) as account_count,\n",
    "             count(t) as transaction_count\n",
    "        RETURN communityId,\n",
    "               customer_count,\n",
    "               account_count,\n",
    "               transaction_count,\n",
    "               reduce(sum = 0.0, x IN risk_scores | sum + x) / size(risk_scores) as avg_risk_score\n",
    "        ORDER BY customer_count DESC\n",
    "        LIMIT 10\n",
    "        \"\"\"\n",
    "    else:\n",
    "        # Simplified analysis\n",
    "        analysis_query = \"\"\"\n",
    "        MATCH (c:Customer)-[:OWNS]->(a:Account)\n",
    "        OPTIONAL MATCH (a)-[t:TRANSFERS]-()\n",
    "        WITH a, c, count(t) as transaction_count\n",
    "        WITH transaction_count / 10 as community_group, \n",
    "             collect(c.risk_score) as risk_scores,\n",
    "             count(DISTINCT c) as customer_count,\n",
    "             count(DISTINCT a) as account_count,\n",
    "             sum(transaction_count) as transaction_count\n",
    "        RETURN community_group as communityId,\n",
    "               customer_count,\n",
    "               account_count,\n",
    "               transaction_count,\n",
    "               reduce(sum = 0.0, x IN risk_scores | sum + x) / size(risk_scores) as avg_risk_score\n",
    "        ORDER BY customer_count DESC\n",
    "        LIMIT 10\n",
    "        \"\"\"\n",
    "    \n",
    "    return run_query(analysis_query)\n",
    "\n",
    "community_analysis = analyze_community_characteristics()\n",
    "print(\"\\nCommunity Risk Analysis:\")\n",
    "for community in community_analysis:\n",
    "    print(f\"Community {community['communityId']}:\")\n",
    "    print(f\"  - Customers: {community['customer_count']}\")\n",
    "    print(f\"  - Accounts: {community['account_count']}\")\n",
    "    print(f\"  - Transactions: {community['transaction_count']}\")\n",
    "    print(f\"  - Avg Risk Score: {community['avg_risk_score']:.3f}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lesson 4: Advanced Analytics for Fraud Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify suspicious transaction patterns\n",
    "def identify_suspicious_patterns():\n",
    "    \"\"\"Identify potentially fraudulent transaction patterns\"\"\"\n",
    "    \n",
    "    # Pattern 1: Rapid successive transactions\n",
    "    rapid_transactions = run_query(\"\"\"\n",
    "    MATCH (a:Account)-[t1:TRANSFERS]->(b:Account)-[t2:TRANSFERS]->(c:Account)\n",
    "    WHERE duration.between(t1.timestamp, t2.timestamp).minutes < 5\n",
    "      AND t1.amount > 1000 AND t2.amount > 1000\n",
    "    RETURN a.id as account1, b.id as account2, c.id as account3,\n",
    "           t1.amount as amount1, t2.amount as amount2,\n",
    "           t1.timestamp as time1, t2.timestamp as time2\n",
    "    ORDER BY t1.timestamp DESC\n",
    "    LIMIT 10\n",
    "    \"\"\")\n",
    "    \n",
    "    # Pattern 2: Circular money flow\n",
    "    circular_flows = run_query(\"\"\"\n",
    "    MATCH path = (a:Account)-[:TRANSFERS*3..4]->(a)\n",
    "    WHERE length(path) >= 3\n",
    "    WITH path, reduce(total = 0, r in relationships(path) | total + r.amount) as total_amount\n",
    "    WHERE total_amount > 5000\n",
    "    RETURN [n in nodes(path) | n.id] as accounts,\n",
    "           total_amount,\n",
    "           length(path) as path_length\n",
    "    ORDER BY total_amount DESC\n",
    "    LIMIT 5\n",
    "    \"\"\")\n",
    "    \n",
    "    # Pattern 3: High-volume accounts with suspicious activity\n",
    "    suspicious_accounts = run_query(\"\"\"\n",
    "    MATCH (a:Account)-[t:TRANSFERS]-()\n",
    "    WITH a, count(t) as transaction_count, \n",
    "         sum(t.amount) as total_volume,\n",
    "         sum(CASE WHEN t.is_suspicious THEN 1 ELSE 0 END) as suspicious_count\n",
    "    WHERE transaction_count > 5 AND suspicious_count > 0\n",
    "    RETURN a.id as account_id,\n",
    "           transaction_count,\n",
    "           total_volume,\n",
    "           suspicious_count,\n",
    "           toFloat(suspicious_count) / transaction_count as suspicion_rate\n",
    "    ORDER BY suspicion_rate DESC, total_volume DESC\n",
    "    LIMIT 10\n",
    "    \"\"\")\n",
    "    \n",
    "    return {\n",
    "        'rapid_transactions': rapid_transactions,\n",
    "        'circular_flows': circular_flows,\n",
    "        'suspicious_accounts': suspicious_accounts\n",
    "    }\n",
    "\n",
    "fraud_patterns = identify_suspicious_patterns()\n",
    "\n",
    "print(\"Fraud Detection Results:\")\n",
    "print(f\"\\n1. Rapid Successive Transactions ({len(fraud_patterns['rapid_transactions'])} found):\")\n",
    "for pattern in fraud_patterns['rapid_transactions'][:3]:\n",
    "    print(f\"   {pattern['account1']} -> {pattern['account2']} -> {pattern['account3']}\")\n",
    "    print(f\"   Amounts: ${pattern['amount1']:.2f} -> ${pattern['amount2']:.2f}\")\n",
    "\n",
    "print(f\"\\n2. Circular Money Flows ({len(fraud_patterns['circular_flows'])} found):\")\n",
    "for pattern in fraud_patterns['circular_flows'][:3]:\n",
    "    accounts_str = ' -> '.join(pattern['accounts'])\n",
    "    print(f\"   {accounts_str} (${pattern['total_amount']:.2f})\")\n",
    "\n",
    "print(f\"\\n3. High-Risk Accounts ({len(fraud_patterns['suspicious_accounts'])} found):\")\n",
    "for account in fraud_patterns['suspicious_accounts'][:5]:\n",
    "    print(f\"   {account['account_id']}: {account['suspicious_count']}/{account['transaction_count']} suspicious ({account['suspicion_rate']:.2%})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create graph-based features for ML\n",
    "def create_ml_features():\n",
    "    \"\"\"Create graph-based features for machine learning\"\"\"\n",
    "    \n",
    "    # Extract features for each account\n",
    "    features_query = \"\"\"\n",
    "    MATCH (c:Customer)-[:OWNS]->(a:Account)\n",
    "    OPTIONAL MATCH (a)-[t:TRANSFERS]-(other:Account)\n",
    "    OPTIONAL MATCH (a)-[t_out:TRANSFERS]->()\n",
    "    OPTIONAL MATCH (a)<-[t_in:TRANSFERS]-()\n",
    "    \n",
    "    WITH a, c,\n",
    "         count(DISTINCT other) as connected_accounts,\n",
    "         count(t) as total_transactions,\n",
    "         count(t_out) as outgoing_transactions,\n",
    "         count(t_in) as incoming_transactions,\n",
    "         sum(t.amount) as total_volume,\n",
    "         sum(CASE WHEN t.is_suspicious THEN 1 ELSE 0 END) as suspicious_count,\n",
    "         avg(t.amount) as avg_transaction_amount,\n",
    "         max(t.amount) as max_transaction_amount\n",
    "    \n",
    "    RETURN a.id as account_id,\n",
    "           c.age as customer_age,\n",
    "           c.risk_score as customer_risk_score,\n",
    "           c.is_fraudster as is_fraudster,\n",
    "           a.balance as account_balance,\n",
    "           connected_accounts,\n",
    "           total_transactions,\n",
    "           outgoing_transactions,\n",
    "           incoming_transactions,\n",
    "           coalesce(total_volume, 0) as total_volume,\n",
    "           coalesce(suspicious_count, 0) as suspicious_count,\n",
    "           coalesce(avg_transaction_amount, 0) as avg_transaction_amount,\n",
    "           coalesce(max_transaction_amount, 0) as max_transaction_amount\n",
    "    ORDER BY account_id\n",
    "    \"\"\"\n",
    "    \n",
    "    return run_query(features_query)\n",
    "\n",
    "# Create feature dataset\n",
    "ml_features = create_ml_features()\n",
    "features_df = pd.DataFrame(ml_features)\n",
    "\n",
    "print(\"ML Feature Dataset Created:\")\n",
    "print(f\"Shape: {features_df.shape}\")\n",
    "print(\"\\nFeature columns:\")\n",
    "for col in features_df.columns:\n",
    "    print(f\"- {col}\")\n",
    "\n",
    "print(\"\\nSample features:\")\n",
    "print(features_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train fraud detection model using graph features\n",
    "def train_fraud_model(features_df):\n",
    "    \"\"\"Train a fraud detection model using graph-based features\"\"\"\n",
    "    \n",
    "    # Prepare features and target\n",
    "    feature_columns = [\n",
    "        'customer_age', 'customer_risk_score', 'account_balance',\n",
    "        'connected_accounts', 'total_transactions', 'outgoing_transactions',\n",
    "        'incoming_transactions', 'total_volume', 'suspicious_count',\n",
    "        'avg_transaction_amount', 'max_transaction_amount'\n",
    "    ]\n",
    "    \n",
    "    X = features_df[feature_columns].fillna(0)\n",
    "    y = features_df['is_fraudster']\n",
    "    \n",
    "    # Scale features\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "    \n",
    "    # Train model\n",
    "    model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "    model.fit(X_scaled, y)\n",
    "    \n",
    "    # Predictions\n",
    "    y_pred = model.predict(X_scaled)\n",
    "    \n",
    "    # Feature importance\n",
    "    feature_importance = pd.DataFrame({\n",
    "        'feature': feature_columns,\n",
    "        'importance': model.feature_importances_\n",
    "    }).sort_values('importance', ascending=False)\n",
    "    \n",
    "    return model, scaler, feature_importance, y_pred\n",
    "\n",
    "# Train the model\n",
    "model, scaler, feature_importance, predictions = train_fraud_model(features_df)\n",
    "\n",
    "print(\"Fraud Detection Model Training Results:\")\n",
    "print(\"\\nTop 5 Most Important Features:\")\n",
    "for _, row in feature_importance.head().iterrows():\n",
    "    print(f\"- {row['feature']}: {row['importance']:.4f}\")\n",
    "\n",
    "# Model performance\n",
    "print(\"\\nModel Performance:\")\n",
    "print(classification_report(features_df['is_fraudster'], predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hands-on Exercise: Complete Financial Network Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comprehensive network analysis dashboard\n",
    "def create_network_dashboard():\n",
    "    \"\"\"Create a comprehensive analysis dashboard\"\"\"\n",
    "    \n",
    "    # Overall network statistics\n",
    "    network_overview = run_query(\"\"\"\n",
    "    MATCH (c:Customer)-[:OWNS]->(a:Account)\n",
    "    OPTIONAL MATCH (a)-[t:TRANSFERS]-()\n",
    "    RETURN \n",
    "        count(DISTINCT c) as total_customers,\n",
    "        count(DISTINCT a) as total_accounts,\n",
    "        count(t) as total_transactions,\n",
    "        sum(t.amount) as total_volume,\n",
    "        sum(CASE WHEN c.is_fraudster THEN 1 ELSE 0 END) as fraudulent_customers,\n",
    "        sum(CASE WHEN t.is_suspicious THEN 1 ELSE 0 END) as suspicious_transactions\n",
    "    \"\"\")\n",
    "    \n",
    "    # Risk distribution\n",
    "    risk_distribution = run_query(\"\"\"\n",
    "    MATCH (c:Customer)\n",
    "    WITH \n",
    "        CASE \n",
    "            WHEN c.risk_score < 0.3 THEN 'Low'\n",
    "            WHEN c.risk_score < 0.7 THEN 'Medium'\n",
    "            ELSE 'High'\n",
    "        END as risk_category,\n",
    "        c.is_fraudster as is_fraudster\n",
    "    RETURN risk_category, \n",
    "           count(*) as total_customers,\n",
    "           sum(CASE WHEN is_fraudster THEN 1 ELSE 0 END) as fraudulent_customers\n",
    "    ORDER BY risk_category\n",
    "    \"\"\")\n",
    "    \n",
    "    # Transaction patterns by time\n",
    "    temporal_patterns = run_query(\"\"\"\n",
    "    MATCH ()-[t:TRANSFERS]-()\n",
    "    WITH t.timestamp.hour as hour, \n",
    "         t.amount as amount,\n",
    "         t.is_suspicious as is_suspicious\n",
    "    RETURN hour,\n",
    "           count(*) as transaction_count,\n",
    "           avg(amount) as avg_amount,\n",
    "           sum(CASE WHEN is_suspicious THEN 1 ELSE 0 END) as suspicious_count\n",
    "    ORDER BY hour\n",
    "    \"\"\")\n",
    "    \n",
    "    return {\n",
    "        'overview': network_overview[0],\n",
    "        'risk_distribution': risk_distribution,\n",
    "        'temporal_patterns': temporal_patterns\n",
    "    }\n",
    "\n",
    "dashboard_data = create_network_dashboard()\n",
    "\n",
    "print(\"=== FINANCIAL NETWORK ANALYSIS DASHBOARD ===\")\n",
    "print(\"\\n📊 Network Overview:\")\n",
    "overview = dashboard_data['overview']\n",
    "print(f\"• Total Customers: {overview['total_customers']:,}\")\n",
    "print(f\"• Total Accounts: {overview['total_accounts']:,}\")\n",
    "print(f\"• Total Transactions: {overview['total_transactions']:,}\")\n",
    "print(f\"• Total Volume: ${overview['total_volume']:,.2f}\")\n",
    "print(f\"• Fraudulent Customers: {overview['fraudulent_customers']} ({overview['fraudulent_customers']/overview['total_customers']:.1%})\")\n",
    "print(f\"• Suspicious Transactions: {overview['suspicious_transactions']} ({overview['suspicious_transactions']/overview['total_transactions']:.1%})\")\n",
    "\n",
    "print(\"\\n⚠️ Risk Distribution:\")\n",
    "for risk in dashboard_data['risk_distribution']:\n",
    "    fraud_rate = risk['fraudulent_customers'] / risk['total_customers'] if risk['total_customers'] > 0 else 0\n",
    "    print(f\"• {risk['risk_category']} Risk: {risk['total_customers']} customers, {risk['fraudulent_customers']} fraudulent ({fraud_rate:.1%})\")\n",
    "\n",
    "print(\"\\n⏰ Peak Activity Hours:\")\n",
    "sorted_hours = sorted(dashboard_data['temporal_patterns'], key=lambda x: x['transaction_count'], reverse=True)\n",
    "for hour_data in sorted_hours[:5]:\n",
    "    hour = hour_data['hour']\n",
    "    count = hour_data['transaction_count']\n",
    "    suspicious = hour_data['suspicious_count']\n",
    "    suspicious_rate = suspicious / count if count > 0 else 0\n",
    "    print(f\"• Hour {hour:02d}:00: {count} transactions, {suspicious} suspicious ({suspicious_rate:.1%})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize network insights\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# 1. Risk distribution\n",
    "risk_data = dashboard_data['risk_distribution']\n",
    "risk_categories = [r['risk_category'] for r in risk_data]\n",
    "risk_counts = [r['total_customers'] for r in risk_data]\n",
    "fraud_counts = [r['fraudulent_customers'] for r in risk_data]\n",
    "\n",
    "x = np.arange(len(risk_categories))\n",
    "width = 0.35\n",
    "\n",
    "axes[0, 0].bar(x - width/2, risk_counts, width, label='Total Customers', alpha=0.7)\n",
    "axes[0, 0].bar(x + width/2, fraud_counts, width, label='Fraudulent Customers', alpha=0.7)\n",
    "axes[0, 0].set_xlabel('Risk Category')\n",
    "axes[0, 0].set_ylabel('Number of Customers')\n",
    "axes[0, 0].set_title('Customer Risk Distribution')\n",
    "axes[0, 0].set_xticks(x)\n",
    "axes[0, 0].set_xticklabels(risk_categories)\n",
    "axes[0, 0].legend()\n",
    "\n",
    "# 2. Feature importance\n",
    "top_features = feature_importance.head(8)\n",
    "axes[0, 1].barh(range(len(top_features)), top_features['importance'])\n",
    "axes[0, 1].set_yticks(range(len(top_features)))\n",
    "axes[0, 1].set_yticklabels(top_features['feature'])\n",
    "axes[0, 1].set_xlabel('Feature Importance')\n",
    "axes[0, 1].set_title('Top Features for Fraud Detection')\n",
    "\n",
    "# 3. Temporal patterns\n",
    "temporal_data = dashboard_data['temporal_patterns']\n",
    "hours = [t['hour'] for t in temporal_data]\n",
    "tx_counts = [t['transaction_count'] for t in temporal_data]\n",
    "susp_counts = [t['suspicious_count'] for t in temporal_data]\n",
    "\n",
    "axes[1, 0].plot(hours, tx_counts, 'b-', label='Total Transactions', marker='o')\n",
    "axes[1, 0].plot(hours, susp_counts, 'r-', label='Suspicious Transactions', marker='s')\n",
    "axes[1, 0].set_xlabel('Hour of Day')\n",
    "axes[1, 0].set_ylabel('Transaction Count')\n",
    "axes[1, 0].set_title('Transaction Patterns by Time')\n",
    "axes[1, 0].legend()\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# 4. Community size distribution\n",
    "if 'community_df' in locals():\n",
    "    community_sizes = community_df.groupby('community').size().values\n",
    "    axes[1, 1].hist(community_sizes, bins=20, alpha=0.7, edgecolor='black')\n",
    "    axes[1, 1].set_xlabel('Community Size')\n",
    "    axes[1, 1].set_ylabel('Number of Communities')\n",
    "    axes[1, 1].set_title('Community Size Distribution')\n",
    "    axes[1, 1].grid(True, alpha=0.3)\n",
    "else:\n",
    "    axes[1, 1].text(0.5, 0.5, 'Community data\\nnot available', \n",
    "                   ha='center', va='center', transform=axes[1, 1].transAxes)\n",
    "    axes[1, 1].set_title('Community Analysis')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n📈 Visualization Complete - Network analysis insights displayed above\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Module Summary and Next Steps\n",
    "\n",
    "In this module, you learned to:\n",
    "- Apply centrality algorithms to identify key nodes in financial networks\n",
    "- Use community detection to segment customers and accounts\n",
    "- Implement fraud detection using graph-based pattern analysis\n",
    "- Create machine learning features from graph structures\n",
    "- Build comprehensive network analysis dashboards\n",
    "\n",
    "### Key Takeaways\n",
    "- Graph algorithms reveal hidden patterns in connected data\n",
    "- Centrality measures identify influential nodes and potential risks\n",
    "- Community detection helps segment networks for targeted analysis\n",
    "- Graph features significantly improve ML model performance\n",
    "- Combining multiple algorithms provides comprehensive insights\n",
    "\n",
    "### Business Applications\n",
    "- **Fraud Detection**: Identify suspicious transaction patterns and account networks\n",
    "- **Risk Assessment**: Score customers based on their network position and behavior\n",
    "- **Customer Segmentation**: Group similar customers for targeted marketing\n",
    "- **Compliance**: Monitor for regulatory violations and suspicious activities\n",
    "\n",
    "### Next Module\n",
    "Module 5: Retrievers - Learn how to implement retrieval patterns for RAG applications using graph databases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleanup (optional)\n",
    "# driver.close()\n",
    "print(\"\\n🎉 Module 4: Graph Analytics completed successfully!\")\n",
    "print(\"You're now ready to move on to Module 5: Retrievers\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}