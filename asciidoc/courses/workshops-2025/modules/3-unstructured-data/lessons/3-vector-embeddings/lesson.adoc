= Semantic Search, Vectors, and Embeddings
:order: 3
:type: lesson
:slides: true

[.slide.col-2]
== Semantic Search

[.col]
====
Machine learning and natural language processing (NLP) often use vectors and embeddings to represent and understand data.

Semantic search aims to understand search phrases' intent and contextual meaning, rather than focusing on individual keywords.

Traditional keyword search often depends on exact-match keywords or proximity-based algorithms that find similar words.

For example, if you input "earnings report" in a traditional search, you might predominantly get results containing those exact words.

However, in a semantic search, the engine tries to gauge the context: Are you searching about quarterly results, annual filings, or financial performance?
====

[.col]
====
**Financial Search Examples:**

Traditional: "quarterly earnings" → exact matches only

Semantic: "quarterly earnings" → financial performance, revenue reports, profit statements, Q3 results
====

[.slide.col-2]
== What are Vectors

[.col]
====
Vectors are simply a list of numbers. 
For example, the vector `[1, 2, 3]` is a list of three numbers and could represent a point in three-dimensional space.

You can use vectors to represent many different types of data, including text, images, and audio.

Using vectors with a dimensionality of hundreds and thousands in machine learning and natural language processing (NLP) is common.
====

[.col]
====
**Financial Vector Example:**
```
"revenue growth" → [0.23, -0.15, 0.87, ...]
"profit margin" → [0.19, -0.12, 0.91, ...]
"market volatility" → [-0.45, 0.73, 0.05, ...]
```
====

[.slide]
== What are Embeddings?

When referring to vectors in the context of machine learning and NLP, the term "embedding" is typically used. 
An embedding is a vector that represents the data in a useful way for a specific task.

Each dimension in a vector can represent a particular semantic aspect of the word or phrase. 
When multiple dimensions are combined, they can convey the overall meaning of the word or phrase.

For example, the word "earnings" might be represented by an embedding with the following dimensions:

* financial performance
* quarterly reporting  
* corporate results
* investor relations
* profit metrics

You can create embeddings in various ways, but one of the most common methods is to use a **large language model**.

[.slide.col-2]
== How are vectors used in semantic search?

[.col]
====
You can use the _distance_ or _angle_ between vectors to gauge the semantic similarity between words or phrases.

Words with similar meanings or contexts will have vectors that are close together, while unrelated words will be farther apart.

This principle is employed in semantic search to find contextually relevant results for a user's query.
====

[.col]
====
**Financial Similarity Examples:**

Close vectors:
- "earnings" ↔ "revenue" 
- "profit" ↔ "income"
- "volatile" ↔ "unstable"

Distant vectors:  
- "earnings" ↔ "weather"
- "portfolio" ↔ "recipe"
====

[.slide]
== Neo4j Vector Indexes

Neo4j provides built-in support for vector similarity search through vector indexes.

You can create vector indexes to store and query embeddings directly in the graph database:

[source, cypher]
----
// Create a vector index for document embeddings
CREATE VECTOR INDEX document_embeddings IF NOT EXISTS
FOR (d:Document) ON (d.embedding)
OPTIONS {
  indexConfig: {
    `vector.dimensions`: 384,
    `vector.similarity_function`: 'cosine'
  }
}
----

[.slide]
== Storing Financial Document Embeddings

Once you have a vector index, you can store documents with their embeddings:

[source, cypher]
----
// Store financial document with embedding
MERGE (d:Document {id: "earnings_q3_2024"})
SET d.title = "Q3 2024 Earnings Report",
    d.content = "Revenue increased 15% year-over-year...",
    d.embedding = $embedding_vector,
    d.sector = "Technology",
    d.report_type = "quarterly"
----

[.slide]
== Vector Similarity Search

Use the vector index to find similar documents:

[source, cypher]
----
// Find documents similar to a query embedding
CALL db.index.vector.queryNodes('document_embeddings', $top_k, $query_embedding)
YIELD node, score
RETURN node.title, node.content, score
ORDER BY score DESC
----

This finds documents that are semantically similar to your query, even if they don't contain exact keyword matches.

== Questions

Answer the following questions about vectors and embeddings:

1. **Question**: What is the main advantage of vector embeddings over traditional keyword search?
   
   **Answer**: Vector embeddings capture semantic meaning, allowing systems to understand that "earnings" and "revenue" are related concepts, enabling search by meaning rather than exact keyword matches.

2. **Question**: What is cosine similarity and why is it used for comparing embeddings?
   
   **Answer**: Cosine similarity measures the angle between two vectors, focusing on direction rather than magnitude. It's ideal for embeddings because it captures semantic similarity regardless of text length.

3. **Question**: How do you create a vector index in Neo4j?
   
   **Answer**: Use `CREATE VECTOR INDEX` with the node label, property, and configuration options like dimensions and similarity function.

4. **Question**: What's the difference between exact search and semantic search in financial documents?
   
   **Answer**: Exact search finds documents containing specific keywords, while semantic search finds documents with similar meaning - for example, "profit" might match documents about "earnings" or "revenue".

[.next]
== Continue

When you are ready, you can move on to the next task.

read::Move on[]

[.summary]
== Summary

You learned about semantic search, vectors, and embeddings for financial document analysis.

Next you will use these techniques to build vector indexes and enable semantic search in Neo4j.