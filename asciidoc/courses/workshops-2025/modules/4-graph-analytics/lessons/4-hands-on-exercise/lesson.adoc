= Hands-on Exercise: Complete Graph Analytics Pipeline
:type: lesson
:order: 4
:duration: 35 minutes

== Learning Objectives

By the end of this lesson, you will be able to:

* Build a comprehensive graph analytics pipeline from data to insights
* Apply multiple graph algorithms to solve real-world business problems
* Create visualizations and reports for stakeholder communication
* Optimize graph projections and algorithm parameters for performance

== Exercise Overview

In this exercise, you'll analyze a corporate network dataset to uncover organizational insights using graph analytics. You'll work with employee collaboration data to identify:

1. **Key Influencers** - Most important people in the organization
2. **Department Communities** - Natural groupings and cross-functional teams
3. **Communication Bridges** - People who connect different parts of the organization
4. **Network Vulnerabilities** - Critical nodes whose removal would fragment the network

== Dataset: Corporate Collaboration Network

=== Loading the Sample Data

```python
import pandas as pd
import numpy as np
from datetime import datetime, timedelta
import random
from neo4j import GraphDatabase

class CorporateNetworkGenerator:
    def __init__(self):
        self.departments = ['Engineering', 'Sales', 'Marketing', 'HR', 'Finance', 'Operations', 'Legal']
        self.positions = ['Junior', 'Mid-level', 'Senior', 'Lead', 'Manager', 'Director', 'VP']
        self.collaboration_types = ['email', 'meeting', 'project', 'mentoring', 'reporting']
        
    def generate_employees(self, num_employees=150):
        """Generate synthetic employee data"""
        employees = []
        
        for i in range(num_employees):
            employee = {
                'employee_id': f'EMP_{i:04d}',
                'name': f'Employee_{i:04d}',
                'department': random.choice(self.departments),
                'position': random.choice(self.positions),
                'seniority_level': random.randint(1, 10),
                'years_at_company': random.randint(1, 15),
                'location': random.choice(['HQ', 'Remote', 'Branch_A', 'Branch_B'])
            }
            employees.append(employee)
        
        return pd.DataFrame(employees)
    
    def generate_collaborations(self, employees_df, num_collaborations=800):
        """Generate synthetic collaboration relationships"""
        collaborations = []
        
        for _ in range(num_collaborations):
            # Select two different employees
            emp1, emp2 = random.sample(range(len(employees_df)), 2)
            
            # Higher probability of collaboration within same department
            same_dept = employees_df.iloc[emp1]['department'] == employees_df.iloc[emp2]['department']
            
            # Calculate collaboration strength based on various factors
            base_strength = random.uniform(0.1, 1.0)
            if same_dept:
                base_strength *= 1.5
            
            # Adjust for seniority levels
            seniority_diff = abs(employees_df.iloc[emp1]['seniority_level'] - 
                               employees_df.iloc[emp2]['seniority_level'])
            if seniority_diff <= 2:
                base_strength *= 1.2
            
            collaboration = {
                'from_employee': employees_df.iloc[emp1]['employee_id'],
                'to_employee': employees_df.iloc[emp2]['employee_id'],
                'collaboration_type': random.choice(self.collaboration_types),
                'frequency_per_month': random.randint(1, 20),
                'strength': min(base_strength, 1.0),
                'duration_months': random.randint(1, 24),
                'start_date': datetime.now() - timedelta(days=random.randint(30, 730))
            }
            collaborations.append(collaboration)
        
        return pd.DataFrame(collaborations)
    
    def create_sample_dataset(self):
        """Create the complete sample dataset"""
        employees = self.generate_employees(150)
        collaborations = self.generate_collaborations(employees, 800)
        
        return employees, collaborations

# Generate sample data
generator = CorporateNetworkGenerator()
employees_df, collaborations_df = generator.create_sample_dataset()

print(f"Generated {len(employees_df)} employees and {len(collaborations_df)} collaborations")
print("\nEmployee departments distribution:")
print(employees_df['department'].value_counts())

print("\nCollaboration types distribution:")
print(collaborations_df['collaboration_type'].value_counts())
```

=== Loading Data into Neo4j

```python
class CorporateNetworkLoader:
    def __init__(self, uri, username, password):
        self.driver = GraphDatabase.driver(uri, auth=(username, password))
    
    def clear_database(self):
        """Clear existing data (use with caution!)"""
        with self.driver.session() as session:
            session.run("MATCH (n) DETACH DELETE n")
            print("Database cleared")
    
    def create_constraints_and_indexes(self):
        """Create necessary constraints and indexes"""
        constraints_and_indexes = [
            "CREATE CONSTRAINT employee_id IF NOT EXISTS FOR (e:Employee) REQUIRE e.employee_id IS UNIQUE",
            "CREATE INDEX employee_department IF NOT EXISTS FOR (e:Employee) ON (e.department)",
            "CREATE INDEX employee_position IF NOT EXISTS FOR (e:Employee) ON (e.position)",
            "CREATE INDEX collaboration_type IF NOT EXISTS FOR ()-[c:COLLABORATES_WITH]->() ON (c.type)",
        ]
        
        with self.driver.session() as session:
            for query in constraints_and_indexes:
                try:
                    session.run(query)
                    print(f"✅ Created: {query.split()[1]} {query.split()[2]}")
                except Exception as e:
                    if "already exists" in str(e):
                        print(f"⚠️ Already exists: {query.split()[1]} {query.split()[2]}")
                    else:
                        print(f"❌ Error: {e}")
    
    def load_employees(self, employees_df):
        """Load employee nodes into Neo4j"""
        with self.driver.session() as session:
            for _, employee in employees_df.iterrows():
                query = """
                CREATE (e:Employee {
                    employee_id: $employee_id,
                    name: $name,
                    department: $department,
                    position: $position,
                    seniority_level: $seniority_level,
                    years_at_company: $years_at_company,
                    location: $location
                })
                """
                session.run(query, employee.to_dict())
        
        print(f"Loaded {len(employees_df)} employees")
    
    def load_collaborations(self, collaborations_df):
        """Load collaboration relationships into Neo4j"""
        with self.driver.session() as session:
            for _, collab in collaborations_df.iterrows():
                query = """
                MATCH (e1:Employee {employee_id: $from_employee})
                MATCH (e2:Employee {employee_id: $to_employee})
                CREATE (e1)-[c:COLLABORATES_WITH {
                    type: $collaboration_type,
                    frequency_per_month: $frequency_per_month,
                    strength: $strength,
                    duration_months: $duration_months,
                    start_date: date($start_date)
                }]->(e2)
                """
                session.run(query, {
                    'from_employee': collab['from_employee'],
                    'to_employee': collab['to_employee'],
                    'collaboration_type': collab['collaboration_type'],
                    'frequency_per_month': collab['frequency_per_month'],
                    'strength': collab['strength'],
                    'duration_months': collab['duration_months'],
                    'start_date': collab['start_date'].strftime('%Y-%m-%d')
                })
        
        print(f"Loaded {len(collaborations_df)} collaborations")
    
    def load_complete_dataset(self, employees_df, collaborations_df):
        """Load the complete dataset"""
        self.create_constraints_and_indexes()
        self.load_employees(employees_df)
        self.load_collaborations(collaborations_df)
        print("✅ Complete dataset loaded successfully")

# Load data into Neo4j
# loader = CorporateNetworkLoader("bolt://localhost:7687", "neo4j", "password")
# loader.load_complete_dataset(employees_df, collaborations_df)
```

== Graph Analytics Pipeline

=== Step 1: Create Graph Projections

```cypher
// Create main collaboration network projection
CALL gds.graph.project(
    'corporate-network',
    'Employee',
    {
        COLLABORATES_WITH: {
            type: 'COLLABORATES_WITH',
            orientation: 'UNDIRECTED',
            properties: ['strength', 'frequency_per_month']
        }
    }
)
YIELD graphName, nodeCount, relationshipCount, projectMillis

// Create department-specific projection
CALL gds.graph.project(
    'department-network',
    {
        Employee: {
            properties: ['department', 'seniority_level', 'years_at_company']
        }
    },
    {
        COLLABORATES_WITH: {
            type: 'COLLABORATES_WITH',
            orientation: 'UNDIRECTED',
            properties: ['strength']
        }
    }
)
```

=== Step 2: Comprehensive Analytics Implementation

```python
class CorporateAnalytics:
    def __init__(self, uri, username, password):
        self.driver = GraphDatabase.driver(uri, auth=(username, password))
    
    def run_centrality_analysis(self, graph_name='corporate-network'):
        """Run comprehensive centrality analysis"""
        
        centrality_queries = {
            'pagerank': f"""
            CALL gds.pageRank.stream('{graph_name}', {{
                relationshipWeightProperty: 'strength'
            }})
            YIELD nodeId, score
            RETURN gds.util.asNode(nodeId).name AS employee,
                   gds.util.asNode(nodeId).department AS department,
                   gds.util.asNode(nodeId).position AS position,
                   score
            ORDER BY score DESC
            LIMIT 20
            """,
            
            'betweenness': f"""
            CALL gds.betweenness.stream('{graph_name}')
            YIELD nodeId, score
            RETURN gds.util.asNode(nodeId).name AS employee,
                   gds.util.asNode(nodeId).department AS department,
                   gds.util.asNode(nodeId).position AS position,
                   score
            ORDER BY score DESC
            LIMIT 20
            """,
            
            'closeness': f"""
            CALL gds.closeness.stream('{graph_name}')
            YIELD nodeId, score
            RETURN gds.util.asNode(nodeId).name AS employee,
                   gds.util.asNode(nodeId).department AS department,
                   gds.util.asNode(nodeId).position AS position,
                   score
            ORDER BY score DESC
            LIMIT 20
            """
        }
        
        results = {}
        with self.driver.session() as session:
            for centrality, query in centrality_queries.items():
                result = session.run(query)
                df = pd.DataFrame([dict(record) for record in result])
                results[centrality] = df
        
        return results
    
    def run_community_detection(self, graph_name='corporate-network'):
        """Run community detection analysis"""
        
        community_query = f"""
        CALL gds.louvain.stream('{graph_name}', {{
            relationshipWeightProperty: 'strength',
            includeIntermediateCommunities: true
        }})
        YIELD nodeId, communityId, intermediateCommunityIds
        WITH gds.util.asNode(nodeId) AS employee, communityId, intermediateCommunityIds
        RETURN employee.name AS employee,
               employee.department AS department,
               employee.position AS position,
               employee.seniority_level AS seniority,
               communityId,
               intermediateCommunityIds
        ORDER BY communityId, employee.name
        """
        
        with self.driver.session() as session:
            result = session.run(community_query)
            df = pd.DataFrame([dict(record) for record in result])
        
        return df
    
    def analyze_cross_department_collaboration(self, graph_name='corporate-network'):
        """Analyze collaboration patterns between departments"""
        
        cross_dept_query = f"""
        CALL gds.louvain.stream('{graph_name}')
        YIELD nodeId, communityId
        WITH communityId, collect(gds.util.asNode(nodeId)) AS members
        UNWIND members AS member
        WITH communityId, 
             member.department AS department,
             collect(member.name) AS dept_members,
             count(*) AS dept_count
        WITH communityId,
             collect({{department: department, members: dept_members, count: dept_count}}) AS dept_breakdown,
             sum(dept_count) AS total_members
        WHERE size(dept_breakdown) > 1  // Only cross-departmental communities
        RETURN communityId,
               total_members,
               dept_breakdown
        ORDER BY total_members DESC
        """
        
        with self.driver.session() as session:
            result = session.run(query)
            return [dict(record) for record in result]
    
    def identify_key_connectors(self, graph_name='corporate-network'):
        """Identify employees who connect different departments"""
        
        connector_query = f"""
        CALL gds.betweenness.stream('{graph_name}')
        YIELD nodeId, score
        WITH gds.util.asNode(nodeId) AS employee, score
        WHERE score > 0
        MATCH (employee)-[c:COLLABORATES_WITH]-(colleague:Employee)
        WHERE employee.department <> colleague.department
        WITH employee, score, 
             collect(DISTINCT colleague.department) AS connected_departments,
             count(DISTINCT colleague.department) AS dept_diversity,
             avg(c.strength) AS avg_collaboration_strength
        WHERE dept_diversity >= 2
        RETURN employee.name AS employee,
               employee.department AS employee_dept,
               employee.position AS position,
               score AS betweenness_score,
               connected_departments,
               dept_diversity,
               avg_collaboration_strength
        ORDER BY betweenness_score DESC, dept_diversity DESC
        LIMIT 15
        """
        
        with self.driver.session() as session:
            result = session.run(connector_query)
            return pd.DataFrame([dict(record) for record in result])
    
    def network_resilience_analysis(self, graph_name='corporate-network'):
        """Analyze network resilience by simulating key node removal"""
        
        # Get top nodes by different centrality measures
        centrality_results = self.run_centrality_analysis(graph_name)
        
        resilience_scenarios = {}
        
        for centrality_type, df in centrality_results.items():
            top_nodes = df.head(5)['employee'].tolist()
            
            # Simulate removal of these nodes
            scenario_query = f"""
            MATCH (n:Employee)
            WHERE NOT n.name IN {top_nodes}
            WITH collect(id(n)) AS remainingNodeIds
            
            CALL gds.wcc.stats('{graph_name}', {{
                nodeLabels: ['Employee'],
                nodeIds: remainingNodeIds
            }})
            YIELD componentCount, componentDistribution
            RETURN componentCount, componentDistribution
            """
            
            with self.driver.session() as session:
                result = session.run(scenario_query)
                record = result.single()
                if record:
                    resilience_scenarios[centrality_type] = dict(record)
        
        return resilience_scenarios
    
    def generate_executive_summary(self, graph_name='corporate-network'):
        """Generate comprehensive executive summary"""
        
        # Get basic network statistics
        stats_query = f"""
        CALL gds.graph.nodeCount('{graph_name}') YIELD nodeCount
        CALL gds.graph.relationshipCount('{graph_name}') YIELD relationshipCount
        
        // Department distribution
        MATCH (e:Employee)
        RETURN e.department AS department, count(*) AS employee_count
        ORDER BY employee_count DESC
        """
        
        with self.driver.session() as session:
            # Basic stats
            node_count = session.run(f"CALL gds.graph.nodeCount('{graph_name}')").single()['nodeCount']
            rel_count = session.run(f"CALL gds.graph.relationshipCount('{graph_name}')").single()['relationshipCount']
            
            # Department distribution
            dept_result = session.run("""
                MATCH (e:Employee)
                RETURN e.department AS department, count(*) AS employee_count
                ORDER BY employee_count DESC
            """)
            dept_distribution = [dict(record) for record in dept_result]
        
        # Get analysis results
        centrality_results = self.run_centrality_analysis(graph_name)
        community_df = self.run_community_detection(graph_name)
        key_connectors = self.identify_key_connectors(graph_name)
        
        # Create summary
        summary = {
            'network_overview': {
                'total_employees': node_count,
                'total_collaborations': rel_count,
                'departments': len(dept_distribution),
                'avg_collaborations_per_employee': rel_count / node_count if node_count > 0 else 0
            },
            'department_distribution': dept_distribution,
            'key_insights': {
                'top_influencer': centrality_results['pagerank'].iloc[0] if not centrality_results['pagerank'].empty else None,
                'key_bridge_person': centrality_results['betweenness'].iloc[0] if not centrality_results['betweenness'].empty else None,
                'most_accessible': centrality_results['closeness'].iloc[0] if not centrality_results['closeness'].empty else None,
                'communities_found': community_df['communityId'].nunique() if not community_df.empty else 0,
                'cross_dept_connectors': len(key_connectors)
            },
            'recommendations': [
                'Leverage top influencers for organizational change initiatives',
                'Protect bridge employees to maintain organizational connectivity',
                'Foster cross-departmental communities for innovation',
                'Develop succession plans for critical connectors'
            ]
        }
        
        return summary

# Run comprehensive analysis
analytics = CorporateAnalytics("bolt://localhost:7687", "neo4j", "password")

print("Running comprehensive corporate network analysis...")
print("=" * 60)

# 1. Centrality Analysis
print("\n1. CENTRALITY ANALYSIS")
centrality_results = analytics.run_centrality_analysis()

for centrality_type, df in centrality_results.items():
    print(f"\nTop 5 employees by {centrality_type.title()}:")
    for _, row in df.head(5).iterrows():
        print(f"  {row['employee']} ({row['department']}, {row['position']}) - Score: {row['score']:.4f}")

# 2. Community Detection
print("\n2. COMMUNITY ANALYSIS")
community_df = analytics.run_community_detection()
communities_summary = community_df.groupby('communityId').agg({
    'employee': 'count',
    'department': lambda x: list(set(x))
}).rename(columns={'employee': 'size'})

print(f"Found {len(communities_summary)} communities:")
for community_id, data in communities_summary.head(10).iterrows():
    departments = ', '.join(data['department'])
    print(f"  Community {community_id}: {data['size']} members from {departments}")

# 3. Key Connectors
print("\n3. KEY CONNECTORS (Bridge People)")
key_connectors = analytics.identify_key_connectors()
print("Top cross-departmental connectors:")
for _, connector in key_connectors.head(5).iterrows():
    connected_depts = ', '.join(connector['connected_departments'])
    print(f"  {connector['employee']} ({connector['employee_dept']}) connects to: {connected_depts}")

# 4. Executive Summary
print("\n4. EXECUTIVE SUMMARY")
summary = analytics.generate_executive_summary()
print(f"Network Overview:")
for key, value in summary['network_overview'].items():
    print(f"  {key.replace('_', ' ').title()}: {value}")

print(f"\nKey Insights:")
for key, value in summary['key_insights'].items():
    if isinstance(value, dict):
        print(f"  {key.replace('_', ' ').title()}: {value.get('employee', 'N/A')} ({value.get('department', 'N/A')})")
    else:
        print(f"  {key.replace('_', ' ').title()}: {value}")
```

== Advanced Analytics and Visualizations

=== Organizational Health Metrics

```python
class OrganizationalHealthAnalyzer:
    def __init__(self, uri, username, password):
        self.driver = GraphDatabase.driver(uri, auth=(username, password))
    
    def calculate_collaboration_density(self, graph_name='corporate-network'):
        """Calculate collaboration density by department"""
        
        density_query = """
        MATCH (e:Employee)
        WITH e.department AS dept, collect(e) AS dept_employees
        WHERE size(dept_employees) > 1
        UNWIND dept_employees AS emp1
        UNWIND dept_employees AS emp2
        WITH dept, emp1, emp2, dept_employees
        WHERE id(emp1) < id(emp2)
        
        OPTIONAL MATCH (emp1)-[c:COLLABORATES_WITH]-(emp2)
        WITH dept, 
             count(*) AS possible_connections,
             sum(CASE WHEN c IS NOT NULL THEN 1 ELSE 0 END) AS actual_connections,
             size(dept_employees) AS dept_size
        
        RETURN dept,
               dept_size,
               actual_connections,
               possible_connections,
               toFloat(actual_connections) / possible_connections AS density
        ORDER BY density DESC
        """
        
        with self.driver.session() as session:
            result = session.run(density_query)
            return pd.DataFrame([dict(record) for record in result])
    
    def analyze_collaboration_patterns(self, graph_name='corporate-network'):
        """Analyze collaboration patterns and trends"""
        
        patterns_query = """
        MATCH (e1:Employee)-[c:COLLABORATES_WITH]-(e2:Employee)
        WITH e1.department AS dept1, 
             e2.department AS dept2,
             c.type AS collab_type,
             avg(c.strength) AS avg_strength,
             count(*) AS frequency
        
        RETURN dept1, dept2,
               collab_type,
               avg_strength,
               frequency,
               CASE 
                 WHEN dept1 = dept2 THEN 'Internal'
                 ELSE 'Cross-departmental'
               END AS pattern_type
        ORDER BY frequency DESC
        """
        
        with self.driver.session() as session:
            result = session.run(patterns_query)
            return pd.DataFrame([dict(record) for record in result])
    
    def identify_organizational_silos(self, graph_name='corporate-network'):
        """Identify departments that are isolated or poorly connected"""
        
        silos_query = """
        MATCH (e:Employee)
        WITH e.department AS dept, collect(e) AS dept_members
        
        // Calculate internal vs external collaboration ratio
        UNWIND dept_members AS member
        MATCH (member)-[c:COLLABORATES_WITH]-(colleague:Employee)
        WITH dept, member, 
             count(CASE WHEN colleague.department = dept THEN 1 END) AS internal_collabs,
             count(CASE WHEN colleague.department <> dept THEN 1 END) AS external_collabs
        
        WITH dept,
             avg(internal_collabs) AS avg_internal,
             avg(external_collabs) AS avg_external,
             count(member) AS dept_size
        
        WITH dept, dept_size, avg_internal, avg_external,
             CASE 
               WHEN avg_external = 0 THEN 999999
               ELSE avg_internal / avg_external 
             END AS silo_ratio
        
        RETURN dept, dept_size, avg_internal, avg_external, silo_ratio
        ORDER BY silo_ratio DESC
        """
        
        with self.driver.session() as session:
            result = session.run(silos_query)
            return pd.DataFrame([dict(record) for record in result])
    
    def create_health_dashboard(self, graph_name='corporate-network'):
        """Create comprehensive organizational health dashboard"""
        
        # Get all metrics
        density_df = self.calculate_collaboration_density(graph_name)
        patterns_df = self.analyze_collaboration_patterns(graph_name)
        silos_df = self.identify_organizational_silos(graph_name)
        
        # Create visualizations
        fig, axes = plt.subplots(2, 3, figsize=(18, 12))
        fig.suptitle('Organizational Health Dashboard', fontsize=16)
        
        # 1. Collaboration Density by Department
        ax1 = axes[0, 0]
        density_df.plot(x='dept', y='density', kind='bar', ax=ax1, color='skyblue')
        ax1.set_title('Collaboration Density by Department')
        ax1.set_ylabel('Density Score')
        ax1.tick_params(axis='x', rotation=45)
        
        # 2. Internal vs Cross-departmental Collaboration
        ax2 = axes[0, 1]
        internal_external = patterns_df.groupby('pattern_type')['frequency'].sum()
        internal_external.plot(kind='pie', ax=ax2, autopct='%1.1f%%')
        ax2.set_title('Internal vs Cross-departmental Collaboration')
        
        # 3. Silo Risk Assessment
        ax3 = axes[0, 2]
        silos_df.plot(x='dept', y='silo_ratio', kind='bar', ax=ax3, color='lightcoral')
        ax3.set_title('Silo Risk (Internal/External Ratio)')
        ax3.set_ylabel('Silo Ratio')
        ax3.tick_params(axis='x', rotation=45)
        
        # 4. Collaboration Type Distribution
        ax4 = axes[1, 0]
        collab_types = patterns_df.groupby('collab_type')['frequency'].sum()
        collab_types.plot(kind='bar', ax=ax4, color='lightgreen')
        ax4.set_title('Collaboration Types Distribution')
        ax4.set_ylabel('Frequency')
        ax4.tick_params(axis='x', rotation=45)
        
        # 5. Department Size vs External Connections
        ax5 = axes[1, 1]
        ax5.scatter(silos_df['dept_size'], silos_df['avg_external'], 
                   s=100, alpha=0.6, color='gold')
        ax5.set_xlabel('Department Size')
        ax5.set_ylabel('Avg External Connections')
        ax5.set_title('Size vs External Connectivity')
        
        # 6. Collaboration Strength Heatmap
        ax6 = axes[1, 2]
        strength_matrix = patterns_df.pivot_table(
            values='avg_strength', 
            index='dept1', 
            columns='dept2', 
            fill_value=0
        )
        im = ax6.imshow(strength_matrix.values, cmap='YlOrRd', aspect='auto')
        ax6.set_title('Inter-departmental Collaboration Strength')
        ax6.set_xticks(range(len(strength_matrix.columns)))
        ax6.set_yticks(range(len(strength_matrix.index)))
        ax6.set_xticklabels(strength_matrix.columns, rotation=45)
        ax6.set_yticklabels(strength_matrix.index)
        
        plt.tight_layout()
        plt.show()
        
        return {
            'density_analysis': density_df,
            'collaboration_patterns': patterns_df,
            'silo_analysis': silos_df
        }

# Generate organizational health dashboard
health_analyzer = OrganizationalHealthAnalyzer("bolt://localhost:7687", "neo4j", "password")
health_metrics = health_analyzer.create_health_dashboard()

print("\nORGANIZATIONAL HEALTH ANALYSIS")
print("=" * 50)

print("\nDepartment Collaboration Density:")
for _, row in health_metrics['density_analysis'].iterrows():
    print(f"  {row['dept']}: {row['density']:.3f} ({row['actual_connections']}/{row['possible_connections']} connections)")

print("\nSilo Risk Assessment:")
for _, row in health_metrics['silo_analysis'].head(5).iterrows():
    risk_level = "HIGH" if row['silo_ratio'] > 2 else "MEDIUM" if row['silo_ratio'] > 1 else "LOW"
    print(f"  {row['dept']}: {risk_level} risk (ratio: {row['silo_ratio']:.2f})")
```

== Performance Optimization and Best Practices

=== Graph Projection Optimization

```python
def optimize_graph_projections():
    """Best practices for graph projection optimization"""
    
    optimization_tips = {
        'node_filtering': {
            'description': 'Filter nodes to include only relevant entities',
            'example': """
            CALL gds.graph.project(
                'filtered-network',
                {
                    Employee: {
                        label: 'Employee',
                        properties: ['seniority_level'],
                        nodeFilter: 'n.years_at_company >= 2'  // Only experienced employees
                    }
                },
                'COLLABORATES_WITH'
            )
            """,
            'benefits': ['Reduced memory usage', 'Faster algorithms', 'Focused analysis']
        },
        'relationship_filtering': {
            'description': 'Filter relationships by properties or types',
            'example': """
            CALL gds.graph.project(
                'strong-collaborations',
                'Employee',
                {
                    STRONG_COLLAB: {
                        type: 'COLLABORATES_WITH',
                        relationshipFilter: 'r.strength >= 0.7'  // Only strong collaborations
                    }
                }
            )
            """,
            'benefits': ['Noise reduction', 'Quality focus', 'Better results']
        },
        'property_selection': {
            'description': 'Include only necessary properties',
            'example': """
            CALL gds.graph.project(
                'minimal-network',
                'Employee',
                {
                    COLLABORATES_WITH: {
                        properties: ['strength']  // Only include required properties
                    }
                }
            )
            """,
            'benefits': ['Lower memory footprint', 'Faster loading', 'Cleaner analysis']
        }
    }
    
    return optimization_tips

# Algorithm Parameter Tuning
def algorithm_parameter_guide():
    """Guide for tuning algorithm parameters"""
    
    parameter_guide = {
        'pagerank': {
            'dampingFactor': {
                'default': 0.85,
                'range': '(0, 1)',
                'tuning': 'Lower = more random walk, Higher = more structure-dependent'
            },
            'maxIterations': {
                'default': 20,
                'range': '1-100',
                'tuning': 'Increase if algorithm hasn\'t converged'
            }
        },
        'louvain': {
            'tolerance': {
                'default': 0.0001,
                'range': '0.0001-0.01',
                'tuning': 'Lower = more precise, Higher = faster convergence'
            },
            'maxLevels': {
                'default': 10,
                'range': '1-20',
                'tuning': 'Higher = deeper hierarchy, may find smaller communities'
            }
        },
        'betweenness': {
            'samplingSize': {
                'default': None,
                'range': '100-10000',
                'tuning': 'Use sampling for large graphs to improve performance'
            }
        }
    }
    
    return parameter_guide
```

== Exercise Validation and Assessment

=== Knowledge Check Questions

1. **Algorithm Selection**: For identifying employees who would be critical to remove from the organization (single points of failure), which centrality measure is most appropriate?

2. **Community Interpretation**: You found a community containing employees from Engineering, Sales, and Marketing. What does this likely represent and how should management respond?

3. **Performance Optimization**: Your graph has 10,000 employees and 50,000 collaborations. Betweenness centrality is taking too long to compute. What optimization strategies would you apply?

=== Practical Assessment Tasks

```python
def validation_exercises():
    """Hands-on validation exercises"""
    
    exercises = {
        'exercise_1': {
            'task': 'Find the top 3 employees who connect the most departments',
            'expected_approach': [
                'Run betweenness centrality',
                'Filter for cross-departmental connections',
                'Count unique departments per employee',
                'Rank by department diversity'
            ],
            'validation_query': """
            // Sample solution approach
            CALL gds.betweenness.stream('corporate-network')
            YIELD nodeId, score
            WITH gds.util.asNode(nodeId) AS emp, score
            MATCH (emp)-[:COLLABORATES_WITH]-(colleague:Employee)
            WHERE emp.department <> colleague.department
            WITH emp, score, collect(DISTINCT colleague.department) AS depts
            RETURN emp.name, emp.department, size(depts) AS dept_connections, score
            ORDER BY dept_connections DESC, score DESC
            LIMIT 3
            """
        },
        'exercise_2': {
            'task': 'Identify the most isolated department (highest silo risk)',
            'expected_approach': [
                'Calculate internal vs external collaboration ratios',
                'Compare departments by silo metrics',
                'Consider both absolute and relative isolation'
            ],
            'validation_query': """
            // Calculate silo ratio for each department
            MATCH (e:Employee)
            WITH e.department AS dept, collect(e) AS members
            UNWIND members AS member
            MATCH (member)-[:COLLABORATES_WITH]-(colleague:Employee)
            WITH dept, member,
                 sum(CASE WHEN colleague.department = dept THEN 1 ELSE 0 END) AS internal,
                 sum(CASE WHEN colleague.department <> dept THEN 1 ELSE 0 END) AS external
            WITH dept, avg(internal) AS avg_internal, avg(external) AS avg_external
            RETURN dept, avg_internal/avg_external AS silo_ratio
            ORDER BY silo_ratio DESC
            """
        },
        'exercise_3': {
            'task': 'Design a succession plan for the top 5 most critical employees',
            'expected_approach': [
                'Identify critical employees using multiple centrality measures',
                'Find potential successors with similar network positions',
                'Assess knowledge transfer requirements'
            ]
        }
    }
    
    return exercises

# Display validation exercises
exercises = validation_exercises()
print("\nVALIDATION EXERCISES")
print("=" * 40)

for ex_id, exercise in exercises.items():
    print(f"\n{ex_id.upper()}: {exercise['task']}")
    print("Expected Approach:")
    for step in exercise['expected_approach']:
        print(f"  • {step}")
    if 'validation_query' in exercise:
        print("Sample Query:")
        print(exercise['validation_query'])
```

== Results Summary and Next Steps

=== Executive Recommendations

```python
def generate_final_recommendations(analysis_results):
    """Generate actionable recommendations based on analysis"""
    
    recommendations = {
        'immediate_actions': [
            'Identify and protect key bridge employees (high betweenness centrality)',
            'Create mentorship programs pairing high-influence employees with emerging talent',
            'Establish cross-departmental project teams in isolated departments'
        ],
        'strategic_initiatives': [
            'Develop succession plans for employees with highest PageRank scores',
            'Create communities of practice based on natural collaboration patterns',
            'Implement knowledge sharing platforms to reduce dependency on key individuals'
        ],
        'ongoing_monitoring': [
            'Monthly collaboration network health dashboards',
            'Quarterly centrality analysis to track changes in influence patterns',
            'Annual community detection to identify evolving organizational structure'
        ],
        'risk_mitigation': [
            'Cross-train employees in critical positions',
            'Establish redundant communication paths between departments',
            'Create early warning systems for organizational fragmentation'
        ]
    }
    
    return recommendations

# Generate final recommendations
final_recommendations = generate_final_recommendations({})

print("\nFINAL RECOMMENDATIONS")
print("=" * 40)

for category, actions in final_recommendations.items():
    print(f"\n{category.replace('_', ' ').title()}:")
    for action in actions:
        print(f"  • {action}")
```

== Summary

This hands-on exercise demonstrated how to build a complete graph analytics pipeline for organizational network analysis. You learned to:

### ✅ **Technical Skills Gained**
- Create and optimize graph projections for different analytical needs
- Apply multiple centrality algorithms to identify influential nodes
- Use community detection to uncover organizational structure  
- Build comprehensive dashboards and visualizations
- Optimize performance for large-scale graph analysis

### 🎯 **Business Value Delivered**
- Identified key influencers and bridge employees in the organization
- Uncovered natural communities and cross-functional collaboration patterns
- Assessed organizational health and silo risks
- Provided actionable recommendations for leadership development and succession planning

### 🚀 **Next Module Preview**
Module 5: Retrievers will teach you to build intelligent information retrieval systems that combine graph traversal with vector search for enhanced RAG applications.

You now have practical experience applying graph analytics to solve real business problems and can confidently use these techniques in your own organizational analysis projects!