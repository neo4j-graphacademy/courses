= Creating an LLM Instance
:type: challenge

In the link:/courses/llm-fundamentals/3-intro-to-langchain/[Initializing the LLM lesson of Neo4j & LLM Fundamentals^], you learned how to initialize an LLM class and generate a response from an LLM.
In this lesson, you will need to put this learning into practice by creating an LLM instance to communicate with a GPT model using OpenAI.

You will need to:

1. Obtain an API key from link:https://platform.openai.com[platform.openai.com]
2. Create a secrets file to save the API key
3. Initialize an instance of the `ChatOpenAI`
4. Create an instance of the `OpenAIEmbeddings` model


== Obtain an API Key

// TODO: Record video
To obtain an API key, you will need to log into link:https://platform.openai.com[platform.openai.com].
Use the left-hand menu to navigate to the **API keys** section and click **+ Create new secret key**.
Give the key the name `GraphAcademy Course` and click **Create secret key**.

If you have successfully followed these steps, you will be presented with a new API key.
Click the **Copy** icon to the right of the key to copy to your clipboard.

== Setting a Streamlit Secret

The Streamlit documentation link:https://docs.streamlit.io/library/advanced-features/secrets-management[outlines four approaches to handling secrets and credentials] in your application.
For simplicity, we recommend creating a `secrets.toml` file in `.streamlit/` folder.

Copy the following text into a newly created `.streamlit/secrets.toml` file, replacing the value in quotes with your API key.

..streamlit/secrets.toml
[source,toml]
----
include::./includes/secrets.toml[]
----

You can access this value in your Streamlit application using the `st.secrets` map.

[source,python]
.Accessing Secrets
----
import streamlit as st

openai_api_key = st.secrets['OPENAI_API_KEY']
----


[WARNING]
.Keep your secrets safe
====
Make sure you do not share your API keys or include them in a `git commit`.
We have added the `.streamlit/secrets.toml` file to the `.gitignore` so the contents is not accidentally uploaded to Github.
====


== Initializing an OpenAI LLM

As the LLM will be used across the application, it makes sense to create a new `llm.py` file in the project root.
This file will contain an `llm` variable which holds an instance of the `ChatOpenAI` object.

The LLM should be initialized with an `openai_api_key` keyword argument.
This should be set to the secret defined in the previous step.

[source,python]
.llm.py
----
include::{repository-raw}/main/llm.py[tag=llm]
----


== Initializing an Embedding Model

To use the Vector Search Index, you will also need to create an instance of the `OpenAIEmbeddings` model.
This will be used by Langchain to create an embedding of the user's input which will then be

[source,python]
.llm.py
----
include::{repository-raw}/main/llm.py[tag=embedding]
----

== Using the LLM

Once you have completed the steps, you will be able to `import` the `llm` and `embeddings` objects into other modules within the project.

[source,python]
from llm import llm, embeddings


== That's it!

Once you have completed the steps above, click the button to mark the lesson as completed.

read::I have an LLM![]


[.summary]
== Summary

In this lesson, you have created the classes required to interact with OpenAI's LLMs.

In the next lesson, you will create the classes required to connect to Neo4j.
