= Cypher Evaluation Chain
:type: challenge
:prompt-filename: cypher-evaluation.txt
:lab-filename: cypher-evaluation.chain.ts
:lab-file: modules/agent/tools/cypher/{lab-filename}
:lab: {repository-blob}/main/src/{lab-file}
:lab-solution: src/solutions/{lab-file}
:test-filename: cypher-evaluation.chain.test.ts
:test-file: src/modules/agent/tools/cypher/{test-filename}

// TODO: Flaky test, the evaluation chain is flaky in general.

Have you ever heard of link:https://meta.wikimedia.org/wiki/Cunningham%27s_Law[Cunningham's Law^]?  Cunningham's Law states:

> The best way to get the right answer on the internet is not to ask a question; it's to post the wrong answer.

It also seems that this is true when it comes to LLMs!

While LLMs are good at _generating_ Cypher statements, they really excel at __correcting__ the statements that they have written.

To complete this challenge, you must modify the `initCypherEvaluationChain()` function in link:{repository-blob}/main/{lab-file}[`{lab-file}`^] to return a chain validates the provided Cypher statement for accuracy and corrects where necessary.

1. Create a prompt that instructs the LLM to analyze a Cypher statement and return a list of errors.
2. Create a chain that replaces placeholders in the prompt for `schema`, `question`, `cypher` and `errors`.
3. Pass the formatted prompt to the LLM
4. Parse the output as a JSON object.

This chain will be used to recursively correct the Cypher statement generated by the LLM.

lab::Open `{lab-filename}`[]

== Prompt Template

In the `initCypherEvaluationChain()` function, use the `PromptTemplate.fromTemplate()` method to create a new prompt template with the following prompt.

.Prompt
[source]
----
include::{repository-raw}/{branch}/prompts/{prompt-filename}[]
----

[TIP]
.Output Instructions
====
This prompt instructs the LLM to output a JSON object containing keys for `cypher` and `errors`.

This differs from the chains you have built before because they all return a string.  To parse the output as a string, you will use the `JsonOutputParser` class to interpret the response and coerce it into an object.
====

Your code should resemble the following:

.Prompt Template
[source,typescript,indent=0]
----
include::{repository-raw}/{branch}/{lab-solution}[tag=prompt, indent=0]
----

[NOTE]
.Braces in prompts
====
The code above uses double braces (`{{` and `}}`) to escape any braces that aren't intended to be placeholders for text.
====

== Return a Runnable Sequence

Use the `RunnableSequence.from()` method to create a new chain.

.Return a RunnableSequence
[source,typescript,indent=0]
----
return RunnableSequence.from([
  // ...
])
----



=== Initial Inputs

The chain will be used to recursively verify using the output described in the prompt, which includes an array of errors.

The prompt will need these in string format, so as the first step, use the `RunnablePassthrough.assign()` method to convert the array of errors into a single string.

[source,typescript]
----
include::{repository-raw}/{branch}/{lab-solution}[tag=startsequence, indent=0]
include::{repository-raw}/{branch}/{lab-solution}[tag=assign, indent=2]
  // ...
include::{repository-raw}/{branch}/{lab-solution}[tag=endsequence, indent=0]
----


// ----



=== Format Prompt and Process

Now that the inputs required for the prompt have been prepared, these can be formatted in the prompt, passed to the LLM and the output parsed as a string.

Then, finally, the response from the LLM should be passed to a new instance of the `JsonOutputParser`.


[source,typescript]
----
include::{repository-raw}/{branch}/{lab-solution}[tags="runnable", indent=0]
----


=== Completed Sequence

If you have followed the steps correctly, your code should resemble the following:

[source,typescript]
----
include::{repository-raw}/{branch}/{lab-solution}[tag=function, indent=0]
----

include::../../../../includes/test.adoc[leveloffset=+1]


== It works!

If all the tests have passed, you will now have a chain that will evaluate a Cypher statement and provide hints to the LLM the next time it is asked to evaluate the statement.

Once you have finished, hit the button below to mark the challenge as complete.

read::It works![]


[.summary]
== Summary

In this lesson, you built a chain that will evaluate the Cypher statement built in the last step.

In the next lesson, you will create a chain that will generate an authoritative answer to a question based on the context provided.
