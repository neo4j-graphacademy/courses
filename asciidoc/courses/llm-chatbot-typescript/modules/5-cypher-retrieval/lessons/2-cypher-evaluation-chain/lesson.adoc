= Cypher Evaluation Chain
:type: challenge
:prompt-filename: cypher-evaluation.txt
:lab-filename: cypher-evaluation.chain.ts
:lab-file: modules/agent/tools/cypher/{lab-filename}
:lab: {repository-blob}/main/src/{lab-file}
:lab-solution: src/solutions/{lab-file}
:test-filename: cypher-evaluation.chain.test.ts
:test-file: src/modules/agent/tools/cypher/{test-filename}
:order: 2

// TODO: Flaky test, the evaluation chain is flaky in general.

Have you ever heard of link:https://meta.wikimedia.org/wiki/Cunningham%27s_Law[Cunningham's Law^]?  Cunningham's Law states:

> The best way to get the correct answer on the internet is not to ask a question; it's to post the wrong answer.

It also seems that this is true when it comes to LLMs!

While LLMs are good at _generating_ Cypher statements, they _really_ excel at __correcting__ the statements they have written.

To complete this challenge, you must modify the `initCypherEvaluationChain()` function in link:{repository-blob}/main/{lab-file}[`{lab-file}`^] to return a chain that validates the provided Cypher statement for accuracy and corrects where necessary.

1. Create a prompt instructing the LLM to analyze a Cypher statement and return a list of errors.
2. Create a chain that replaces placeholders in the prompt for `schema`, `question`, `cypher`, and `errors`.
3. Pass the formatted prompt to the LLM
4. Parse the output as a JSON object.

This chain will recursively correct the Cypher statement generated by the LLM.

lab::Open `{lab-filename}`[]

== Prompt Template

In the `initCypherEvaluationChain()` function, use the `PromptTemplate.fromTemplate()` method to create a new prompt template with the following prompt.

.Prompt
[source]
----
include::{repository-raw}/{branch}/prompts/{prompt-filename}[]
----

[TIP]
.Output Instructions
====
This prompt instructs the LLM to output a JSON object containing keys for `cypher` and `errors`.

This differs from the chains you have built before because they all return a string.  To parse the output as a string, you will use the `JsonOutputParser` class to interpret the response and coerce it into an object.
====

Your code should resemble the following:

.Prompt Template
[source,typescript,indent=0]
----
include::{repository-raw}/{branch}/{lab-solution}[tag=prompt, indent=0]
----

[NOTE]
.Braces in prompts
====
Use double braces (`{{` and `}}`) to escape braces that are not text placeholders.
====

== Return a Runnable Sequence

Use the `RunnableSequence.from()` method to create a new chain.

.Return a RunnableSequence
[source,typescript,indent=0]
----
return RunnableSequence.from([
  // ...
])
----



=== Initial Inputs

The chain will recursively verify using the output described in the prompt, which includes an array of errors.

The prompt will need these in string format, so as the first step, use the `RunnablePassthrough.assign()` method to convert the array of errors into a single string.

[source,typescript]
----
include::{repository-raw}/{branch}/{lab-solution}[tag=startsequence, indent=0]
include::{repository-raw}/{branch}/{lab-solution}[tag=assign, indent=2]
  // ...
include::{repository-raw}/{branch}/{lab-solution}[tag=endsequence, indent=0]
----


// ----



=== Format Prompt and Process

Now that you have the inputs that the prompt expects, update the chain to format the prompt, pass it to the LLM to process and parse the output.

[source,typescript]
----
include::{repository-raw}/{branch}/{lab-solution}[tags="runnable", indent=0]
----


=== Completed Sequence

If you have followed the steps correctly, your code should resemble the following:

[source,typescript]
----
include::{repository-raw}/{branch}/{lab-solution}[tag=function, indent=0]
----

include::../../../../includes/test.adoc[leveloffset=+1]


== It works!

If all the tests have passed, you will have a chain that evaluates a Cypher statement and provides hints if any errors are detected.

Hit the button below to mark the challenge as complete.

read::It works![]


[.summary]
== Summary

In this lesson, you built a chain that evaluates the Cypher statement generated in the Cypher Generation chain.

In the next lesson, you will create a chain that will generate an authoritative answer to a question based on the context provided.
