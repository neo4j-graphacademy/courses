= Cypher Retrieval Chain
:type: challenge
:lab-filename: cypher-retrieval.chain.ts
:lab-file: modules/agent/tools/cypher/{lab-filename}
:lab: {repository-blob}/main/src/{lab-file}
:lab-solution: src/solutions/{lab-file}
:test-filename: cypher-retrieval.chain.test.ts
:test-file: src/modules/agent/tools/cypher/{test-filename}
:order: 4

Now that you have all of the components needed to perform data retrieval from Neo4j with Cypher, you need to combine them.

To complete this challenge, you must create a `Runnable` instance that:

1. Generates and evaluates a Cypher statement
2. Use the Cypher statement to retrieve data from the database
3. Extract the element IDs and convert the results to a `string` for use in the context prompt
4. Generate an answer using the answer generation chain
5. Save the response to the database along with the Cypher statement
6. Return the LLM response

lab::Open `{lab-filename}`[]


== Cypher Generation and Evaluation

To generate and evaluate a new Cypher statement, you'll need to create a function that will first generate the statement.

The link:{repository-blob}/main/src/{lab-file}[`{lab-filename}` file^] already has a placeholder function called `recursivelyEvaluate()` to perform this task.

[source,typescript]
----
include::{repository-raw}/{branch}/src/{lab-file}[tag="recursive"]
----


In this function, first use the link:../1-cypher-generation-chain/[`initCypherGenerationChain` function from Cypher Generation Chain lesson^] and
link:../2-cypher-evaluation-chain/[`initCypherEvaluationChain` function from the Cypher Evaluation Chain lesson^] to create the generation and evaluation chains.

.Cypher Chains
[source,typescript]
----
include::{repository-raw}/{branch}/{lab-solution}[tag="chains",indent=0]
----

Next, invoke the `generationChain` to generate an initial Cypher statement.


.Generate Initial Cypher
[source,typescript]
----
include::{repository-raw}/{branch}/{lab-solution}[tag="initialcypher",indent=0]
----

Now, use a `while` loop to recursively evaluate the Cypher statement, up to five times, until the number of errors returned by the evaluation chain is 0.

.Evaluate Cypher
[source,typescript]
----
include::{repository-raw}/{branch}/{lab-solution}[tag="evaluateloop",indent=0]
----

Finally, return the `cypher` statement.

.Return Cypher
[source,typescript]
----
include::{repository-raw}/{branch}/{lab-solution}[tag="evaluatereturn",indent=0]
----

// TODO: How do we fix this?!
[TIP]
.`id()` to `elementId()` replacement
====
The first line of this code contains a fix that converts `id({variable})` to `elementId({variable})`.
No matter what we try in the prompt, both the GPT-3.5 Turbo and GPT-4 models seem to use the deprecated `id()` method over the `elementId()`.

At some point, the models will learn that the `id()` method is deprecated.
This is also a hint that we may need to train a model to generate valid Cypher statements.
====

[%collapsible]
.View full `recursivelyEvaluate` function
====

[source,typescript]
----
include::{repository-raw}/{branch}/{lab-solution}[tag="recursive",indent=0]
----

====



== Handling errors

The LLM will generate a correct Cypher statement _most_ of the time.
But, as we've found in testing, depending on the instructions provided to the prompt the loop of Cypher generation and evaluation can be flaky.

To make the application more robust, you can execute your Cypher statement with an additional evaluation loop.
If the database throws an error, for example highlighting a syntax error, you can use the same evaluation chain to analyze the error message and rewrite the statement accordingly.

Find the `getResults()` function in `{lab-filename}`.

[source,typescript]
----
include::{repository-raw}/{branch}/src/{lab-file}[tag="results"]
----

Replace the `// TODO` comment with code that will attempt to execute the Cypher statement and retry if the `graph.query()` method throws an error.

Start by defining a `results` variable and an `attempts` variable to hold the maximum number of attempts.
Define a mutable `cypher` statement to hold the Cypher statement.
Then, call the `initCypherEvaluationChain()` function to create an instance of the evaluation chain.

[source,typescript]
----
include::{repository-raw}/{branch}/{lab-solution}[tag="resultvars",indent=0]
----

Next, create a `while` loop that will iterate for a maximum of five times.
Inside use `try`/`catch` to attempt to execute the Cypher statement.

If an error is thrown, pass the `.message` property along with the cypher statement, question, and schema to the evaluation chain.

Assign the output of the evaluation chain to the `cypher` statement.

[source,typescript]
----
include::{repository-raw}/{branch}/src/{lab-solution}[tag="resultloop"]
----

Finally, return the `results`.



== Building the Chain

This portion of the challenge will take place in the `initCypherRetrievalChain()` function.

[source,typescript]
----
include::{repository-raw}/{branch}/src/{lab-file}[tag="function"]
----

As this chain will be called by an agent, it will receive a structured input containing an `input` and `rephrasedQuestion`.

.Agent to Tool Input
[source,typescript]
----
include::{repository-raw}/{branch}/src/modules/agent/agent.types.ts[tag=agenttoolinput,indent=0]
----


=== Initialize Chains

You will need to use the Generate Authoritative Answer Chain from the previous lesson to generate an answer.
Use the `initGenerateAuthoritativeAnswerChain()` function

.Generate Answer Chain
[source,typescript]
----
include::{repository-raw}/{branch}/{lab-solution}[tag="answerchain",indent=0]
----



=== Generate a Cypher Statement

Now, define the output.
As with the Vector Retrieval tool, you can return a `Runnable` using `RunnablePassthrough.assign()`.

The first step is to call the `recursivelyEvaluate()` function, assigning the output to the `cypher` key.

.Generate Initial Cypher
[source,typescript]
----
include::{repository-raw}/{branch}/{lab-solution}[tag="cypher",indent=0]
----

=== Get Results

Use the `getResults()` function to get the results from the database.

[source,typescript]
----
include::{repository-raw}/{branch}/{lab-solution}[tag="getresults",indent=0]
----

=== Manipulate Results

To save the context to the database, we'll need to extract any element IDs from the results.
The link:{repository-blob}main/src/utils.ts[`utils.ts`^] file exports an `extractIds()` function that recursively iterates through the results to find any objects with a key of `_id`.

[%collapsible]
.View the `extractIds()` function
====

[source,typescript]
----
include::{repository-raw}/{branch}/src/utils.ts[tag="extractids",indent=0]
----
====

The `result` obtained in the previous step also needs to be converted to a string.
If there is only one result, use `JSON.stringify()` to convert the first object to a JSON string, otherwise return a string representing the entire array.

[source,typescript]
----
include::{repository-raw}/{branch}/{lab-solution}[tag="extract",indent=0]
----

=== Generate Output

The input and context can then be passed to the Authoritative Answer Generation chain to generate an answer.

[source,typescript]
----
include::{repository-raw}/{branch}/{lab-solution}[tag="answer",indent=0]
----

=== Save response to database

Next, use the `saveHistory()` function built in module 3 to save the details of the response to the database.

[source,typescript]
----
include::{repository-raw}/{branch}/{lab-solution}[tag="save",indent=0]
----

=== Return the output

Finally, use the `pick()` function to return the `output` key.

[source,typescript]
----
include::{repository-raw}/{branch}/{lab-solution}[tag="output",indent=0]
----


== Final Function

If you have followed the instructions correctly, your code should resemble the following:

.Full Function
[source,typescript]
----
include::{repository-raw}/{branch}/{lab-solution}[tag=function]
----

include::../../../../includes/test.adoc[leveloffset=+1]

[IMPORTANT]
.Randomized responses
====
LLMs are probabilistic models and will generate different responses each time they are called.

There are multiple tests for this function, depending on the responses from the LLM, you will find that not all tests will pass all of the time.

You may need to run the test multiple times.
====

include::questions/verify.adoc[leveloffset=+1]

[.summary]
== Summary

In this lesson, you combined the components built during this module to create a chain that will generate a Cypher statement that answers the user's question, executes the Cypher statement and generates a response.

In the next module, you will build an agent that combines this chain with the Vector Retrieval Chain to create a single interface that can be used in the chat interface.
