= Authoritative Answers
:type: challenge
:speculative-prompt-filename: speculative-answer-generation.txt
:prompt-filename: authoritative-answer-generation.txt
:lab-filename: authoritative-answer-generation.chain.ts
:lab-file: modules/agent/chains/{lab-filename}
:lab: {repository-blob}/main/src/{lab-file}
:lab-solution: src/solutions/{lab-file}
:test-filename: authoritative-answer-generation.chain.test.ts
:test-file: src/modules/agent/chains/{test-filename}



In the link:../../2-chains/2-answer-generation[Answer Generation Chain challenge^], you created a chain that took speculative results based on similar documents found in the vector search index.

Because of the way the semantic search works, similar documents may be identified that don't provide an answer to the question.

In this case, the prompt must contain specific instructions in case the context doesn't answer the question.

[%collapsible]
.View the original prompt
====
.Speculative Answers
[source]
----
include::{repository-raw}/{branch}/prompts/{speculative-prompt-filename}[]
----
====

In the case of answers that are retrieved from the database, as long as the Cypher statement that the LLM has generated is semantically correct, the results _will_ answer the question.

As such, the prompt should reflect that the information has come from an authoritative source.


To complete this challenge, you must:

1. Create a prompt that provides the LLM with instructions to authoritatively answer the question based on the context provided.
2. Pass the formatted prompt to the LLM
3. Convert the output to a string


== Create a Prompt Template

Inside the `initGenerateAuthoritativeAnswerChain()` function, use the `PromptTemplate.fromTemplate()` method to create a new prompt template.
Use the following prompt as the first parameter.

.Prompt
[source]
----
include::{repository-raw}/{branch}/prompts/{prompt-filename}[]
----

Your code should resemble the following:

.Prompt Template
[source,typescript,indent=0]
----
include::{repository-raw}/{branch}/{lab-solution}[tag=prompt, indent=0]
----

== Create the Runnable Sequence

Use the `RunnableSequence.from()` method to create a new chain.

The chain must first check the `context` value passed to the chain, and replace it with text instructing the LLM that no answer has been provided undefined or an empty string.

The input can then be used to format the prompt, pass to the LLM and coerce the output as a string.

[source,typescript]
----
include::{repository-raw}/{branch}/{lab-solution}[tag=sequence, indent=0]
----


== Working Solution

.Click here to reveal the fully-implemented `authoritative-answer-generation.chain.ts`
[%collapsible]
====
[source,js,indent=0]
----
include::{repository-raw}/{branch}/{lab-solution}[]
----
====

== Using the Chain

You will be able to initialize and run the chain in your application with the following code:

[source,typescript]
----
include::{repository-raw}/{branch}/{lab-solution}[tag=usage]
----


include::../../../../includes/test.adoc[leveloffset=+1]


read::it works![]


[.summary]
== Summary

In this lesson, you created a chain to authoritatively answer a question based on the context provided.

In the next lesson, you will build a chain that uses the chains you have built so far to generate a Cypher statement, generate an answer to the question and save the response and context in the database.
