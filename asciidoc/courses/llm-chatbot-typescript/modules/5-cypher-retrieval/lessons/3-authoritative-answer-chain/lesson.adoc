= Authoritative Answers
:type: challenge
:speculative-prompt-filename: speculative-answer-generation.txt
:prompt-filename: authoritative-answer-generation.txt
:lab-filename: authoritative-answer-generation.chain.ts
:lab-file: modules/agent/chains/{lab-filename}
:lab: {repository-blob}/main/src/{lab-file}
:lab-solution: src/solutions/{lab-file}
:test-filename: authoritative-answer-generation.chain.test.ts
:test-file: src/modules/agent/chains/{test-filename}
:order: 3


In the link:../../2-chains/2-answer-generation[Answer Generation Chain challenge^], you created a chain that took speculative results based on similar documents identified using the vector search index.

Due to the nature of semantic search, it might return documents that seem similar but do not address the question. Therefore, the prompt should include specific instructions for handling situations where the context does not provide an answer to the question.


[%collapsible]
.View the original prompt
====
.Speculative Answers
[source]
----
include::{repository-raw}/{branch}/prompts/{speculative-prompt-filename}[]
----
====

In the case of answers retrieved from the database, as long as the Cypher statement that the LLM has generated is semantically correct, the results _will_ answer the question.

As such, the prompt should reflect that the information has come from an authoritative source.


To complete this challenge, you must:

1. Create a prompt instructing the LLM to answer the question authoritatively based on the provided context
2. Pass the formatted prompt to the LLM
3. Convert the output to a string


== Create a Prompt Template

Modify the `initGenerateAuthoritativeAnswerChain()` function, in link:{repository-blob}/main/{lab-file}[`{lab-file}`^], to use the `PromptTemplate.fromTemplate()` method to create a new prompt template.
Use the following prompt as the first parameter.

.Prompt
[source]
----
include::{repository-raw}/{branch}/prompts/{prompt-filename}[]
----

lab::Open `{lab-filename}`[]

Your code should resemble the following:

.Prompt Template
[source,typescript,indent=0]
----
include::{repository-raw}/{branch}/{lab-solution}[tag=prompt, indent=0]
----

== Create the Runnable Sequence

Use the `RunnableSequence.from()` method to create a new chain.

The chain must initially inspect the context value passed to it; if it is empty or undefined, it should inform the LLM that no data was found to answer the question.

Then, format the prompt, pass it to the LLM, and coerce the output into a string.

[source,typescript]
----
include::{repository-raw}/{branch}/{lab-solution}[tag=sequence, indent=0]
----


== Working Solution

.Click here to reveal the fully-implemented `authoritative-answer-generation.chain.ts`
[%collapsible]
====
[source,js,indent=0]
----
include::{repository-raw}/{branch}/{lab-solution}[]
----
====

== Using the Chain

Later in the course, you will update the application to use the chain.
You could initialize and run the chain with the following code:

[source,typescript]
----
include::{repository-raw}/{branch}/{lab-solution}[tag=usage]
----


include::../../../../includes/test.adoc[leveloffset=+1]


read::it works![]


[.summary]
== Summary

In this lesson, you created a chain to answer a question authoritatively based on the context provided.

In the next lesson, you will build a chain that combines the chains made in this module to generate and execute a Cypher statement before generating an answer.
