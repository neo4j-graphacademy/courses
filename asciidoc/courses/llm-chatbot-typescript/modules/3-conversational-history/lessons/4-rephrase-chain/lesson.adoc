= Question Rephrasing
:type: challenge
:prompt-filename: rephrase-question.txt
:lab-filename: rephrase-question.chain.ts
:lab-file: modules/agent/chains/{lab-filename}
:lab: {repository-blob}/main/src/{lab-file}
:lab-solution: src/solutions/{lab-file}
:test-filename: rephrase-question.chain.test.ts
:test-file: src/modules/agent/chains/{test-filename}




In this challenge, you must modify the `initGenerateAnswerChain()` function in link:{repository-blob}/main/{lab-file}[`{lab-file}`^] to add a chain that rephrases.

The chain will accept the following input:

.Chain Input
[source,typescript]
----
include::{repository-raw}/{branch}/{lab-solution}[tag=interface]
----


The output of the chain will be a `string`.

You will need to:

1. Take the input and convert the message history into a string in the following format: +
[source]
Human: {input}
AI: {output}

2. Pass the history and input to a `PromptTemplate` containing the prompt in link:{repository-blob}/main/prompts/{prompt-filename}[`prompts/{prompt-filename}`^].
3. Pass the formatted prompt to the LLM passed to the function as an argument
4. Convert the output to a string


== Convert Conversation History to a string

// TOOD: have we covered this before?

To convert the conversation history to a string, you should use the `RunnablePassthrough.assign()` method to format the array of Chatbot responses in the `history` key as a single string.

The input variable for `history` will be an array of `ChatbotResponse` objects.
Use the `.map()` method on the array to convert the `input` and `output` properties of each object to a string, and use the `.join()` method to join them into a single string.

.Reformatting Messages
[source,typescript]
----
include::{repository-raw}/{branch}/{lab-solution}[tag=assign,indent=0]
----

== Create a Prompt Template

Use the `PromptTemplate.fromTemplate()` static method to create a new prompt template containing the following prompt.

[source]
.Rephrase Question Prompt
----
include::{repository-raw}/{branch}/prompts/{prompt-filename}[]
----

Your code should resemble the following:


.Prompt Template
[source,typescript,indent=0]
----
include::{repository-raw}/{branch}/{lab-solution}[tag=prompt]
----


== Runnable Sequence

Next, use the `RunnableSequence.from()` static method to create a new chain that takes the `RephraseQuestionInput` and outputs a `string`.

The `RunnableSequence` will need to:

1. Convert message history to a string
2. Use the input and formatted history to format the prompt
3. Pass the formatted prompt to the LLM
4. Coerce the output into a string

Use the `return` keyword to return the sequence from the function.

.Full Sequence
[source,typescript]
----
include::{repository-raw}/{branch}/{lab-solution}[tag=sequence,indent=0]
----

== Using the Chain

You will be able to initialize and run the chain in your application with the following code:

[source,typescript]
----
include::{repository-raw}/{branch}/{lab-solution}[tag=usage]
----

include::../../../../includes/test.adoc[leveloffset=+1]

== It works!

Once you have received a rephrased question from the LLM, click the button below to mark the challenge as completed.

read::It works![]


[.summary]
== Summary

In this lesson, you built a chain that will take this history to rephrase the user's input into a standalone question.

In the next module, you will build a chain that uses a retriever to query a vector store for documents that are similar to an input.
