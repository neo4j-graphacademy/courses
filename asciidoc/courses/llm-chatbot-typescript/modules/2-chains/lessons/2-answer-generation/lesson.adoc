= Answer Generation Chain
:type: challenge
:prompt-filename: speculative-answer-generation.txt
:lab-filename: answer-generation.chain.ts
:lab-file: modules/agent/chains/{lab-filename}
:lab: {repository-blob}/main/src/{lab-file}
:lab-solution: src/solutions/{lab-file}
:test-filename: speculative-answer-generation.chain.test.ts
:test-file: src/modules/agent/chains/{test-filename}
:order: 2


When implementing **Retrieval Augmented Generation (RAG)**, you provide information as part of the prompt that will help the LLM generate an accurate response.

In this challenge, you must modify the `initGenerateAnswerChain()` function in link:{repository-blob}/main/src/{lab-file}[`{lab-file}`^] to return a chain that instructs the LLM to generate an answer to a question based on the context provided.
The chain should not be concerned with where that information comes from.

The chain will accept the following input:

.Chain Input
[source,typescript]
----
include::{repository-raw}/main/{lab-solution}[tag=interface]
----

The chain will need to:

1. Format a prompt based on the input to the chain.  The prompt template must include `{question}` and `{context}`
2. Pass the formatted prompt to the LLM
3. Convert the output to a string

Open `{lab-filename}`


[TIP]
.Modifying Functions
====
Each challenge in this course will follow the same structure.
The repository contains helper functions that will accept the appropriate arguments.
You will complete the implementation using the arguments passed to the function.

Later in this course, as you build out the agent, you will use these functions to create instances of the chains and register them as tools.
====


== Create a Prompt Template

Inside the `initGenerateAnswerChain()` function, use the `PromptTemplate.fromTemplate()` method to create a new prompt template.
Use the following prompt as the first parameter.

.Prompt
[source]
----
include::{repository-raw}/main/prompts/{prompt-filename}[]
----

Due to the nature of semantic search, the documents passed by the application to this prompt may not fully answer the question.
Therefore. this prompt instructs the LLM to do its best to generate an answer based on the input or respond with "I don't know" if it cannot answer the question.

Later in the course, you will build a chain to provide a definitive answer.

Your code should resemble the following:

.Prompt Template
[source,typescript,indent=0]
----
include::{repository-raw}/main/{lab-solution}[tag=prompt, indent=0]
----

== Create the Runnable Sequence

Use the `RunnableSequence.from()` method to create a new chain.
The chain should pass the prompt to the LLM passed as a parameter, then format the response as a string using a new instance of the `StringOutputParser`.

[source,typescript]
----
include::{repository-raw}/main/{lab-solution}[tag=sequence, indent=0]
----

Use the `return` keyword to return the chain from the function.

== Working Solution

.Click here to reveal the fully-implemented `answer-generation.chain.ts`
[%collapsible]
====
[source,js,indent=0]
----
include::{repository-raw}/main/{lab-solution}[tag=function]
----
====

== Using the Chain

You will be able to initialize and run the chain in your application with the following code:

[source,typescript]
----
include::{repository-raw}/main/{lab-solution}[tag=usage]
----


include::../../../../includes/test.adoc[leveloffset=+2]


read::it works![]


[.summary]
== Summary

In this lesson, you implemented your first chain using LCEL.
You will use this chain to convert raw data from Neo4j into a natural language response.

In the next lesson, you will learn how to validate the response generated by the LLM.
