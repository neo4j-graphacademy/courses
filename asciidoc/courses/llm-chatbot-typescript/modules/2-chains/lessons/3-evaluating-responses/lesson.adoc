= Evaluating LLM Responses
:type: quiz
:lab-filename: src/modules/agent/chains/speculative-answer-generation.chain.test.ts
:lab: {repository-blob}/main/{lab-filename}
:order: 3


You may have noticed in the previous lesson that a you ran link:{repository-raw}/main/{lab-filename}[a unit test used that validated the interaction with the LLM^].

Unit tests are not only a convenient way to test individual elements of an application.
They also provide an automated way to test the response.

The actual response, however, can be more challenging to validate.

== LLM Evaluation

The test uses an evaluation chain to determine whether the response from the LLM has answered the original question.
The test is a take on a technique called **LLM as Judge**, where the LLM is used to judge the quality of the response.

Before running any tests, the `beforeAll()` function creates an instance of the following chain.

.Answer Evaluation Chain
[source,typescript]
----
include::{repository-raw}/main/{lab-filename}[tag=evalchain,indent=0]
----

Each test in the suite uses this chain to evaluate the answer provided in the test.

.Evaluating an answer
[source,typescript]
----
include::{repository-raw}/main/{lab-filename}[tag=eval,indent=0]
----

The test uses a concatenation of the output of the evaluation chain and the original response, so if the evaluation does not return a _yes_, the response is appended to the test output to help debug the issue.

== Prompt Iteration

As you develop your application and test different models, you may need to modify the prompt iteratively until the model returns a consistent response.
Running the tests in _watch_ mode will allow you to receive instant feedback.

This repository includes a link:https://github.com/neo4j-graphacademy/llm-chatbot-typescript/blob/main/package.json#L11[`npm run test:watch`^ command].

.Watching for changes
[source]
----
npm run test:watch
----



== Check your understanding

include::./questions/1-purpose.adoc[leveloffset=+1]


[.summary]
== Summary

In this lesson, you learned how to run automated tests that evaluate the response generated by your chains.

In the next module, you will learn how to use conversation history to rephrase questions and provide more accurate responses.
