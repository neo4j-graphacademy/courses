= Chains and LCEL
:type: lesson

/main

In the link:/courses/llm-fundamentals[Neo4j & LLM Fundamentals course^], you used chains provided by LangChain to achieve communicate with the LLM.
In this course, you will learn how to create custom chains using **LangChain Expression Language**.

== What is LCEL?

LangChain Expression Language, abbreviated to LCEL, is a declarative method for composing chains.
LCEL provides an expressive syntax capable of handling simple tasks such as simple _Prompt to LLM_ chains, or complex combinations of steps.

LCEL provides the following benefits:

* **Streaming Support**
* **Asynchronous Calls**
* **Optimized parallel execution**
* **Streaming Support**


You can link:https://js.langchain.com/docs/expression_language/[read more about LCEL in the LangChain documentation^].


== An example chain

In the link:https://graphacademy.neo4j.com/courses/llm-fundamentals/3-intro-to-langchain/2.5-chains/[Chains lesson in Neo4j & LLM Fundamentals^], you learned about the `LLMChain`.
The link:https://api.python.langchain.com/en/latest/_modules/langchain/chains/llm.html#LLMChain[`LLMChain`^] is an example of a simple chain that, when invoked, takes a user input, replaces the value inside the prompt and passes the prompt to an LLM and specifies the result.

// [source]
// ----
// // TODO: Diagram
// Prompt >> LLM >> Response
// ----

If you link:https://api.python.langchain.com/en/latest/_modules/langchain/chains/llm.html#LLMChain[take a look at the reference documentation], the code is quite verbose.
This chain can be greatly simplified.

The chain should consist of:

1. A `PromptTemplate` containing instructions and placeholders
2. Passed to an LLM
3. The response from the LLM should then be parsed into a specific format.

=== The Prompt

The prompt in the lesson instructs the LLM to act as a _Cockney fruit and vegetable seller_ and provide information about fruit.

This is how you construct the `PromptTemplate` in LangChain.js.

[source,typescript]
----
include::{repository-raw}/{branch}/examples/chain.mjs[tag=prompt]
----

=== The LLM

The prompt will be passed to an LLM, in this case, the `ChatOpenAI` model.

[source,typescript]
----
include::{repository-raw}/{branch}/examples/chain.mjs[tag=llm]
----


=== Creating the Chain

In LangChain.js, chains are instances of the `RunnableSequence` class.
To create a new chain, call the `RunnableSequence.from` method, passing through an array of steps.

[source,typescript]
----
include::{repository-raw}/{branch}/examples/chain.mjs[tag=chain]
----


=== Invoke the chain

The `RunnableSequence` instance has an `invoke()` method.
The input that this function expects depends on the template variables contained in the prompt.

Because the prompt expects `{fruit}` as an input, you call the `.invoke()` method with an object containing a `fruit` key.


[source,typescript]
----
include::{repository-raw}/{branch}/examples/chain.mjs[tag=invoke, indent=0]
----


[TIP]
.Type Safety
====

You can ensure type safety in your chains by defining input and output types on the `.from()` method.

[source,typescript]
----
include::{repository-raw}/{branch}/examples/chain.mjs[tag=types]
----
====


== Check Your Understanding

include::./questions/1-runnable-sequence.adoc[leveloffset=+1]
include::./questions/2-invoke.adoc[leveloffset=+1]

// TODO: Question type, sequence list - Prompt, LLM, Output Parser??


[.summary]
== Lesson Summary

In this lesson, you learned how to combine an array of steps into a single `RunnableSequence`.

In the next lesson, you will use this knowledge to create a chain that will generate an answer based on a given context, a technique known as **Retrieval Augmented Generation (RAG)**.
