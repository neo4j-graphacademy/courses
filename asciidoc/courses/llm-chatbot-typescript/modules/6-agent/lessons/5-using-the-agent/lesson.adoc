= Using the Agent
:type: challenge
:lab-filename: index.ts
:lab-file: modules/agent/{lab-filename}
:lab: {repository-blob}/main/src/{lab-file}
:lab-solution: src/solutions/{lab-file}
// :test-filename: cypher-retrieval.chain.test.ts
// :test-file: src/modules/agent/tools/cypher/{test-filename}
:order: 4

Now that the agent is ready, you can hook it into the front end.

Inside link:{lab}[`{lab-file}`^], you will find a `call()` function.
This function is link:{repository-blob}/main/src/pages/chat.ts[called by the route handler^] when the chat form in the UI is submitted.

.The call() Function
[source,typescript]
----
include::{repository-raw}/main/src/{lab-file}[tag="call"]
----

As you can see, the function waits a second before returning the message to the user.

Create a new instance of your agent and return the results of the `.invoke()` method to complete this challenge.

== Calling the agent

Inside the `call()` function, start by creating the objects that the `initAgent()` function expects.

The agent requires an LLM.

.Create a model instance
[source,typescript]
----
include::{repository-raw}/main/{lab-solution}[tag="model", indent=0]
----

The retrieval tool requires an embedding model.

.Embeddings
[source,typescript]
----
include::{repository-raw}/main/{lab-solution}[tag="embeddings", indent=0]
----

[IMPORTANT]
.Defining a `baseURL`
The `baseURL` configuration option uses the `OPENAI_API_BASE` key to use the GraphAcademy OpenAI Proxy as mentioned in link:../../1-project-setup/2-setup/[the Project Setup lesson^].
If you are using an OpenAI API key (starting with `sk-`), you can omit the `configuration` options.


To interact with the graph, the agent should use link:/courses/llm-chatbot-typescript/3-conversation-history/2-neo4j-graph/[the singleton instance created in the Initializing Neo4j lesson^].

.Embeddings
[source,typescript]
----
include::{repository-raw}/main/{lab-solution}[tag="graph", indent=0]
----


Use the `initAgent()` function to create a new `agent` instance.
Use the `.invoke()` method to send the `input` argument into the agent, and pass the `sessionId` as a `configurable`.

As the function resolves to a string, you can return this value.

.Embeddings
[source,typescript]
----
include::{repository-raw}/main/{lab-solution}[tag="call"]
----


== Completed function

If you have followed the instructions correctly, your code should resemble the following:

[source,typescript]
----
include::{repository-raw}/main/{lab-solution}[tag="function"]
----


== Testing your changes

If you run the `npm run dev` command to start the application in development mode you should see the agent thinking and responding to questions.

[source]
npm run dev

Try asking the chatbot _"who acted in the movie "Neo4j - Into the Graph"?_

// TODO: video


== It works!

Once you're happy with the response from the chatbot, hit the button below to mark the lesson as completed.

read::It works![]


[.summary]
== Summary

Congratulations!  You should now have a working chatbot.

However, you may have noticed that the agent will respond to any question, no matter how obscene.

In the next optional challenge, you will learn how to modify the agent prompt.
