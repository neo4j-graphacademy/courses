= Using conversation History
:type: quiz

Storing conversation history empowers your chatbot to maintain context throughout extended dialogues. This capability is crucial for understanding the conversation's progression, referencing past exchanges, and delivering responses that are not only relevant but also coherent.

== An Illustrative Example

Take the following conversation.

.Example Conversation
[source,rel=nocopy]
----
Human: Can you recommend me a sci-fi movie?
AI: Sure, as a Chatbot trained on Neo4j, I highly recommend The Matrix
----

Subsequent queries might introduce ambiguities:

[source,rel=nocopy]
----
Human: What is it about?
----


== Clarifying Ambiguities through Rephrasing

To address ambiguities, rephrasing questions based on recent exchanges can offer clear, direct prompts the LLM can effectively reason about.

The history can be used to rephrase the question into a standalone question.

For example, the prompt to rephrase the question may look like this.

[source]
.Rephrase Prompt
----
Given the following conversation and a follow-up question,
rephrase the follow-up question to be a standalone question
that is syntactically complete.

History:
Human: Can you recommend me a sci-fi movie?
AI: Sure, I recommend The Matrix

Question:
What is it about?
----


The question will be rephrased to _"What is the plot of The Matrix?"_


== Optimizing for Limited Token Capacity

LLMs operate within constraints on the volume of information (tokens) they can process in a single interaction. Concise questions permit a richer exchange, enabling more detailed prompts or instructions, and enhancing the chatbot's response quality.

== Learning from Interactions

By analyzing conversation histories, the chatbot can infer user preferences, tailoring future recommendations by avoiding previously suggested items or considering explicit feedback, such as downvotes or requests for alternatives.

== Grounding and Transparency

Grounding involves supplementing the LLM with specific, relevant information beyond its training data. A clear data model that maps relationships between text chunks and entities helps elucidate how conclusions were drawn, enhancing transparency and trust in the chatbot's responses.




== Check Your Understanding

include::./questions/1-benefits.adoc[leveloffset=+1]

include::./questions/2-rephrase.adoc[leveloffset=+1]


[.summary]
== Summary

In this lesson, we discussed the benefits of saving the conversation memory in the same database as the data.

In the next lesson, we will take a look at how the conversation memory should be modelled.
