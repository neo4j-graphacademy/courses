= Using conversation History
:type: quiz
:order: 1

Storing conversation history empowers your chatbot to maintain context throughout extended dialogues. This capability is crucial for understanding the conversation's progression, referencing past exchanges, and delivering responses that are not only relevant but also coherent.

== An Illustrative Example

Take the following conversation.

.Example Conversation
[source,rel=nocopy]
----
Human: Can you recommend me a sci-fi movie?
AI: Sure, as a Chatbot trained on Neo4j, I highly recommend The Matrix
----

Subsequent queries might introduce ambiguities:

[source,rel=nocopy]
----
Human: What is it about?
----


== Clarifying Ambiguities through Rephrasing

To address ambiguities, rephrasing questions based on recent exchanges can offer clear, direct prompts the LLM can effectively reason about.

You can use the history to rephrase the question into a standalone question.

For example, the prompt to rephrase the question may look like this.

[source]
.Rephrase Prompt
----
Given the following conversation and a follow-up question,
rephrase the follow-up question to be a standalone question
that is syntactically complete.

History:
Human: Can you recommend a sci-fi movie?
AI: Sure, I recommend The Matrix

Question:
What is it about?
----

The standalone question will become _"What is the plot of The Matrix?"_


== Optimizing for Limited Token Capacity

LLMs operate within constraints on the volume of information (tokens) they can process in a single interaction. Concise questions permit a richer exchange, enabling more detailed prompts or instructions and enhancing the chatbot's response quality.

== Learning from Interactions

The chatbot can analyze conversation histories to infer user preferences, tailoring future recommendations by avoiding previously suggested items or considering explicit feedback, such as downvotes or requests for alternatives.


== Grounding and Transparency

Grounding involves supplementing the LLM with specific, relevant information beyond its training data. A data model that features relationships between text chunks and entities will help a developer or administrator to conclude how the LLM came to generate its response to a specific question, enhancing transparency and trust in the chatbot's responses.


== Check Your Understanding

include::./questions/1-benefits.adoc[leveloffset=+1]

include::./questions/2-rephrase.adoc[leveloffset=+1]


[.summary]
== Summary

In this lesson, you learned the benefits of saving the conversation history in the same database as the data.

In the next lesson, you will learn how to model the conversation history to achieve the goal.
