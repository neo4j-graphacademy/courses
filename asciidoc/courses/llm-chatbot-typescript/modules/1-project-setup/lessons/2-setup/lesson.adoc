= Project Setup
:type: lesson
:order: 2
:disable-cache: true
:lab-filename: bot.py
:lab: {repository-blob}/main/{lab-filename}
:branch: main


Let's start by getting the project up and running.

include::../../../../../../shared/courses/codespace/get-started.adoc[]

[%collapsible]
.Develop on your local machine
====
You will need link:https://nodejs.org/[Node.js] installed and the ability to install packages using `npm`.

The project was created using Node.js **v20.11.x0 LTS**.

You can verify your Node.js version by running the following command.

[source,sh]
.Verify Node.js Version
node --version

Clone the link:{repository-link}[github.com/neo4j-graphacademy/llm-chatbot-typescript] repository:

[source,bash]
.Clone the Repository
----
git clone https://github.com/neo4j-graphacademy/llm-chatbot-typescript
----

[TIP]
.Fork the repository
You can https://github.com/neo4j-graphacademy/llm-chatbot-typescript/fork[fork the repository] and have a copy for future reference.

Run the `npm install` command to install the dependencies.

[source,sh]
.Install Depencencies
cd llm-chatbot-typescript
npm install --include=dev
====

== Setup the environment

To connect to Neo4j, you must set your Neo4j Sandbox credentials as environment variables.

To set environment variables in a Next.js application, you can create a `.env.local` file.
You can link:https://nextjs.org/docs/pages/building-your-application/configuring/environment-variables[read more about Next.js Environment Variables in the Next.js documentation^].

Copy the `.env.example` file to `.env.local` and update the Neo4j connection settings:

.env.local
[source,env,subs="attributes+"]
----
NEO4J_URI="bolt://{instance-ip}:{instance-boltPort}"
NEO4J_USERNAME="{instance-username}"
NEO4J_PASSWORD="{instance-password}"
----

You must also set your OpenAI API key.
You can find link:https://help.openai.com/en/articles/4936850-where-do-i-find-my-openai-api-key[instructions on generating an API key on openai.com^].

Replace the `OPENAI_API_KEY` value with your OpenAI API key.

.env.local
[source,env]
----
OPENAI_API_KEY="sk-..."
----

[WARNING]
.Keep your secrets safe
====
Never share your API keys.
The `.env.local` file has been added to `.gitignore` so the contents are not unintentionally committed to Github.
====

== Start the Application

To start the app, run the `npm run dev` command.

[source,sh]
.Run the Next.js app
----
npm run dev
----

This command command will start a server on http://localhost:3000.
Once you have run the command, you should see the following interface.

image::images/working-app.png[GraphAcademy Chatbot App Interface with an introductory message from the assistant and a box to send a message.]

When the user sends a message, the app will render the message in a box aligned to the right of the screen, similar to other chat apps.
The app will display any messages returned by the API to the left of the screen.


[#explore]
== Exploring the `src/` folder

The code is purposely simple so you can focus on the LLM integration.

The `app/` folder contains the general application code, plus some custom CSS.
The `components/` folder contains React components that display the chat interface, including the form, message, and a _thinking_ indicator.

The `hooks/` folder contains the React hook that holds the chatbot's state and enables the chat interface.
When the form is submitted, the application state is updated to show the loading indicator and send an HTTP POST request to the `/api/chat/` endpoint, where it will generate a response.
When a response is received, the message is stored in the state and appended to the chat interface.


=== Handling Submissions

A link:{repository-blob}/main/src/pages/api/chat.ts[route handler in the `src/pages/api/chat.ts` file^] extracts the user's input and the current Session ID from the request.

Then, it passes the input and Session ID to link:{repository-blob}/main/src/modules/agent/index.ts[the `call()` function in the `src/modules/agent/index.ts` file^]. For now, the `call()` function simply returns the user's input after a short delay, but it will eventually provide an interface to the LLM.


// TODO: video?

[.quiz]
== Check your understanding

include::questions/1-server.adoc[leveloffset=+1]

[.summary]
== Summary

In this lesson, you obtained a copy of the course code, installed the dependency and used the `npm run dev` command to start the app.

In the next lesson, you will set the scope for the project.
