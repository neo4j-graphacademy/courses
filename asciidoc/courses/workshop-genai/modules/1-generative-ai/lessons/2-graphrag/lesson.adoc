= GraphRAG?
:order: 2
:type: lesson

[.slide]
== Retrieval-Augmented Generation

Retrieval-Augmented Generation (RAG) is an approach that enhances the responses of LLMs by providing them with relevant, up-to-date information retrieved from external sources.

RAG helps generate more accurate and tailored answers, especially when the required information is not present in the model’s training data.

[.slide.col-2]
== The Retrieval-Augmented Generation (RAG) Process

[.col]
====
The RAG process typically involves three main steps:

. **Understanding the User Query**
+
The system first interprets the user’s input or question to determine what information is needed.
. **Information Retrieval**
+
A _retriever_ searches external data sources (such as documents, databases, or knowledge graphs) to find relevant information based on the user’s query.
. **Response Generation**
+
The retrieved information is inserted into the prompt, and the language model uses this context to generate a more accurate and relevant response.
====

[.col]
image::images/llm-rag-process.svg["A diagram showing the RAG process. A question from a user is sent to a retriever, which searches for relevant information. The retrieved information is then combined with the original question and sent to a language model, which generates a response."]

[.slide.col-2.discrete]
== The RAG Process

[.col]
====
RAG systems can provide responses that are both contextually aware and grounded in real, up-to-date information.

If building a chatbot for a news agency, you could use RAG to pull real-time headlines or results from a news API.

When a user asks, "What’s the latest news on the Olympics?", the chatbot, can provide a current headline or summary from the most recent articles, ensuring the response is timely and accurate.

[NOTE]
.Grounding
=====
The process of providing context to an LLM to improve the accuracy of its responses and reduce the likelihood of hallucinations is known as _Grounding_.
=====
====

[.col]
image::images/llm-news-agency.svg["A news agency chatbot, showing the user asking a question, the chatbot grounding the question with a news API, and the chatbot responding with the latest news."]


[.slide]
== Retrievers

The retriever is a key component of the RAG process. A retriever is responsible for searching and retrieving relevant information from external data sources based on the user’s query.

A retriever typically takes an *unstructured input* (like a question or prompt) and searches for structured data that can provide context or answers.

Neo4j support various methods for building retrievers, including:

* Full-text search
* Vector search
* Text to Cypher

You will explore these methods in the rest of the course.

[.slide]
== Data sources

The data sources used in the RAG process can vary widely, depending on the application and the type of information needed. Common data sources include:

* **Documents**
+
Textual data sources, such as articles, reports, or manuals, that can be searched for relevant information.

* **APIs**
+
External services that can provide real-time data or specific information based on user queries.

* **Knowledge Graphs**
+
Graph-based representations of information that can provide context and relationships between entities.

[.slide.col-2.discrete]
== Data sources

[.col]
====
The news agency chatbot could use the following data sources:

* A news API to retrieve the latest articles or headlines.
* A knowledge graph to understand the relationships between different news topics, such as how they relate to each other or their historical context. This would help the chatbot provide more in-depth and contextual responses.
* A document database to store and retrieve articles, reports, or other textual data that can be used to answer user queries.
====

[.col]
image::images/llm-news-agency-knowledge-graph.svg["A news agency chatbot, showing the user asking a question, the chatbot grounding with the addition of data from a knowledge graph"]

[.transcript-only]
====
[TIP]
.Learn more about knowledge graphs
=====
You will learn more about knowledge graphs and their construction in the next module.
=====
====

[.slide.discrete]
== GraphRAG

GraphRAG (Graph Retrieval Augmented Generation) is an approach that uses the strengths of graph databases to provide relevant and useful context to LLMs.

GraphRAG can be used in conjunction with vector RAG.

While vector RAG uses embeddings to find contextually relevant information, GraphRAG enhances this process by leveraging the relationships and structure within a graph.

[.slide.discrete]
== GraphRAG

Benefits of GraphRAG:

* *Richer Context*
+
Graphs capture relationships between entities, enabling retrieval of more relevant and connected information.

* *Improved Accuracy*
+
By combining vector similarity with graph traversal, results are more precise and context-aware.

* *Explainability*
+
Graphs provide clear paths and connections, making it easier to understand why certain results were retrieved.

* *Flexible Queries*:
+
GraphRAG supports complex queries, such as combining full-text, vector, and text-to-cypher searches.

* *Enhanced Reasoning*
+
Graphs enable reasoning over data, supporting advanced use cases like recommendations and knowledge discovery.

[.slide.col-60-40]
== Graph-Enhanced Vector Search

[.col]
====
A common approach to GraphRAG is to use a combination of vector search and graph traversal.

This allows for the retrieval of relevant documents based on semantic similarity, followed by a graph traversal to find related entities or concepts.

The high-level process is as follows:

. A user submits a query.
. The system uses a vector search to find nodes similar to the users query.
. The graph is then traversed to find related nodes or entities.
. The entities and relationships are added to the context for the LLM.
. The related data could also be scored based on its relevance to the user query.
====

[.col]
image::images/llm-rag-vector+graph-process.svg[A diagram showing a user question being passed to a vector search to find semantically similar data. The results are then used to find related nodes or entities in the graph. The most relevant results are used as context for the LLM.]

[.slide]
== Full Text Search

Full text search is another powerful technique that can be combined with graph-enhanced search to further improve information retrieval.

While vector search excels at finding semantically similar content, full text search allows users to match specific keywords or phrases within documents or nodes.

If the user is looking for a movie or actor by name, full text search can quickly locate those entities based on exact text matches.

Full text search can be used as a replacement for or in conjunction with vector search.

When used in conjunction with vector search, full text search can refine results by filtering out irrelevant content based on specific keywords or phrases.

[TIP]
.Learn more about full text search
Full text search is available in Neo4j using link:https://neo4j.com/docs/cypher-manual/current/indexes/semantic-indexes/full-text-indexes/[full-text indexes^].

[.slide.col-2]
== Text to Cypher

[.col]
====
Text to Cypher is an alternative approach in GraphRAG that allows users to express their information needs in natural language, which is then automatically translated into Cypher queries.

You leverage the power of LLMs to interpret user intent and generate precise graph queries, enabling direct access to structured data and relationships within the graph.

You can use text to Cypher to turn users' queries into complex searches, aggregations, or traversals, making advanced graph querying more accessible and flexible.

Text to Cypher works by passing the user's query and the graph schema to an LLM, which generates a Cypher query that can be executed against the graph database.
====

[.col]
image::images/llm-text-to-cypher-process.svg[A diagram showing a user question being passed to an LLM, which generates a Cypher query based on the graph schema. The generated Cypher query is then executed against the graph database to retrieve relevant data.]

[.transcript-only]
====
[IMPORTANT]
.Exercise caution with LLM-generated queries
=====
Caution should always be taken when executing LLM-generated Cypher queries, as they may not always be safe or efficient.

You are trusting the generation of Cypher to the LLM.
It may generate invalid Cypher queries that could corrupt data in the graph or provide access to sensitive information.

In a production environment, you should ensure that access to data is limited, and sufficient security is in place to prevent malicious queries.
=====
====

read::Continue[]

[.summary]
== Lesson Summary

In this lesson, you learned about RAG and GraphRAG techniques, and how they can be used to enhance information retrieval.

In the next lesson, you will learn about knowledge graphs and how they represent real-world entities and their relationships.
