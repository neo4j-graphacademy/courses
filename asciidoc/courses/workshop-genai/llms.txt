# Neo4j & Generative AI Hands-On Workshop

Learn how Neo4j and GraphRAG can support your Generative AI projects

[Learn more about this course](mdc:https://graphacademy.neo4j.com/courses/workshop-genai)

## Concepts

* **Generative AI (GenAI)** - Artificial intelligence systems that create new content using patterns learned from training data, including text, images, audio, or code
* **Large Language Models (LLMs)** - Generative AI models that understand and generate human-like text based on statistical patterns from training data
* **Knowledge Graphs** - Graph databases using nodes (entities), relationships (connections), and properties (attributes) to organize and access interrelated data
* **Retrieval-Augmented Generation (RAG)** - Architecture combining LLMs with external data retrieval to provide accurate, contextual responses grounded in domain-specific data
* **GraphRAG** - Advanced RAG approach using knowledge graphs to provide structured, relationship-aware retrieval instead of flat document chunks
* **Vector Embeddings** - Numerical representations of text that capture semantic meaning, enabling similarity search based on context rather than keywords
* **Retrievers** - Components that search and return relevant information from knowledge graphs to answer questions or provide context to language models
* **Agents** - Conversational wrappers around retrievers that analyze queries, select appropriate tools, and maintain conversation context
* **Schema-Driven Extraction** - Using predefined entity types and relationships to guide AI extraction of structured data from unstructured documents

## Overview

### Summary of Lessons on Generative AI, Knowledge Graphs, and Retrievers

#### **Module 1: Generative AI**
1. **What is Generative AI**  
   - Generative AI systems create new content resembling human-made data (text, images, audio, code).  
   - Models like GPT and DALL-E are trained on large datasets, learning patterns to generate coherent outputs.  
   - Generative AI lacks true understanding, relying on statistical patterns from training data.  
   - Applications include chatbots, content creation, image synthesis, and code generation.  

2. **Large Language Models (LLMs)**  
   - LLMs are generative AI models designed for human-like text generation.  
   - They perform tasks like answering questions, summarizing data, and analyzing text.  
   - Responses are probabilistic continuations of instructions based on learned patterns.  

3. **Knowledge Graphs**  
   - Knowledge graphs organize interrelated data using design patterns.  
   - They enable structured access to entities and relationships, enhancing context-aware retrieval.  

---

#### **Module 2: Retrievers**
1. **What is a Retriever**  
   - Retrievers search and return relevant information from a knowledge graph.  
   - Types of retrievers:  
     - **Vector Retriever**: Semantic search across text chunks.  
     - **Vector + Cypher Retriever**: Combines semantic search with graph traversal.  
     - **Text2Cypher Retriever**: Converts natural language to Cypher queries for precise answers.  

2. **Hands-On with Retrievers**  
   - Explored retrievers in a Jupyter notebook:  
     - **Vector Retriever**: Finds semantically similar text chunks.  
     - **Vector + Cypher Retriever**: Adds contextual relationships to semantic search.  
     - **Text2Cypher Retriever**: Handles structured queries for precise data.  
   - Compared retriever results for broad, entity-specific, and precise questions.  

---

#### **Module 3: Agents**
1. **What is an Agent**  
   - Agents wrap retrievers into conversational tools for intelligent query handling.  
   - They analyze user queries, select the appropriate retriever tool, and format responses.  

2. **Building Agents**  
   - **First Agent**: Schema introspection tool for database exploration.  
   - **Enhanced Agent**: Added document retrieval tool for semantic and contextual search.  
   - **Complete Agent**: Added Text2Cypher tool for precise database queries.  

3. **Agent Capabilities**  
   - Combines retrievers for comprehensive query handling:  
     - Semantic exploration.  
     - Contextual relationship traversal.  
     - Precise structured queries.  
   - Intelligent tool selection based on question type.  

---

#### **Key Insights**
- Generative AI models excel at creating coherent outputs but lack true understanding.  
- Knowledge graphs enhance retrieval by preserving connections and enabling context-aware search.  
- Retrievers and agents provide flexible, intelligent interfaces for querying knowledge graphs.  
- Combining retrievers into agents allows for conversational, multi-tool reasoning.  

## Building Knowledge Graphs from Unstructured Data

The code used to build the knowledge graph from unstructured data.

```python
# Example of SimpleKGPipeline for extracting structured data from PDFs
from neo4j_graphrag.experimental.pipeline.kg_builder import SimpleKGPipeline

# Define entity schema for extraction
entities = {
    "Company": ["name", "industry"],
    "Executive": ["name", "title"], 
    "Product": ["name", "description"],
    "FinancialMetric": ["name", "value", "period"],
    "RiskFactor": ["name", "description"],
    "StockType": ["type", "class"],
    "Transaction": ["type", "amount", "date"],
    "TimePeriod": ["start_date", "end_date"]
}

# Define relationship schema
relations = [
    "Company HAS_METRIC FinancialMetric",
    "Company FACES_RISK RiskFactor", 
    "Company ISSUED_STOCK StockType",
    "Company MENTIONS Product",
    "Executive WORKS_FOR Company"
]

# Create and configure extraction pipeline
pipeline = SimpleKGPipeline(
    driver=driver,  # Neo4j connection
    llm=llm,        # OpenAI LLM for extraction
    embedder=embedder,  # OpenAI embeddings
    entities=entities,  # Entity schema definition
    relations=relations,  # Relationship schema
    enforce_schema="STRICT",  # Enforce strict schema compliance
    prompt_template=custom_prompt_template  # Custom extraction prompts
)

# Process PDF documents to build knowledge graph
pdf_documents = [
    "apple-10K-2023.pdf", 
    "microsoft-10K-2023.pdf",
    "amazon-10K-2023.pdf"
]

# Extract entities and relationships from each document
for pdf_file in pdf_documents:
    pipeline.run(file_path=pdf_file)
```
## Vector Retriever Implementation

```python
# Vector retriever for semantic search across text chunks
vector_retriever = VectorRetriever(
    driver=driver,
    index_name='chunkEmbeddings',  # Name of the vector index
    embedder=embedder,             # OpenAI embeddings model
    return_properties=['text']     # Properties to return from matched nodes
)

# Perform semantic search
query = "What are the risks that Apple faces?"
results = vector_retriever.search(query_text=query, top_k=10)

# Process results
for item in results.items:
    print(f"Score: {item.metadata['score']:.4f}")
    print(f"Content: {item.content[:100]}...")
    print(f"ID: {item.metadata['id']}")
    print("---")

# Use vector retriever with GraphRAG for complete answers
rag = GraphRAG(llm=llm, retriever=vector_retriever)
answer = rag.search(query)
print(f"Answer: {answer.answer}")
```

## Vector + Cypher Retriever for Contextual Search

```python
# Custom Cypher query to traverse from chunks to related entities
contextual_query = """
WITH node
MATCH (node)-[:FROM_DOCUMENT]-(doc:Document)-[:FILED]-(company:Company)
OPTIONAL MATCH (company)-[:FACES_RISK]->(risk:RiskFactor)
OPTIONAL MATCH (company)<-[:OWNS]-(manager:AssetManager)
RETURN 
    company.name AS company,
    node.text AS context, 
    collect(DISTINCT risk.name) AS risks,
    collect(DISTINCT manager.managerName) AS asset_managers
"""

# Vector + Cypher retriever for hybrid search
vector_cypher_retriever = VectorCypherRetriever(
    driver=driver,
    index_name='chunkEmbeddings',
    embedder=embedder,
    retrieval_query=contextual_query  # Custom Cypher for graph traversal
)

# Query that leverages both semantic search and graph relationships
query = "Which asset managers are most affected by cryptocurrency policies?"

# Retrieve contextually enriched results
results = vector_cypher_retriever.search(query_text=query, top_k=10)
for item in results.items:
    print(f"Company: {item.content.get('company', 'N/A')}")
    print(f"Context: {item.content.get('context', '')[:150]}...")
    print(f"Asset Managers: {item.content.get('asset_managers', [])}")
    print("---")

# Use with GraphRAG for comprehensive answers
rag = GraphRAG(llm=llm, retriever=vector_cypher_retriever)
answer = rag.search(query)
print(f"Answer: {answer.answer}")
```

[Reference: Vector + Cypher Retriever](mdc:https://graphacademy.neo4j.com/courses/workshop-genai/2-retrievers/1-retrievers)

## Text2Cypher Retriever for Structured Queries

```python
# Get graph schema for Text2Cypher generation
schema = get_schema(driver)
print("Graph Schema:")
print(schema)

# Text2Cypher retriever for natural language to Cypher conversion
text2cypher_retriever = Text2CypherRetriever(
    driver=driver,
    llm=llm,              # LLM for query generation
    neo4j_schema=schema   # Graph schema for context
)

# Natural language query converted to Cypher
query = "What are the company names of companies owned by BlackRock Inc?"
cypher_result = text2cypher_retriever.get_search_results(query)

print(f"Original Query: {query}")
print(f"Generated Cypher: {cypher_result.metadata['cypher']}")

# Execute the generated Cypher query
result = driver.execute_query(cypher_result.metadata["cypher"])
print("Results:")
for record in result.records:
    print(record)

# Example queries that work well with Text2Cypher
structured_queries = [
    "How many companies mention cloud computing?",
    "Count the risk factors for Microsoft",
    "What financial metrics does Apple report?",
    "Which asset managers own the most companies?",
    "List all products mentioned by technology companies"
]

# Process multiple structured queries
for question in structured_queries:
    result = text2cypher_retriever.get_search_results(question)
    print(f"Q: {question}")
    print(f"Cypher: {result.metadata['cypher']}")
    print("---")
```

## Building Conversational Agents with LangChain

```python
from langchain.chat_models import init_chat_model
from langgraph.prebuilt import create_react_agent
from langchain_core.tools import tool
from langchain_neo4j import Neo4jGraph, Neo4jVector
from langchain_openai import OpenAIEmbeddings

# Initialize LLM for agent
model = init_chat_model("gpt-4o", model_provider="openai")

# Connect to Neo4j for agent operations
graph = Neo4jGraph(
    url=os.getenv("NEO4J_URI"),
    username=os.getenv("NEO4J_USERNAME"), 
    password=os.getenv("NEO4J_PASSWORD"),
)

# Tool 1: Schema introspection
@tool("Get-graph-database-schema")
def get_schema():
    """Get the schema of the graph database."""
    return graph.schema

# Tool 2: Document retrieval with graph context
embedding_model = OpenAIEmbeddings(model="text-embedding-ada-002")

# Retrieval query for contextual information
retrieval_query = """
MATCH (node)-[:FROM_DOCUMENT]-(doc:Document)-[:FILED]-(company:Company)
RETURN 
    node.text as text,
    score,
    {
        company: company.name,
        risks: [ (company:Company)-[:FACES_RISK]->(risk:RiskFactor) | risk.name ]
    } AS metadata
ORDER BY score DESC
"""

# Create vector index for document search
chunk_vector = Neo4jVector.from_existing_index(
    embedding_model,
    graph=graph,
    index_name="chunkEmbeddings",
    embedding_node_property="embedding",
    text_node_property="text",
    retrieval_query=retrieval_query,
)

@tool("Retrieve-financial-documents")
def retrieve_docs(query: str):
    """Find details about companies in their financial documents."""
    context = chunk_vector.similarity_search(query, k=3)
    return context

# Tool 3: Text2Cypher for structured queries
@tool("Execute-cypher-query")
def execute_cypher(natural_language_query: str):
    """Convert natural language to Cypher and execute against the graph."""
    # Use Text2CypherRetriever to generate Cypher
    text2cypher = Text2CypherRetriever(driver=driver, llm=llm, neo4j_schema=schema)
    result = text2cypher.get_search_results(natural_language_query)
    
    # Execute the generated Cypher
    cypher_query = result.metadata["cypher"]
    execution_result = driver.execute_query(cypher_query)
    
    return {
        "generated_cypher": cypher_query,
        "results": [dict(record) for record in execution_result.records]
    }

# Create agent with all tools
tools = [get_schema, retrieve_docs, execute_cypher]
agent = create_react_agent(model, tools)

# Agent conversation examples
def chat_with_agent(query):
    """Run agent conversation with streaming output"""
    print(f"Query: {query}")
    print("Agent Response:")
    
    for step in agent.stream(
        {"messages": [{"role": "user", "content": query}]},
        stream_mode="values",
    ):
        step["messages"][-1].pretty_print()

# Example conversations
chat_with_agent("Summarize the schema of the graph database.")
chat_with_agent("What risk factors are mentioned in Apple's financial documents?")
chat_with_agent("How many companies does BlackRock own?")
chat_with_agent("Which asset managers are most affected by banking regulations?")
```
## Summary

This comprehensive implementation guide covers:

**Core Technologies:**
- Neo4j GraphRAG with Python for building intelligent retrieval systems
- OpenAI LLM and embeddings for semantic understanding
- LangChain agents for conversational interfaces

**Key Patterns:**
- **Vector Retrieval** for semantic exploration and broad topic search
- **Vector + Cypher Retrieval** for contextual search with entity relationships  
- **Text2Cypher Retrieval** for precise structured queries and aggregations
- **Agent Orchestration** for intelligent tool selection and conversation management

**Production Considerations:**
- Error handling and fallback strategies
- Health monitoring and system diagnostics
- Query routing and intent analysis
- Context management and conversation flow

**Best Practices:**
- Use schema-driven extraction for consistent data quality
- Implement proper connection management and cleanup
- Design retrieval queries that match your use cases
- Combine multiple retrieval strategies for comprehensive coverage

This framework enables building production-ready GraphRAG applications that can handle complex queries across structured and unstructured data while maintaining conversational interfaces for end users.
