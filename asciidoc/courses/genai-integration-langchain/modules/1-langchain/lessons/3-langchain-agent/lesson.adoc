= Simple LangChain Agent
:order: 3
:type: lesson
:branch: main

Throughout this course, you will be adapting a simple LangChain agent to interact with Neo4j. 

You will update the agent to query a Neo4j graph database, retrieve information using RAG and GraphRAG, and dynamically generate Cypher queries based on user input.

In this lesson, you will review the agents code to understand how it works.

== Agent

Open the `genai-integration-langchain/simple_agent.py` file.

[source, python]
.simple_agent.py
----
include::{repository-raw}/{branch}/genai-integration-langchain/simple_agent.py[tag=**]
----

Review the code and try to answer the following questions:

. What is the agents purpose?
. What `context` is added to the agents prompt?
. What do you think the final `answer` will be?

Run the agent to see what it does.

[%collapsible]
.Click to reveal the answers
====
The agent is designed to answer questions about the information that is provided in the `context`.

The context contains weather information about London and San Francisco, so:

* When passed the question _"What is the weather in San Francisco?"_.
* The agent responds with _"Sunny skies, raining overnight."_.

Regardless of what data is in the context, the agent will provide an answer based on the information it has.
====

The application is a simple link:https://www.langchain.com/langgraph[LangGraph^] agent that has 2 steps:

. Retrieve information
. Generate an answer based on the retrieved information

The code has 4 main sections:

. Create an LLM and Prompt
. Define the application state
. Create the application workflow
. Invoke the application

== LLM and Prompt

The agent uses an `OpenAI` LLM and a simple prompt to generate an answer based on the retrieved information.

[source, python]
----
include::{repository-raw}/{branch}/genai-integration-langchain/solutions/simple_agent.py[tag=llm_prompt]
----

The prompt sets the instructions for the LLM to generate an answer based on the retrieved information.

The variables `context` and `question` are used to provide the information to the LLM.

== Application State

The application state holds the information that is required to run the agent.
The state includes the original `question`, the `context`, and the generated `answer`.

[source, python]
----
include::{repository-raw}/{branch}/genai-integration-langchain/solutions/simple_agent.py[tag=application_state]
----

The `context` can be any information that is relevant to the question being asked.

== Application Workflow

The application workflow consists of:

. Functions that will be executed to `retrieve` the context and `generate` the answer:
+
[source, python]
----
include::{repository-raw}/{branch}/genai-integration-langchain/solutions/simple_agent.py[tag=application_functions]
----
+
The application state will be updated with data returned by the functions.
. The `workflow` defines the order the functions are executed:
+
[source, python]
----
include::{repository-raw}/{branch}/genai-integration-langchain/solutions/simple_agent.py[tag=application_workflow]
----
+
The `retrieve` function is called at the `START` of the workflow, before the `generate` function.

== Invoke

Finally, the application is _invoked_ passing a question to the agent and printing the answer:

[source, python]
----
include::{repository-raw}/{branch}/genai-integration-langchain/solutions/simple_agent.py[tag=invoke]
----

== Check Your Understanding

include::questions/1-no-context.adoc[leveloffset=+1]

[.summary]
== Lesson Summary

In this lesson, you review a simple LangChain agent that generates an answer based on a provided context.

In the next lesson, you will modify the agent to retrieve the schema from a Neo4j database and use it to answer questions about a graph.
