= Vectors
:order: 4
:type: lesson

Vectors are a fundamental concept in linear algebra.
You can use Vectors to represent data in a way that is easy to manipulate and analyze.

In this lesson, you will learn how vectors can support semantic search.

== What are vectors?

Vectors are simply a list of numbers.
For example, the vector `[1, 2, 3]` is a list of three numbers and could represent a point in three-dimensional space.

image::images/3d-vector.svg[A diagram showing a 3d representation of the x,y,z coordinates 1,1,1 and 1,2,3]

You can use vectors to represent many different types of data, including text, images, and audio.

The number of dimensions in a vector is called the **dimensionality** of the vector.
For example, a vector with three numbers has a dimensionality of 3. A vector containing 100 numbers has a dimensionality of 100.

Using vectors with a dimensionality of hundreds and thousands in machine learning and natural language processing (NLP) is very common.

== What are embeddings?

When referring to vectors in the context of machine learning and NLP, the term "embedding" is typically used.
An embedding is a vector that represents the data in a useful way for a specific task.

For example, in the previous lesson, you looked at an embedding for a movie plot.
This embedding was a vector that represented the plot of a movie that was useful for finding similar movies.

Each dimension in a vector can represent a particular semantic aspect of the word or phrase.
When multiple dimensions are combined, they can convey the overall meaning of the word or phrase.

For example, the word "apple" might be represented by an embedding with the following dimensions:

* fruit
* technology
* color
* taste
* shape

When applied in a search context, the vector for "apple" can be compared to the vectors for other words or phrases to determine the most relevant results.

You can create embeddings in various ways, but one of the most common methods is to use a **large language model**.

For example, the embedding for the word "apple" is `0.0077788467, -0.02306925, -0.007360777, -0.027743412, -0.0045747845, 0.01289164, -0.021863015, -0.008587573, 0.01892967, -0.029854324, -0.0027962727, 0.020108491, -0.004530236, 0.009129008,` ... and so on.

[%collapsible]
.Reveal the completed embeddings for the word "apple"!
====
[source]
----
include::apple-embedding.adoc[]
----
====

[NOTE]
.Embedding models
====
OpenAI's `text-embedding-ada-002` embedding model created this embedding - a vector of 1,536 dimensions.

LLM providers typically expose API endpoints that convert a _chunk_ of text into a vector embedding.
Depending on the provider, the shape and size of the vector may differ.
====

While it is possible to create embeddings for individual words, embedding entire sentences or paragraphs is more common. 
The meaning of a word can change based on its context. 
For example, the word _bank_ will have a different vector in _river bank_ than in _savings bank_.

Semantic search systems can use these contextual embeddings to understand user intent.

Embeddings can represent more than just words.
They can also represent entire documents, images, audio, or other data types.
They are instrumental in the operation of many other machine-learning tasks.

== How are vectors used in semantic search?

You can use the _distance_ or _angle_ between vectors to gauge the semantic similarity between words or phrases.

image::images/vector-distance.svg[A 3 dimensional chart illustrating the distance between vectors. The vectors are for the words "apple" and "fruit",width=700,align=center]

Words with similar meanings or contexts will have vectors that are close together, while unrelated words will be farther apart.

This principle is employed in semantic search to find contextually relevant results for a user's query.

A semantic search involves the following steps:

. The user submits a query.
. The system creates a vector representation (embedding) of the query.
. The system compares the query vector to the vectors of the indexed data.
. The results are scored based on their similarity.
. The system returns the most relevant results to the user.

image::images/semantic-vector-search.svg[A diagram show the steps of a semantic search,width=700,align=center.]


== Vectors and Neo4j

Vectors are the backbone of semantic search.
They enable systems to understand and represent the complex, multi-dimensional nature of language, context, and meaning.

Since the v5.11 release, Neo4j has a link:https://neo4j.com/docs/cypher-manual/current/indexes-for-vector-search/[Vector search index^], allowing you to query for nodes based on their vector representations.

In the next lesson, you will learn about unstructured data and how vectors can help you understand and find information.


== Check Your Understanding

include::questions/1-dimensions.adoc[leveloffset=+1]

[.summary]
== Lesson Summary

In this lesson, you learned about vectors, embeddings, and their role in semantic search.

In the next lesson, you will learn about unstructured data and how vectors can help you understand and find information.