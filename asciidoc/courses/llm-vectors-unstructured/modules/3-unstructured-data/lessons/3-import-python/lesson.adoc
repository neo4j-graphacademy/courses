= Import data with Python and Langchain
:order: 3
:type: lesson
:disable-cache: true

In this lesson, you will use Python and Langchain to chunk up course content and create embeddings for each chunk.
You will then load the chunks into a Neo4j graph database.

== Install requirement components

To get started, you will need to install the following Python packages using pip:

[source]
.Install packages
pip install langchain langchain_community langchain_openai openai neo4j unstructured python-magic-bin tiktoken

== Download the data

You will load the content from the course link:https://graphacademy.neo4j.com/courses/llm-fundamentals/[Neo4j & LLM Fundamentals^].

The course content has been downloaded for you and is available in the file link:https://data.neo4j.com/llm-vectors-unstructured/llm-fundamentals.zip[data.neo4j.com/llm-vectors-unstructured/llm-fundamentals.zip^]

[NOTE]
.GraphAcademy courses repository
====
Alternatively, you could clone Neo4j GraphAcademy courses repository from link:https://github.com/neo4j-graphacademy/courses[https://github.com/neo4j-graphacademy/courses]. However, all the data you will need is in the zip file.
====

Download the data, unzip it, and review the directory and file structure.

You should note the following structure:

* `asciidoc` - contains all the course content in ascidoc format
** `courses` - the course content
*** `llm-fundamentals` - the course name
**** `modules` - contains numbered directories for each module
***** `01-name` - the module name
****** `lessons` - contains numbered directories for each lesson
******* `01-name` - the lesson name
******** `lesson.adoc` - the lesson content

== Load the content and chunk it

You can now load the content and chunk it using Python and Langchain.

You will split the lesson content into chunks of text, around 1500 characters long, with each chunk containing one or more paragraphs.
You can determine the paragraph in the content with two newline characters (`\n\n`).

Review the following program before running it:

[source,python]
----
include::code/load_and_chunk.py[]
----

The program does the following:

. Uses the link:https://api.python.langchain.com/en/latest/document_loaders/langchain_community.document_loaders.directory.DirectoryLoader.html[`DirectoryLoader`^] class to load the content from the `asciidoc` directory.
. Creates a link:https://api.python.langchain.com/en/latest/text_splitter/langchain.text_splitter.CharacterTextSplitter.html[`CharacterTextSplitter`^] object  to split the content into chunks of text.
. Calls the `split_documents` method to split the documents into chunks of text based on the existence of `\n\n` and a chunk size of 1500 characters.

Create a new Python file and run the program.

[IMPORTANT]
.Course content location
====
You may need to modify the `COURSES_PATH` variable to point to the location of the course content on your computer.

The program assumes that the `asciidoc` directory is in the same directory as your Python program.
====

The program should output a list of `Document` objects containing the _chunked up_ lesson content.

[NOTE]
.Splitting
====
The content isn't split simply by a character (`\n\n`) or on a fixed number of characters.
The process is more complicated.
Chunks should be up to maximum size but conform to the character split.

In this example, the `split_documents` method does the following:

. Splits the documents into paragraphs (using the `separator` - `\n\n`)
. Combines the paragraphs into chunks of text that are up 1500 characters (`chunk_size`)
** if a single paragraph is longer than 1500 characters, the method will not split the paragraph but create a chunk larger than 1500 characters
. Adds the last paragraph in a chunk to the start of the next paragraph to create an overlap between chunks.
** if the last paragraph in a chunk is more than 200 characters (`chunk_overlap`) it will *not* be added to the next chunk

This process ensures that:

* Chunks are never too small.
* That a paragraph is never split between chunks.
* That chunks are significantly different, and the overlap doesn't result in a lot of repeated content.

Investigate what happens when you modify the `separator`, `chunk_size` and `chunk_overlap` parameters.
====

== OpenAI API Key and connection details

You must update the code to include your `openai_api_key` and use the `url`, `username`, and `password` of your Neo4j sandbox.

Sandbox connection details:

url:: [copy]#bolt://{sandbox_ip}:{sandbox_boltPort}#
username:: [copy]#{sandbox_username}#
password:: [copy]#{sandbox_password}#

== Create vector index

Once you have chunked the content, you can use the Langchain link:https://python.langchain.com/docs/integrations/vectorstores/neo4jvector[`Neo4jVector`^] class to create embeddings, a vector index, and store the chunks in a Neo4j graph database.

Modify your Python program to include the following code:

[source, python]
----
from langchain_community.vectorstores.neo4j_vector import Neo4jVector
from langchain_openai import OpenAIEmbeddings

neo4j_db = Neo4jVector.from_documents(
    chunks,
    OpenAIEmbeddings(openai_api_key="sk-..."),
    url=NEO4J_URI,
    username=NEO4J_USERNAME,
    password=NEO4J_PASSWORD,
    database="neo4j",
    index_name="chunkVector",
    node_label="Chunk",
    text_node_property="text",
    embedding_node_property="embedding",
)
----

[%collapsible]
.View the complete code
====
[source]
----
include::code/create_vector.py[]
----
====



The `Neo4jVector.from_documents` method does the following:

. Creates embeddings for each chunk using the `OpenAIEmbeddings` object.
. Creates nodes with the label `Chunk` and the properties `text` and `embedding` in the Neo4j database.
. Creates a vector index called `chunkVector`.

Run the program to create the chunk nodes and vector index. It may take a minute or two to complete.

== View chunks in the sandbox

You can now view the chunks in the Neo4j sandbox.

[source,cypher]
----
MATCH (c:Chunk) RETURN c LIMIT 25
----

You can also query the vector index to find similar chunks.
For example, you can find lesson chunks relating to a specific question, "What does Hallucination mean?":

[source,cypher]
----
WITH genai.vector.encode(
    "What does Hallucination mean?",
    "OpenAI",
    { token: "sk-..." }) AS userEmbedding
CALL db.index.vector.queryNodes('chunkVector', 6, userEmbedding)
YIELD node, score
RETURN node.text, score
----

[IMPORTANT]
Remember to replace `sk-...` with your OpenAI API key.


== Check Your Understanding

include::questions/1-character-split.adoc[leveloffset=+1]

[.summary]
== Lesson Summary

In this lesson, you learned how to chunk data and create a vector index using Python and Langchain.

In the next lesson, you will create a graph of the course content.